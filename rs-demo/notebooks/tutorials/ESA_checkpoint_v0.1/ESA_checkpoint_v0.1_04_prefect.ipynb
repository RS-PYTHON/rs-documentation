{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e4c4026-d406-4114-966a-d5f757f13a00",
   "metadata": {},
   "source": [
    "<table align='right'><tr>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/logos/EC_POS.png\" style=\"max-height:50px;width:auto;\"/></td>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/logos/ESA_logo_2020_Deep.png\" style=\"max-height:40px;width:auto;\"/></td>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/logos/Copernicus_blue.png\" style=\"max-height:60px;width:auto;\"/></td>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/logos/AIRBUS_Blue.png\" style=\"max-height:30px;width:auto;\"/></td>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/logos/CS-GROUP.png\" style=\"max-height:50px;width:auto;\"/></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26abb86-e472-4c47-981a-32b67871e569",
   "metadata": {},
   "source": [
    "<a href=\"./ESA_checkpoint_v0.1_03_stac_catalog.ipynb\" target=\"_blank\"><< Part 3: use the STAC catalog</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeaa622-5796-475b-8746-11a540613c64",
   "metadata": {},
   "source": [
    "<font color=\"#138D75\">**Copernicus Reference System Python**</font> <br>\n",
    "**Copyright:** Copyright 2024 ESA <br>\n",
    "**License:** Apache License, Version 2.0 <br>\n",
    "**Authors:** Airbus, CS Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85eadd-c73b-4c45-b1d9-cefb26dee873",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3>Copernicus Reference System Python tutorial for the ESA checkpoint 0.1</h3></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01537ae8-55c6-4b00-889c-ac6a4b0f1200",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<h4>Part 4: Prefect workflows</h4>\n",
    "\n",
    "Prerequisites:\n",
    "* <a href=\"./ESA_checkpoint_v0.1_01_initialisation.ipynb\" target=\"_blank\">Part 1: initialisation</a>\n",
    "\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf9681-d649-4b33-8c79-021e39e4dd1b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Links\n",
    "\n",
    "* GitHub: https://github.com/RS-PYTHON\n",
    "* Documentation: https://home.rs-python.eu/rs-documentation/\n",
    "\n",
    "## Data used\n",
    "\n",
    "In this notebook, we use simulated Auxip and Cadip data.\n",
    "\n",
    "## Learning outcomes\n",
    "\n",
    "At the end of this notebook you will know how to:\n",
    "* Use the RS-Client Python library.\n",
    "* Call Prefect flows to run parallel tasks to:\n",
    "    * Stage multiple Auxip and Cadip files at once.\n",
    "    * Run a simulated DPR processing on the staged files, and save results in the catalog.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## Contents\n",
    "\n",
    "</div>\n",
    "    \n",
    "1. [Check your installation](#Check-your-installation) \n",
    "1. [RsClient initialisation](#RsClient-initialisation)\n",
    " 1. [Prefect workflows](#Prefect-workflows)\n",
    "     1. [Initialisation](#Workflow:-initialisation)\n",
    "     1. [Stage Cadip chunk files](#Workflow:-stage-Cadip-chunk-files)\n",
    "     1. [Stage Auxip files](#Workflow:-stage-Auxip-files)\n",
    "     1. [DPR simulator](#Workflow:-DPR-simulator)\n",
    "     1. [Exercises](#Exercises)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d5806-1d27-4320-b374-278dc4d873d0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## Check your installation\n",
    "\n",
    "In this section, we will check that your Jupyter Notebook environment is correctly set.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df4aab-bc0e-405f-9dd6-f84cbe231033",
   "metadata": {},
   "source": [
    "### `rs-client-libraries` installation\n",
    "\n",
    "The `rs-client-libraries` Python library is the preferred way to access the RS-Server services from your environment. It is automatically installed in this notebook.\n",
    "\n",
    "**Note**: don't worry about these OpenTelemetry messages for now, they will be fixed in a later version:\n",
    "```\n",
    "Overriding of current TracerProvider is not allowed\n",
    "Attempting to instrument while already instrumented\n",
    "Transient error StatusCode.UNAVAILABLE encountered while exporting metrics to ..., retrying in ...s\n",
    "Failed to export metrics to ..., error code: StatusCode.UNIMPLEMENTED\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8b6cc-7b8c-4629-b0ab-1c900106a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rs_client\n",
    "import rs_common\n",
    "import rs_workflows\n",
    "\n",
    "# Set logger level to info\n",
    "import logging\n",
    "rs_common.logging.Logging.level = logging.INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58759bd2-2226-4847-85e8-4c7b20ff3ab8",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a380943-4c4a-4696-b9ca-98d9800a5142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# In local mode, all your services are running locally.\n",
    "# In hybrid or cluster mode, we use the services deployed on the RS-Server website.\n",
    "# This configuration is set in an environment variable.\n",
    "local_mode = (os.getenv(\"RSPY_LOCAL_MODE\") == \"1\")\n",
    "\n",
    "# In local mode, the service URLs are hardcoded in the docker-compose file\n",
    "if local_mode:\n",
    "    rs_server_href = None # not used\n",
    "    RSPY_HOST_AUXIP = \"http://localhost:8001/docs\"\n",
    "    RSPY_HOST_CADIP = \"http://localhost:8002/docs\"\n",
    "    RSPY_HOST_CATALOG = \"http://localhost:8003/api.html\"\n",
    "    RSPY_PREFECT_URL = \"http://localhost:4200\"\n",
    "    RSPY_DPR_SIMU_URL = \"http://dpr-simulator:8000\"\n",
    "\n",
    "# In hybrid or cluster mode, they are set in an environment variables\n",
    "else:\n",
    "    rs_server_href = os.environ[\"RSPY_WEBSITE\"]\n",
    "    RSPY_PREFECT_URL = os.environ['RSPY_PREFECT_URL']\n",
    "    RSPY_DPR_SIMU_URL = os.environ[\"RSPY_DPR_SIMU_URL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6551c-9282-4bd8-b947-7a312980e16b",
   "metadata": {},
   "source": [
    "### API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5c544-1103-4ff8-a412-6d569e831cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey = os.getenv(\"RSPY_APIKEY\")\n",
    "if (not local_mode) and (not apikey):\n",
    "    import getpass\n",
    "    apikey = getpass.getpass(f\"Enter your API key:\")\n",
    "    os.environ[\"RSPY_APIKEY\"] = apikey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fdda79-8941-4def-ad91-2d742ad90059",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## RsClient initialisation\n",
    "\n",
    "Initialise Python RsClient class instances to access the RS-Server services.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e516d9-2698-49ca-aa79-6df9560a64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rs_client.rs_client import RsClient\n",
    "from rs_common.config import ECadipStation\n",
    "\n",
    "# Init a generic RS-Client instance. Pass the:\n",
    "#   - RS-Server website URL\n",
    "#   - API key. If not set, we try to read it from the RSPY_APIKEY environment variable.\n",
    "#   - ID of the owner of the STAC catalog collections.\n",
    "#     By default, this is the user login from the keycloak account, associated to the API key.\n",
    "#     Or, in local mode, this is the local system username.\n",
    "#     Else, your API Key must give you the rights to read/write on this catalog owner (see next cell).\n",
    "#   - Logger (optional, a default one can be used)\n",
    "generic_client = RsClient(rs_server_href, rs_server_api_key=None, owner_id=None, logger=None)\n",
    "print(f\"STAC catalog owner: {generic_client.owner_id!r}\")\n",
    "\n",
    "# From this generic instance, get an Auxip client instance\n",
    "auxip_client = generic_client.get_auxip_client()\n",
    "\n",
    "# Or get a Cadip client instance. Pass the cadip station.\n",
    "cadip_station = ECadipStation.CADIP # you can also have: INS, MPS, MTI, NSG, SGS\n",
    "cadip_client = generic_client.get_cadip_client(cadip_station)\n",
    "\n",
    "# Or get a Stac client to access the catalog\n",
    "stac_client = generic_client.get_stac_client()\n",
    "\n",
    "print(\"\\nValidate that our catalog is valid to the STAC format...\")\n",
    "stac_client.validate_all()\n",
    "\n",
    "print(\"\\nDisplay the Stac catalog as a treeview in notebook:\")\n",
    "display(stac_client)\n",
    "\n",
    "print(\"\\nOr just display all its contents at once:\")\n",
    "print(json.dumps(stac_client.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9118f-79a5-4c54-af9b-bc335994d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In hybrid or cluster mode, show information from the keycloak account, associated to the api key\n",
    "if not local_mode:\n",
    "\n",
    "    # = keycloak account user login\n",
    "    print(f\"API key user login: {generic_client.apikey_user_login!r}\")\n",
    "\n",
    "    # Print the IAM (Identity and Access Management) roles\n",
    "    # For this tutorial, you must have: \n",
    "    #   - read/download access for Adgs (=Auxip) = \"rs_adgs_<read|download>\"\n",
    "    #   - read/download access to the Cadip station you passed on the above cell = \"rs_cadip_<station>_<read|download>\"\n",
    "    #   - (optional) read/write/download access to STAC catalog collections from other owners = \"rs_catalog_<owner_id>:<collection|*>_<read|write|download>\"\n",
    "    #     (you always have all access to your own collections with owner_id=apikey_user_login as printed above)\n",
    "    iam_roles = \"\\n\".join (sorted (generic_client.apikey_iam_roles))\n",
    "    print(f\"\\nAPI key IAM roles: \\n{iam_roles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73bfc1-d12b-4c6a-bf97-766e72b6fc66",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## Prefect workflows\n",
    "\n",
    "Prefect is a workflow orchestration tool. We will use it to:\n",
    "* Stage multiple Auxip and Cadip files at once.\n",
    "* Run a simulated DPR processing on the staged files, and save results in the catalog.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219cf31-e9c7-4c57-838c-7e1d694959fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some initialisation\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# We use this bucket name that is deployed on the cluster. \n",
    "# RS-Server has read/write access to this bucket, but as an end-user, you won't manipulate it directly.\n",
    "RSPY_TEMP_BUCKET = os.environ[\"RSPY_TEMP_BUCKET\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9ae81-a627-4887-b462-97979004cdab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Workflow: initialisation\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d953a-a0e0-4f87-94b3-55ba9ee7ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from rs_common.config import EPlatform\n",
    "\n",
    "# The workflow will stage all files between a date interval.\n",
    "# In this example, we will stage all the Cadip chunk files from the first Cadip session.\n",
    "\n",
    "# Get the first Cadip session ID for this date interval and mission\n",
    "start_date = datetime(2010, 1, 1, 12, 0, 0)\n",
    "stop_date = datetime(2024, 1, 1, 12, 0, 0)\n",
    "platforms = [EPlatform.S1A]\n",
    "session_id = cadip_client.search_sessions(\n",
    "    start_date=start_date, stop_date=stop_date, platforms=platforms)[0][\"id\"]\n",
    "print(f\"First Cadip session ID: {session_id!r}\")\n",
    "\n",
    "# We extract the mission and date from the session ID: <mission>_YYYYmmdd<other_info>\n",
    "mission, date_and_other = session_id.split(\"_\")\n",
    "date = datetime.strptime (date_and_other[:8], \"%Y%m%d\")\n",
    "start_date = date # start from midnight\n",
    "stop_date = date + timedelta(days=1) # midnight the day after\n",
    "\n",
    "print(f\"Mission: {mission!r}\")\n",
    "print(f\"Date interval: '{start_date} -> {stop_date}'\")\n",
    "\n",
    "# Note: we will miss files if the session overlaps two days. \n",
    "# We could also get the time interval from the session information \n",
    "# but the simulated data used in this notebook is not relevant.\n",
    "session_info = cadip_client.search_sessions(session_ids=[session_id])\n",
    "print(f\"Date interval from the session information (not used): \"\n",
    "      f\"'{session_info[0]['properties']['start_datetime']} -> {session_info[0]['properties']['end_datetime']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f44335-1bac-4daa-b01c-a1e26b68fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: if you re-run this tutorial, you can remove your old collections to start from fresh\n",
    "# for suffix in [\"_aux\", \"_chunk\", \"_dpr\"]:\n",
    "#     stac_client.remove_collection (f\"{mission}{suffix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a9bd0-ac99-4829-9094-c323f052505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a prerequisite, we must create manually the Auxip and Cadip \n",
    "# collections in the catalog, if they don't already exist.\n",
    "from pystac import Collection, Extent, SpatialExtent, TemporalExtent\n",
    "from pystac_client.exceptions import APIError\n",
    "from rs_workflows import staging\n",
    "\n",
    "for client in [auxip_client, cadip_client]:\n",
    "    collection_name = staging.create_collection_name(mission, client.station_name)\n",
    "\n",
    "    # Save the collection name for later\n",
    "    if client == auxip_client:\n",
    "        auxip_collection = collection_name\n",
    "    else:\n",
    "        cadip_collection = collection_name\n",
    "\n",
    "    # Try to get collection information\n",
    "    try:\n",
    "        stac_client.get_collection(collection_id=collection_name)\n",
    "        print(f\"Collection already exists: {collection_name!r}\")\n",
    "\n",
    "    # If it fails, this means that the collection doesn't exist, so create it\n",
    "    except APIError:\n",
    "\n",
    "        print(f\"Create collection: {collection_name!r}\")\n",
    "        response = stac_client.add_collection(\n",
    "            Collection(\n",
    "                id=collection_name,\n",
    "                description=None, # rs-client will provide a default description for us\n",
    "                extent=Extent(\n",
    "                    spatial=SpatialExtent(bboxes=[-180.0, -90.0, 180.0, 90.0]),\n",
    "                    temporal=TemporalExtent([start_date, stop_date])\n",
    "                )\n",
    "            ))\n",
    "        response.raise_for_status()\n",
    "        stac_client.validate_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8958f1f-737a-4d1f-96c0-02756d1926af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Workflow: stage Cadip chunk files\n",
    "\n",
    "Here we stage all the Cadip chunk files from the date interval corresponding to the Cadip session calculated above.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7563544-7dd6-4103-ad29-218ccf8cf9f5",
   "metadata": {},
   "source": [
    "![Workflow: staging](resources/img/v0.1/workflow_staging.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4422d974-9e50-44ec-a7f9-66fc6f94f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nView Prefect flow runs from: {RSPY_PREFECT_URL}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e432dfc6-aca9-46f4-8a0d-6749b11786bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tasks to be run in parallel\n",
    "MAX_WORKERS = 5\n",
    "\n",
    "# Staging workflow configuration\n",
    "config = staging.PrefectFlowConfig(\n",
    "    cadip_client, \n",
    "    mission, \n",
    "    s3_path = f\"s3://{RSPY_TEMP_BUCKET}/{cadip_client.owner_id}/{cadip_client.station_name}\",\n",
    "    tmp_download_path=None, # no local download\n",
    "    max_workers=MAX_WORKERS,\n",
    "    start_datetime=start_date,\n",
    "    stop_datetime=stop_date,\n",
    "    limit=None) # no limit on the number of files\n",
    "\n",
    "# Start the prefect flow\n",
    "staging.staging_flow(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec3f6b-d18b-4907-a1f5-5eef47e27598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that our catalog is valid to the STAC format.\n",
    "stac_client.validate_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c67f70-38ae-4e82-b0b4-f52ab58bc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the staged Cadip files in the STAC catalog\n",
    "\n",
    "# For searching, we need to prefix our collection name by <owner_id>_\n",
    "owner_collection = f\"{stac_client.owner_id}_{cadip_collection}\"\n",
    "\n",
    "# Use a cql2 filter to search by session ID\n",
    "filter_on_session = {\n",
    "    \"op\": \"and\",\n",
    "    \"args\": [\n",
    "      {\"op\": \"=\", \"args\": [{\"property\": \"collection\"}, owner_collection]},\n",
    "      {\"op\": \"=\", \"args\": [{\"property\": \"cadip:session_id\"}, session_id]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "search = stac_client.search(filter=filter_on_session)\n",
    "results = list(search.items_as_dicts())\n",
    "assert len(results) > 0, f\"At least one Cadip files should be staged for session ID: {session_id!r}\"\n",
    "print (f\"\\n{len(results)} Cadip files are staged for session ID: {session_id!r}.\")\n",
    "print (f\"First one:\\n{json.dumps (results[0], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe019b40-326c-4e3f-a9bd-33af6eaa26ea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Workflow: stage Auxip files\n",
    "\n",
    "We need to pass Auxip files to the DPR processing. They must be staged into the catalog.\n",
    "\n",
    "As, for now, the DPR processing is only a simulation, we can pass any Auxip files.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f05d5-3ffc-4b7e-8d0d-cf2bafcfc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the 3 most recent files between today and any old date.\n",
    "# NOTE: the \"created\" field corresponds to the publication date.\n",
    "files = auxip_client.search_stations(\n",
    "    datetime(year=1970, month=1, day=1), \n",
    "    datetime.today(), \n",
    "    sortby=\"-created\",\n",
    "    limit=None) # NOTE: for now \"limit\" is not working well with \"sortby\" so don't use it\n",
    "\n",
    "# Only keep the first 3 files\n",
    "files = files[:3]\n",
    "\n",
    "# Save the IDs = the filenames\n",
    "auxip_files = [f[\"id\"] for f in files]\n",
    "print_ids = \"\\n\".join(auxip_files)\n",
    "print(f\"Auxip files: \\n{print_ids}\")\n",
    "\n",
    "# Save the min and max dates for these 3 files\n",
    "dates = [datetime.strptime (f[\"properties\"][\"created\"], \"%Y-%m-%dT%H:%M:%S.%fZ\") for f in files]\n",
    "start_date = min(dates) - timedelta(seconds=1) # remove 1 second because the interval is exclusive\n",
    "stop_date = max(dates) + timedelta(seconds=1) # add 1 second\n",
    "print(f\"\\nDate interval: '{start_date} -> {stop_date}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa6a29-69a9-4d23-b290-3475e80e5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Staging workflow configuration\n",
    "config = staging.PrefectFlowConfig(\n",
    "    auxip_client, \n",
    "    mission, \n",
    "    s3_path = f\"s3://{RSPY_TEMP_BUCKET}/{auxip_client.owner_id}/{auxip_client.station_name}\",\n",
    "    tmp_download_path=None, # no local download\n",
    "    max_workers=MAX_WORKERS,\n",
    "    start_datetime=start_date,\n",
    "    stop_datetime=stop_date,\n",
    "    limit=None) # no limit on the number of files\n",
    "\n",
    "# Start the prefect flow\n",
    "staging.staging_flow(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a50f12-f08f-44c9-9449-bd413e836952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that our catalog is valid to the STAC format.\n",
    "stac_client.validate_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4841f-3798-4324-ade9-cd63ca76ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These 3 items should be in the STAC catalog, in the Auxip collection\n",
    "\n",
    "# Be sure that we don't have any duplicate filenames\n",
    "auxip_files = list(set(auxip_files))\n",
    "\n",
    "# Search by ID and collection\n",
    "owner_collection = f\"{stac_client.owner_id}_{auxip_collection}\"\n",
    "search = stac_client.search(ids=auxip_files, collections=[owner_collection])\n",
    "results = list(search.items_as_dicts())\n",
    "assert len(results) == len(auxip_files), f\"{len(results)} Auxip files were staged, we expected {len(auxip_files)}\"\n",
    "print(f\"Staged Auxip files:\\n\" + \"\\n\".join(auxip_files))\n",
    "print (f\"\\nFirst one:\\n{json.dumps (results[0], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19670b80-2a72-4e8d-b6c1-ff5b8c7f576e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Workflow: DPR simulator\n",
    "\n",
    "For now, this simulated DPR processor takes any input and writes any output.\n",
    "\n",
    "We use it to simulate a L0 processing that takes staged Cadip chunk files and Auxip files as input, and writes raw L0 products as output.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eef418-4d9f-4885-a74d-58c773c5949e",
   "metadata": {},
   "source": [
    "![Workflow: dpr](resources/img/v0.1/workflow_dpr.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8aaa8d-e0e8-468c-bbe2-d904eef0ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rs_workflows import s1_l0\n",
    "\n",
    "# The product types to process can be any of these 4 values\n",
    "product_types = [\"S1SEWRAW\", \"S1SIWRAW\", \"S1SSMRAW\", \"S1SWVRAW\"]\n",
    "\n",
    "# DPR workflow configuration\n",
    "config = s1_l0.PrefectS1L0FlowConfig(\n",
    "    stac_client,\n",
    "    RSPY_DPR_SIMU_URL,\n",
    "    mission,\n",
    "    session_id,\n",
    "    product_types,\n",
    "    auxip_files,\n",
    "    s3_path = f\"s3://{RSPY_TEMP_BUCKET}/{stac_client.owner_id}/DPR_S1L0\",\n",
    "    temp_s3_path = f\"s3://{RSPY_TEMP_BUCKET}/{stac_client.owner_id}/DPR_S1L0\",\n",
    ")\n",
    "\n",
    "# Start the prefect flow\n",
    "s1_l0.s1_l0_flow(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30079d-0c85-41b3-9547-d9ad5de7ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that our catalog is valid to the STAC format.\n",
    "stac_client.validate_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912853c-87e9-464b-9611-91a79f78ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output products in the STAC catalog\n",
    "\n",
    "# The DPR collection name is hardcoded in the workflow source code\n",
    "dpr_collection = f\"{mission}_dpr\"\n",
    "dpr_products = list(stac_client.get_collection(dpr_collection).get_items())\n",
    "\n",
    "assert len(dpr_products) > 0, f\"At least one DPR product should be saved in the catalog.\"\n",
    "print_ids = \"\\n\".join([product.id for product in dpr_products])\n",
    "print (f\"\\n{len(dpr_products)} DPR products are saved in the catalog:\\n{print_ids}\")\n",
    "print (f\"\\nFirst one:\\n{json.dumps (dpr_products[0].to_dict(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582507d-5c1f-41bb-88e2-d794eccef131",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "### Exercises\n",
    "\n",
    "</div>\n",
    "\n",
    "Run again the previous cells but this time: \n",
    "* Use Cadip chunk files from a different Cadip session.\n",
    "* Use any other Auxip files.\n",
    "* Check that these Cadip and Auxip files are staged in the STAC catalog.\n",
    "* Check that the resulting simulated L0 products are staged in the STAC catalog.\n",
    "\n",
    "[Back to top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131f86ea-c4cf-421e-afa7-82f3ee805f50",
   "metadata": {},
   "source": [
    "<a href=\"./ESA_checkpoint_v0.1_03_stac_catalog.ipynb\" target=\"_blank\"><< Part 3: use the STAC catalog</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43fa8c-4558-41d6-92cf-70ceb7764e79",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a href=\"https://github.com/RS-PYTHON\" target=\"_blank\">View on GitHub</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
