{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#reference-system-python","title":"Reference System Python","text":"<p>Reference System Python can be split into the following components:</p> <ul> <li> <p>RS-Server: The RS-Server controls user access to all sensitive     interfaces: Catalog, LTA, ADGS, PRIP, CADIP. As a consequence, we     can distinguish following components:</p> <ul> <li> <p>RS-Server Frontend</p> </li> <li> <p>RS-Server Catalog</p> </li> <li> <p>RS-Server PRIP</p> </li> <li> <p>RS-Server CADIP</p> </li> <li> <p>RS-Server LTA</p> </li> <li> <p>RS-Server AUXIP</p> </li> </ul> </li> <li> <p>RS-Client libraries: this is a set of python functions that allow     to build Copernicus processing chains. Among these libraries, we will find:</p> <ul> <li> <p>rs.client : python libraries that provide atomic call to RS-Server</p> </li> <li> <p>rs.processing : python libraries that wrap DPR processing and provide      high level service (combination of rs.client calls)</p> </li> <li> <p>rs.common : python libraries for metric, log and traces.</p> </li> <li> <p>Prefect tasks &amp; flows for On Demand processing chains</p> </li> <li> <p>Prefect tasks &amp; flows for Systematic processing chains</p> </li> </ul> </li> <li> <p>RS-Virtual environment: Component that gives users access to     computational environments and resources for executing processing     chains.</p> </li> <li> <p>Processing: this group hosts all components to process Sentinel     products. The CFI are not part of the group. Only wrappers,     preparation worker and Dask cluster provide processing power.     Here are the components:</p> <ul> <li> <p>Dask cluster</p> </li> <li> <p>DPR libraries</p> </li> </ul> </li> <li> <p>Infrastructure: The infrastructure is a portable, extensible and     open-source platform orchestrated by Kubernetes. We can isolate two     specific layouts: monitoring and security. As a consequence,     we can distinguish the following components:</p> <ul> <li> <p>Infrastructure core</p> </li> <li> <p>Infrastructure monitoring</p> </li> <li> <p>Infrastructure security</p> </li> </ul> </li> </ul> <p></p>"},{"location":"#main-components","title":"Main components","text":"<p>The documentation for RS-Server may be found here: RS-Server.</p> <p>The documentation for RS-Client may be found here: RS-Client-Libraries.</p>"},{"location":"#documentation-how-to","title":"Documentation how to","text":"<p>You may also check how to generate the documentation.</p>"},{"location":"description/","title":"Introduction","text":"<p>TO BE UPDATED !</p> <p>The rs-server technical documentation is mainly written using markdown. The python API documentation is built using mkdocs functionalities while the REST API documentation is built using fastapi functionalities.</p> <p>Therefore, the built procedure includes specific steps to handle these elements. The technical documentation include links to open the python api and rest api reference guides.</p>"},{"location":"description/#python-api-reference-guide","title":"Python api reference guide","text":"<p>The python api is generated using mkdocs functionalities. It enables us to generate a reference guide from python doc-strings.</p> <p>A link to this reference guide is added in the technical documentation.</p>"},{"location":"description/#rest-api-reference-guide","title":"Rest api reference guide","text":"<p>The REST API is provided by FastAPI on a dynamic way on the /docs endpoint, using SwaggerUI functionalities. The technical documentation should also be provided statically in the technical documentation. FastAPI enables us to export the openapi specification in a json format. Using Swagger UI functionalities, we can generate a static REST API documentation.</p> <p>It takes the form of an HTML index file using swagger for the rendering and the exported openapi file for the content.</p> <p>To provide a link to this REST API from the technical documentation, the same trick is used than the one used for the python API.</p>"},{"location":"how_to/","title":"Generate Documentation","text":"<p>This procedure is run automatically in the CI by the generate documentation workflow. It can be executed also locally to verify the generated documentation (before publishing it for example).</p> <p>See the description for more details on the process and technical stuff.</p>"},{"location":"how_to/#prerequisites","title":"Prerequisites","text":"<p>The following have to be installed - python version 3.11 - poetry at least version 1.7.1</p>"},{"location":"how_to/#prepare-the-environment","title":"Prepare the environment","text":"<ol> <li> <p>Clone the rs-documentation repository:  <pre><code>git clone https://github.com/RS-PYTHON/rs-documentation.git\ncd rs-documentation\n</code></pre></p> </li> <li> <p>Clone the RS-Server, RS-Client, RS-Demo, RS-Infrastructure and RS-Helm repositories. The default branch from which  the projects are pulled is 'develop', but one can choose whatever branch is needed by using '-b' flag.</p> <p>NOTE: The repositories need to be pulled in 'docs' directory</p> <p>NOTE: The RS-Demo, RS-Infrastructure and RS-Helm repositories are not public, so a github access is needed <pre><code>cd docs\ngit clone -b develop https://github.com/RS-PYTHON/rs-server.git\ngit clone -b develop https://github.com/RS-PYTHON/rs-client-libraries.git\ngit clone -b develop git@github.com:RS-PYTHON/rs-demo.git\ngit clone -b develop git@github.com:RS-PYTHON/rs-infrastructure.git\ngit clone -b develop git@github.com:RS-PYTHON/rs-helm.git\ncd ..\n</code></pre></p> </li> <li> <p>Install needed rs-documentation packages: <pre><code>poetry install\n</code></pre></p> </li> </ol>"},{"location":"how_to/#generate-and-integrate-the-documentation","title":"Generate and integrate the documentation","text":"<p>From the rs-documentation directory, execute the command: <pre><code>poetry run mkdocs build\n</code></pre></p> <p>This command generates the html pages of all the technical documentation as well as for the static documentation and write them in the site directory.</p>"},{"location":"how_to/#verify-the-generated-documentation","title":"Verify the generated documentation","text":"<p>The documentation is generated in the directory \"site\". The main page is the file \"index.html\" is this folder. It can be open in a web browser for example.</p> <p>The REST API can\u2019t be opened like that since it needs to be served. You can open the REST API locally starting the rs-server and connecting to the /docs endpoint.</p> <p>You can verify the generated documentation before publishing it. mkdocs may be used to serve the generated site: <pre><code>poetry run mkdocs serve\n</code></pre></p> <p>Important elements to check :</p> <ul> <li> <p>the python api is accessible in the technical documentation</p> </li> <li> <p>the python api is well formatted</p> </li> <li> <p>the rest api is accessible in the technical documentation</p> </li> <li> <p>the rest api is well formatted</p> </li> <li> <p>\u2026</p> </li> </ul>"},{"location":"rs_deployment_start/","title":"General","text":"<p>The deployment of RS-Python is divided in two parts:</p> <ul> <li> <p>Infrastructure deployment</p> </li> <li> <p>Applications deployment</p> </li> </ul>"},{"location":"generate_src_doc/rs-client/","title":"Python API Library for RS-Client","text":"<p>This is the generated documentation from the source code. It is divided into 2 sections, the documentation for RS-Client libraries and the documentation for the already implemented Prefect flows that use the RS-Client instances to access the RS-Server endpoints.</p>"},{"location":"generate_src_doc/rs-client/#rs-client-classes","title":"RS-Client Classes","text":"<p>Documentation for RS-Client: rs-client</p>"},{"location":"generate_src_doc/rs-client/#rs-client-workflows","title":"RS-Client Workflows","text":"<p>Currently, there are 3 Prefect flows implemented that use the a RS-Client instance to access the RS-Server  enpoints:</p> <ul> <li>an Example prefect flow </li> <li>a prefect flow that stages the files from a CADIP server or from an ADGS server: Staging</li> <li>a prefect flow specifically implemented for processing S1 L0 products.</li> </ul>"},{"location":"generate_src_doc/rs-client/example/","title":"Example","text":"<p>Prefect flow example for rs-client-libraries</p>"},{"location":"generate_src_doc/rs-client/example/#rs_workflows.example.hello_world","title":"<code>hello_world(name='COPERNICUS', tasks_number=2)</code>","text":"<p>Example flow that can be use in a COPERNICUS chain</p> <p>This prefect flow may be used as start point in creating your own prefect flows. It runs in parallel tasks_number of print_hello</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Username to be printed. Default COPERNICUS</p> <code>'COPERNICUS'</code> <code>tasks_number</code> <code>int</code> <p>Number of tasks to be run. Default 2</p> <code>2</code> Source code in <code>docs/rs-client-libraries/rs_workflows/example.py</code> <pre><code>@flow(name=\"Hello RS-Server flow \")\ndef hello_world(name=\"COPERNICUS\", tasks_number=2):\n    \"\"\"Example flow that can be use in a COPERNICUS chain\n\n    This prefect flow may be used as start point in creating your own prefect flows. It runs in parallel\n    tasks_number of print_hello\n\n    Args:\n        name (str): Username to be printed. Default COPERNICUS\n        tasks_number (int): Number of tasks to be run. Default 2\n    \"\"\"\n    for idx in range(0, tasks_number):\n        print_hello(name + \"_\" + str(idx))\n</code></pre>"},{"location":"generate_src_doc/rs-client/example/#rs_workflows.example.print_hello","title":"<code>print_hello(name)</code>","text":"<p>Example task to be used in a prefect flow</p> <p>This prefect task may be used as start point in creating your own prefect tasks</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Username to be printed</p> required Source code in <code>docs/rs-client-libraries/rs_workflows/example.py</code> <pre><code>@task\ndef print_hello(name: str):\n    \"\"\"Example task to be used in a prefect flow\n\n    This prefect task may be used as start point in creating your own prefect tasks\n\n    Args:\n        name (str): Username to be printed\n    \"\"\"\n    print(f\"Hello {name}!\")\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/","title":"API Clients Documentation","text":"<p>This documentation provides an overview of the various API clients available in the <code>rs_client</code> package. Each client is designed to interact with specific services and provide a convenient way to access their functionalities.  </p> <p>RsClient </p> <p>RsClient class implementation.</p> <p>Attributes:</p> Name Type Description <code>rs_server_href</code> <code>str</code> <p>RS-Server URL. In local mode, pass None.</p> <code>rs_server_api_key</code> <code>str</code> <p>API key for RS-Server authentication.                      If not set, we try to read it from the RSPY_APIKEY environment variable.</p> <code>owner_id</code> <code>str</code> <p>ID of the owner of the STAC catalog collections (no special characters allowoed).             By default, this is the user login from the keycloak account, associated to the API key.             Or, in local mode, this is the local system username.             Else, your API Key must give you the rights to read/write on this catalog owner.             This owner ID is also used in the RS-Client logging.</p> <code>logger</code> <code>Logger</code> <p>Logging instance.</p> <code>local_mode</code> <code>bool</code> <p>Local mode or hybrid/cluster mode.</p> <code>apikey_headers</code> <code>dict</code> <p>API key in a dict, ready-to-use in HTTP request headers.</p> Source code in <code>docs/rs-client-libraries/rs_client/rs_client.py</code> <pre><code>class RsClient:\n    \"\"\"\n    RsClient class implementation.\n\n    Attributes:\n        rs_server_href (str): RS-Server URL. In local mode, pass None.\n        rs_server_api_key (str): API key for RS-Server authentication.\n                                 If not set, we try to read it from the RSPY_APIKEY environment variable.\n        owner_id (str): ID of the owner of the STAC catalog collections (no special characters allowoed).\n                        By default, this is the user login from the keycloak account, associated to the API key.\n                        Or, in local mode, this is the local system username.\n                        Else, your API Key must give you the rights to read/write on this catalog owner.\n                        This owner ID is also used in the RS-Client logging.\n        logger (logging.Logger): Logging instance.\n        local_mode (bool): Local mode or hybrid/cluster mode.\n        apikey_headers (dict): API key in a dict, ready-to-use in HTTP request headers.\n    \"\"\"\n\n    def __init__(\n        self,\n        rs_server_href: str | None,\n        rs_server_api_key: str | None = None,\n        owner_id: str | None = None,\n        logger: logging.Logger | None = None,\n    ):\n        \"\"\"RsClient class constructor.\"\"\"\n        self.rs_server_href: str | None = rs_server_href\n        self.rs_server_api_key: str | None = rs_server_api_key\n        self.owner_id: str = owner_id or \"\"\n        self.logger: logging.Logger = logger or Logging.default(__name__)\n\n        # Remove trailing / character(s) from the URL\n        if self.rs_server_href:\n            self.rs_server_href = self.rs_server_href.strip().rstrip(\"/\").strip()\n\n        # We are in local mode if the URL is undefined.\n        # Env vars are used instead to determine the different services URL.\n        self.local_mode = not bool(self.rs_server_href)\n\n        # If the API key is not set, we try to read it from the RSPY_APIKEY environment variable.\n        if not self.rs_server_api_key:\n            self.rs_server_api_key = os.getenv(\"RSPY_APIKEY\")  # None if the env var is not set\n\n        if (not self.local_mode) and (not self.rs_server_api_key):\n            raise RuntimeError(\"API key is mandatory for RS-Server authentication\")\n\n        # For HTTP request headers\n        self.apikey_headers: dict = (\n            {\"headers\": {APIKEY_HEADER: self.rs_server_api_key}} if self.rs_server_api_key else {}\n        )\n\n        # Determine automatically the owner id\n        if not self.owner_id:\n            # In local mode, we use the local system username\n            if self.local_mode:\n                self.owner_id = getpass.getuser()\n\n            # In hybrid/cluster mode, we retrieve the API key login\n            else:\n                self.owner_id = self.apikey_user_login\n\n        # Remove special characters\n        self.owner_id = re.sub(r\"[^a-zA-Z0-9]+\", \"\", self.owner_id)\n\n        if not self.owner_id:\n            raise RuntimeError(\"The owner ID is empty or only contains special characters\")\n\n        self.logger.debug(f\"Owner ID: {self.owner_id!r}\")\n\n    # The following variable is needed for the tests to pass\n    apikey_security_cache: TTLCache = TTLCache(maxsize=sys.maxsize, ttl=120)\n\n    @cached(cache=apikey_security_cache)\n    def apikey_security(self) -&gt; tuple[list[str], dict, str]:\n        \"\"\"\n        Check the api key validity. Cache an infinite (sys.maxsize) number of results for 120 seconds.\n\n        Returns:\n            Tuple of (IAM roles, config, user login) information from the keycloak account, associated to the api key.\n        \"\"\"\n\n        # In local mode, we have no API key, so return empty results\n        if self.local_mode:\n            return [], {}, \"\"\n\n        # self.logger.warning(\n        #     f\"TODO: use {self.rs_server_href}/apikeymanager/check/api_key instead, see: \"\n        #     \"https://pforge-exchange2.astrium.eads.net/jira/browse/RSPY-257\",\n        # )\n        # Does not work in hybrid mode for now because this URL is not exposed.\n        check_url = os.environ[\"RSPY_UAC_CHECK_URL\"]\n\n        # Request the API key manager, pass user-defined api key in http header\n        # check_url = f\"{self.rs_server_href}/apikeymanager/check/api_key\"\n        self.logger.debug(\"Call the API key manager\")\n        response = requests.get(check_url, **self.apikey_headers, timeout=TIMEOUT)\n\n        # Read the api key info\n        if response.ok:\n            contents = response.json()\n            # Note: for now, config is an empty dict\n            return contents[\"iam_roles\"], contents[\"config\"], contents[\"user_login\"]\n\n        # Try to read the response detail or error\n        try:\n            json = response.json()\n            if \"detail\" in json:\n                detail = json[\"detail\"]\n            else:\n                detail = json[\"error\"]\n\n        # If this fail, get the full response content\n        except Exception:  # pylint: disable=broad-exception-caught\n            detail = response.content\n\n        raise RuntimeError(f\"API key manager status code {response.status_code}: {detail}\")\n\n    @property\n    def apikey_iam_roles(self) -&gt; list[str]:\n        \"\"\"\n        Return the IAM (Identity and Access Management) roles from the keycloak account,\n        associated to the api key.\n        \"\"\"\n        return self.apikey_security()[0]\n\n    @property\n    def apikey_config(self) -&gt; dict:\n        \"\"\"Return the config from the keycloak account, associated to the api key.\"\"\"\n        return self.apikey_security()[1]\n\n    @property\n    def apikey_user_login(self) -&gt; str:\n        \"\"\"Return the user login from the keycloak account, associated to the api key.\"\"\"\n        return self.apikey_security()[2]\n\n    #############################\n    # Get child class instances #\n    #############################\n\n    def get_auxip_client(self) -&gt; \"AuxipClient\":  # type: ignore # noqa: F821\n        \"\"\"\n        Return an instance of the child class AuxipClient, with the same attributes as this \"self\" instance.\n        \"\"\"\n        from rs_client.auxip_client import (  # pylint: disable=import-outside-toplevel,cyclic-import\n            AuxipClient,\n        )\n\n        return AuxipClient(self.rs_server_href, self.rs_server_api_key, self.owner_id, self.logger)\n\n    def get_cadip_client(\n        self,\n        station: ECadipStation,\n    ) -&gt; \"CadipClient\":  # type: ignore # noqa: F821\n        \"\"\"\n        Return an instance of the child class CadipClient, with the same attributes as this \"self\" instance.\n\n        Args:\n            station (ECadipStation): Cadip station\n        \"\"\"\n        from rs_client.cadip_client import (  # pylint: disable=import-outside-toplevel,cyclic-import\n            CadipClient,\n        )\n\n        return CadipClient(self.rs_server_href, self.rs_server_api_key, self.owner_id, station, self.logger)\n\n    def get_stac_client(self, *args, **kwargs) -&gt; \"StacClient\":  # type: ignore # noqa: F821\n        \"\"\"\n        Return an instance of the child class StacClient, with the same attributes as this \"self\" instance.\n        \"\"\"\n        from rs_client.stac_client import (  # pylint: disable=import-outside-toplevel,cyclic-import\n            StacClient,\n        )\n\n        return StacClient.open(self.rs_server_href, self.rs_server_api_key, self.owner_id, self.logger, *args, **kwargs)\n\n    ############################\n    # Call RS-Server endpoints #\n    ############################\n\n    def staging_status(self, filename, timeout: int = TIMEOUT) -&gt; EDownloadStatus:\n        \"\"\"Check the status of a file download from the specified rs-server endpoint.\n\n        This function sends a GET request to the rs-server endpoint with the filename as a query parameter\n        to retrieve the status of the file download. If the response is successful and contains a 'status'\n        key in the JSON response, the function returns the corresponding download status enum value. If the\n        response is not successful or does not contain the 'status' key, the function returns a FAILED status.\n\n        Args:\n            filename (str): The name of the file for which to check the status.\n            timeout (int): The timeout duration for the HTTP request.\n\n        Returns:\n            EDownloadStatus: The download status enum value based on the response from the endpoint.\n        \"\"\"\n\n        # TODO: check the status for a certain timeout if http returns NOK ?\n        try:\n            response = requests.get(\n                self.href_status,  # pylint: disable=no-member # (\"self\" is AuxipClient or CadipClient)\n                params={\"name\": filename},\n                timeout=timeout,\n                **self.apikey_headers,\n            )\n\n            eval_response = response.json()\n            if (\n                response.ok\n                and \"name\" in eval_response.keys()\n                and filename == eval_response[\"name\"]\n                and \"status\" in eval_response.keys()\n            ):\n                return EDownloadStatus(eval_response[\"status\"])\n\n        except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:\n            self.logger.exception(f\"Status endpoint exception: {e}\")\n\n        return EDownloadStatus.FAILED\n\n    def staging(self, filename: str, s3_path: str = \"\", tmp_download_path: str = \"\", timeout: int = TIMEOUT):\n        \"\"\"Stage a file for download.\n\n        This method stages a file for download by sending a request to the staging endpoint\n        with optional parameters for S3 path and temporary download path.\n\n        Args:\n            filename (str): The name of the file to be staged for download.\n            timeout (int): The timeout duration for the HTTP request.\n            s3_path (str, optional): The S3 path where the file will be stored after download.\n                Defaults to an empty string.\n            tmp_download_path (str, optional): The temporary download path for the file.\n                Defaults to an empty string.\n\n        Raises:\n            RuntimeError: If an error occurs while staging the file.\n\n        \"\"\"\n\n        # dictionary to be used for payload request\n        payload = {}\n        # some protections for the optional args\n        if s3_path:\n            payload[\"obs\"] = s3_path\n        if tmp_download_path:\n            payload[\"local\"] = tmp_download_path\n\n        # update the filename to be ingested\n        payload[\"name\"] = filename\n        try:\n            # logger.debug(f\"Calling  {endpoint} with payload {payload}\")\n            response = requests.get(\n                self.href_staging,  # pylint: disable=no-member #\u00a0(\"self\" is AuxipClient or CadipClient)\n                params=payload,\n                timeout=timeout,\n                **self.apikey_headers,\n            )\n            self.logger.debug(f\"Download start endpoint returned in {response.elapsed.total_seconds()}\")\n            if not response.ok:\n                self.logger.error(f\"The download endpoint returned error for file {filename}\\n\")\n                raise RuntimeError(f\"The download endpoint returned error for file {filename}\")\n        except (\n            requests.exceptions.RequestException,\n            requests.exceptions.Timeout,\n            requests.exceptions.ReadTimeout,\n        ) as e:\n            self.logger.exception(f\"Staging file exception for {filename}:\", e)\n            raise RuntimeError(f\"Staging file exception for {filename}\") from e\n\n    def search_stations(  # pylint: disable=too-many-arguments\n        self,\n        start_date: datetime,\n        stop_date: datetime,\n        limit: Union[int, None] = None,\n        sortby: Union[str, None] = None,\n        timeout: int = TIMEOUT,\n    ) -&gt; list:\n        \"\"\"Retrieve a list of files from the specified endpoint.\n\n        This function queries the specified endpoint to retrieve a list of files available in the\n        station (CADIP, ADGS, LTA ...) that were collected by the satellite within the provided time range,\n        starting from 'start_date' up to 'stop_date' (inclusive).\n\n        Args:\n            start_date (datetime): The start date of the time range.\n            stop_date (datetime): The stop date of the time range.\n            timeout (int): The timeout duration for the HTTP request.\n            limit (int, optional): The maximum number of results to return. Defaults to None.\n            sortby (str, optional): The attribute to sort the results by. Defaults to None.\n\n        Returns:\n            files (list): The list of files available at the station within the specified time range.\n\n        Raises:\n            RuntimeError: if the endpoint can't be reached\n\n        Notes:\n            - This function queries the specified endpoint with a time range to retrieve information about\n            available files.\n            - It constructs a payload with the start and stop dates in ISO 8601 format and sends a GET\n            request to the endpoint.\n            - The response is expected to be a STAC Compatible formatted JSONResponse, containing information about\n             available files.\n            - The function converts a STAC FeatureCollection to a Python list.\n        \"\"\"\n\n        payload = {\n            \"datetime\": start_date.strftime(DATETIME_FORMAT)\n            + \"/\"  # 2014-01-01T12:00:00Z/2023-12-30T12:00:00Z\",\n            + stop_date.strftime(DATETIME_FORMAT),\n        }\n        if limit:\n            payload[\"limit\"] = str(limit)\n        if sortby:\n            payload[\"sortby\"] = str(sortby)\n        try:\n            response = requests.get(\n                self.href_search,  # pylint: disable=no-member #\u00a0(\"self\" is AuxipClient or CadipClient)\n                params=payload,\n                timeout=timeout,\n                **self.apikey_headers,\n            )\n        except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:\n            self.logger.exception(f\"Could not get the response from the station search endpoint: {e}\")\n            raise RuntimeError(\"Could not get the response from the station search endpoint\") from e\n\n        files = []\n        try:\n            if response.ok:\n                for file_info in response.json()[\"features\"]:\n                    files.append(file_info)\n            else:\n                self.logger.error(f\"Error: {response.status_code} : {response.json()}\")\n        except KeyError as e:\n            raise RuntimeError(\"Wrong format of search endpoint answer\") from e\n\n        return files\n\n    ##############################################\n    # Methods to be implemented by child classes #\n    ##############################################\n\n    @property\n    def href_search(self) -&gt; str:\n        \"\"\"Implemented by AuxipClient and CadipClient.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def href_staging(self) -&gt; str:\n        \"\"\"Implemented by AuxipClient and CadipClient.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def href_status(self) -&gt; str:\n        \"\"\"Implemented by AuxipClient and CadipClient.\"\"\"\n        raise NotImplementedError\n</code></pre> <p>This client is used for interacting with the RS service. It provides methods for performing common operations such as querying and managing data.</p> <p>AuxipClient </p> <p>               Bases: <code>RsClient</code></p> <p>AuxipClient class implementation.</p> <p>Attributes: see :py:class:<code>RsClient</code></p> Source code in <code>docs/rs-client-libraries/rs_client/auxip_client.py</code> <pre><code>class AuxipClient(RsClient):\n    \"\"\"\n    AuxipClient class implementation.\n\n    Attributes: see :py:class:`RsClient`\n    \"\"\"\n\n    @property\n    def href_adgs(self) -&gt; str:\n        \"\"\"\n        Return the RS-Server ADGS URL hostname.\n        This URL can be overwritten using the RSPY_HOST_ADGS env variable (used e.g. for local mode).\n        Either it should just be the RS-Server URL.\n        \"\"\"\n        if from_env := os.getenv(\"RSPY_HOST_ADGS\", None):\n            return from_env.rstrip(\"/\")\n        if not self.rs_server_href:\n            raise RuntimeError(\"RS-Server URL is undefined\")\n        return self.rs_server_href.rstrip(\"/\")\n\n    @property\n    def href_search(self) -&gt; str:\n        \"\"\"Return the RS-Server hostname and path where the ADGS search endpoint is deployed.\"\"\"\n        return f\"{self.href_adgs}/adgs/aux/search\"\n\n    @property\n    def href_staging(self) -&gt; str:\n        \"\"\"Return the RS-Server hostname and path where the ADGS staging endpoint is deployed.\"\"\"\n        return f\"{self.href_adgs}/adgs/aux\"\n\n    @property\n    def href_status(self) -&gt; str:\n        \"\"\"Return the RS-Server hostname and path where the ADGS status endpoint is deployed.\"\"\"\n        return f\"{self.href_adgs}/adgs/aux/status\"\n\n    @property\n    def station_name(self) -&gt; str:\n        \"\"\"Return \"AUXIP\".\"\"\"\n        return AUXIP_STATION\n</code></pre> <p>The AuxipClient is tailored for accessing the AUXIP service. It includes functionalities for handling auxiliary data and metadata.</p> <p>CadipClient </p> <p>               Bases: <code>RsClient</code></p> <p>CadipClient class implementation.</p> <p>see :py:class:`RsClient`</p> Name Type Description <code>station</code> <code>ECadipStation</code> <p>Cadip station</p> Source code in <code>docs/rs-client-libraries/rs_client/cadip_client.py</code> <pre><code>class CadipClient(RsClient):\n    \"\"\"\n    CadipClient class implementation.\n\n    Attributes: see :py:class:`RsClient`\n        station (ECadipStation): Cadip station\n    \"\"\"\n\n    def __init__(  # pylint: disable=too-many-arguments\n        self,\n        rs_server_href: str | None,\n        rs_server_api_key: str | None,\n        owner_id: str,\n        station: ECadipStation,\n        logger: logging.Logger | None = None,\n    ):\n        \"\"\"CadipClient class constructor.\"\"\"\n        super().__init__(rs_server_href, rs_server_api_key, owner_id, logger)\n        try:\n            self.station: ECadipStation = ECadipStation[station]\n        except KeyError as e:\n            self.logger.exception(f\"There is no such CADIP station: {station}\")\n            raise RuntimeError(f\"There is no such CADIP station: {station}\") from e\n\n    @property\n    def href_cadip(self) -&gt; str:\n        \"\"\"\n        Return the RS-Server CADIP URL hostname.\n        This URL can be overwritten using the RSPY_HOST_CADIP env variable (used e.g. for local mode).\n        Either it should just be the RS-Server URL.\n        \"\"\"\n        if from_env := os.getenv(\"RSPY_HOST_CADIP\", None):\n            return from_env.rstrip(\"/\")\n        if not self.rs_server_href:\n            raise RuntimeError(\"RS-Server URL is undefined\")\n        return self.rs_server_href.rstrip(\"/\")\n\n    @property\n    def href_search(self) -&gt; str:\n        \"\"\"Return the RS-Server hostname and path where the CADIP search endpoint is deployed.\"\"\"\n        return f\"{self.href_cadip}/cadip/{self.station.value}/cadu/search\"\n\n    @property\n    def href_session(self) -&gt; str:\n        \"\"\"Return the RS-Server hostname and path where the CADIP search session endpoint is deployed.\"\"\"\n        return f\"{self.href_cadip}/cadip/{self.station.value}/session\"\n\n    @property\n    def href_staging(self) -&gt; str:\n        \"\"\"Return the RS-Server hostname and path where the CADIP staging endpoint is deployed.\"\"\"\n        return f\"{self.href_cadip}/cadip/{self.station.value}/cadu\"\n\n    @property\n    def href_status(self) -&gt; str:\n        \"\"\"Return the RS-Server hostname and path where the CADIP status endpoint is deployed.\"\"\"\n        return f\"{self.href_cadip}/cadip/{self.station.value}/cadu/status\"\n\n    @property\n    def station_name(self) -&gt; str:\n        \"\"\"Return the station name.\"\"\"\n        return self.station.value  # TO BE DISCUSSED: maybe just return \"CADIP\"\n\n    ############################\n    # Call RS-Server endpoints #\n    ############################\n\n    def search_sessions(  # pylint: disable=too-many-arguments\n        self,\n        session_ids: list[str] | None = None,\n        start_date: datetime | None = None,\n        stop_date: datetime | None = None,\n        platforms: list[EPlatform] | None = None,\n        timeout: int = TIMEOUT,\n    ) -&gt; list[dict]:  # TODO return pystac.ItemCollection instead\n        \"\"\"Endpoint to retrieve list of sessions from any CADIP station.\n\n        Args:\n            timeout (int): The timeout duration for the HTTP request.\n            session_ids (list[str]): Session identifiers\n                (eg: [\"S1A_20170501121534062343\"] or [\"S1A_20170501121534062343, S1A_20240328185208053186\"])\n            start_date (datetime): Start date of the time interval\n            stop_date (datetime): Stop date of the time interval\n            platforms (list[PlatformEnum]): platform list\n        \"\"\"\n\n        payload = {}\n        if session_ids:\n            payload[\"id\"] = \",\".join(session_ids)\n        if platforms:\n            payload[\"platform\"] = \",\".join([platform.value for platform in platforms])\n        if start_date:\n            payload[\"start_date\"] = start_date.strftime(DATETIME_FORMAT)\n        if stop_date:\n            payload[\"stop_date\"] = stop_date.strftime(DATETIME_FORMAT)\n        try:\n            response = requests.get(\n                self.href_session,\n                params=payload,\n                timeout=timeout,\n                **self.apikey_headers,\n            )\n        except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:\n            self.logger.exception(f\"Could not get the response from the session search endpoint: {e}\")\n            raise RuntimeError(\"Could not get the response from the session search endpoint\") from e\n\n        sessions = []\n        try:\n            if response.ok:\n                for session_info in response.json()[\"features\"]:\n                    sessions.append(session_info)\n            else:\n                self.logger.error(f\"Error: {response.status_code} : {response.json()}\")\n        except KeyError as e:\n            raise RuntimeError(\"Wrong format of session search endpoint answer\") from e\n\n        return sessions\n</code></pre> <p>CadipClient is designed to interface with the CADIP service. Use this client to perform tasks related to CADIP data processing and retrieval.</p> <p>StacClient </p> <p>               Bases: <code>RsClient</code>, <code>Client</code></p> <p>StacClient inherits from both rs_client.RsClient and pystac_client.Client. The goal of this class is to allow an user to use RS-Server services more easily than calling REST endpoints directly.</p> Source code in <code>docs/rs-client-libraries/rs_client/stac_client.py</code> <pre><code>class StacClient(RsClient, Client):  # type: ignore # pylint: disable=too-many-ancestors\n    \"\"\"StacClient inherits from both rs_client.RsClient and pystac_client.Client. The goal of this class is to\n    allow an user to use RS-Server services more easily than calling REST endpoints directly.\n    \"\"\"\n\n    ##################\n    # Initialisation #\n    ##################\n\n    def __init__(  # pylint: disable=too-many-arguments,super-init-not-called # super-init is called in .open(...)\n        self,\n        id: str,  # pylint: disable=redefined-builtin\n        description: str,\n        title: Optional[str] = None,\n        stac_extensions: Optional[List[str]] = None,\n        extra_fields: Optional[Dict[str, Any]] = None,\n        href: Optional[str] = None,\n        catalog_type: CatalogType = CatalogType.ABSOLUTE_PUBLISHED,\n        strategy: Optional[HrefLayoutStrategy] = None,\n        *,\n        modifier: Optional[Callable[[Modifiable], None]] = None,\n        **kwargs: Dict[str, Any],\n    ):\n        \"\"\"\n        Constructor. Called only by pystac.\n        As an user: don't use this directly, call the open(...) class method instead or RsClient.get_stac_client(...).\n        \"\"\"\n\n        # Call manually the parent pystac Client constructor.\n        # The RsClient constructor will be called manually later.\n        Client.__init__(\n            self,\n            id=id,\n            description=description,\n            title=title,\n            stac_extensions=stac_extensions,\n            extra_fields=extra_fields,\n            href=href,\n            catalog_type=catalog_type,\n            strategy=strategy,\n            modifier=modifier,\n            **kwargs,\n        )\n\n    @classmethod\n    def open(  # type: ignore  # pylint: disable=arguments-renamed, too-many-arguments\n        cls,\n        # RsClient parameters\n        rs_server_href: str | None,\n        rs_server_api_key: str | None,\n        owner_id: str | None,\n        logger: logging.Logger | None = None,\n        # pystac Client parameters\n        headers: Optional[Dict[str, str]] = None,\n        parameters: Optional[Dict[str, Any]] = None,\n        ignore_conformance: Optional[bool] = None,\n        modifier: Optional[Callable[[Modifiable], None]] = None,\n        request_modifier: Optional[Callable[[Request], Union[Request, None]]] = None,\n        stac_io: Optional[StacApiIO] = None,\n        timeout: Optional[Timeout] = TIMEOUT,\n    ) -&gt; StacClient:\n        \"\"\"Create a new StacClient instance.\"\"\"\n\n        if rs_server_api_key:\n            if headers is None:\n                headers = {}\n            headers[APIKEY_HEADER] = rs_server_api_key\n\n        client: StacClient = super().open(  # type: ignore\n            cls.__href_catalog(rs_server_href) + \"/catalog/\",\n            headers,\n            parameters,\n            ignore_conformance,\n            modifier,\n            request_modifier,\n            stac_io,\n            timeout,\n        )\n\n        # Manual call to the parent RsClient constructor\n        RsClient.__init__(\n            client,\n            rs_server_href=rs_server_href,\n            rs_server_api_key=rs_server_api_key,\n            owner_id=owner_id,\n            logger=logger,\n        )\n\n        return client\n\n    ##############\n    # Properties #\n    ##############\n\n    @property\n    def href_catalog(self) -&gt; str:\n        \"\"\"\n        Return the RS-Server catalog URL hostname.\n        This URL can be overwritten using the RSPY_HOST_CATALOG env variable (used e.g. for local mode).\n        Either it should just be the RS-Server URL.\n        \"\"\"\n        return self.__href_catalog(self.rs_server_href)\n\n    @staticmethod\n    def __href_catalog(rs_server_href) -&gt; str:\n        if from_env := os.getenv(\"RSPY_HOST_CATALOG\", None):\n            return from_env.rstrip(\"/\")\n        if not rs_server_href:\n            raise RuntimeError(\"RS-Server URL is undefined\")\n        return rs_server_href.rstrip(\"/\")\n\n    def full_collection_id(self, owner_id: str | None, collection_id: str):\n        \"\"\"\n        Return the full collection name as: &lt;owner_id&gt;:&lt;collection_id&gt;\n\n        Args:\n            owner_id (str): Collection owner ID. If missing, we use self.owner_id.\n            collection_id (str): Collection name\n        \"\"\"\n        return f\"{owner_id or self.owner_id}:{collection_id}\"\n\n    ################################\n    # Specific STAC implementation #\n    ################################\n\n    @lru_cache()\n    def get_collection(self, collection_id: str, owner_id: str | None = None) -&gt; Union[Collection, CollectionClient]:\n        \"\"\"Get the requested collection as &lt;owner_id&gt;:&lt;collection_id&gt;\"\"\"\n        full_collection_id = self.full_collection_id(owner_id, collection_id)\n        return Client.get_collection(self, full_collection_id)\n\n    def add_collection(\n        self,\n        collection: Collection,\n        add_public_license: bool = True,\n        owner_id: str | None = None,\n        timeout: int = TIMEOUT,\n    ) -&gt; Response:\n        \"\"\"Update the collection links, then post the collection into the catalog.\n\n        Args:\n            collection (Collection): STAC collection\n            add_public_license (bool): If True, add a public domain license field and link.\n            owner_id (str, optional): Collection owner ID. If missing, we use self.owner_id.\n            timeout (int): The timeout duration for the HTTP request.\n\n        Returns:\n            JSONResponse (json): The response of the request.\n        \"\"\"\n\n        full_owner_id = owner_id or self.owner_id\n\n        # Use owner_id:collection_id instead of just the collection ID, before adding the links,\n        # so the links contain the full owner_id:collection_id\n        short_collection_id = collection.id\n        full_collection_id = self.full_collection_id(owner_id, short_collection_id)\n        collection.id = full_collection_id\n\n        # Default description\n        if not collection.description:\n            collection.description = f\"This is the collection {short_collection_id} from user {full_owner_id}.\"\n\n        # Add the owner_id as an extra field\n        collection.extra_fields[\"owner\"] = full_owner_id\n\n        # Add public domain license\n        if add_public_license:\n            collection.license = \"public-domain\"\n            collection.add_link(\n                Link(\n                    rel=RelType.LICENSE,\n                    target=\"https://creativecommons.org/licenses/publicdomain/\",\n                    title=\"public-domain\",\n                ),\n            )\n\n        # Update the links\n        self.add_child(collection)\n\n        # Restore the short collection_id at the root of the collection\n        collection.id = short_collection_id\n\n        # Check that the collection is compliant to STAC\n        collection.validate_all()\n\n        # Post the collection to the catalog\n        return requests.post(\n            f\"{self.href_catalog}/catalog/collections\",\n            json=collection.to_dict(),\n            **self.apikey_headers,\n            timeout=timeout,\n        )\n\n    def remove_collection(\n        self,\n        collection_id: str,\n        owner_id: str | None = None,\n        timeout: int = TIMEOUT,\n    ) -&gt; Response:\n        \"\"\"Remove/delete a collection from the catalog.\n\n        Args:\n            collection_id (str): The collection id.\n            owner_id (str, optional): Collection owner ID. If missing, we use self.owner_id.\n            timeout (int): The timeout duration for the HTTP request.\n\n        Returns:\n            JSONResponse: The response of the request.\n        \"\"\"\n        # owner_id:collection_id\n        full_collection_id = self.full_collection_id(owner_id, collection_id)\n\n        # Remove the collection from the \"child\" links of the local catalog instance\n        collection_link = f\"{self.self_href.rstrip('/')}/collections/{full_collection_id}\"\n        self.links = [\n            link for link in self.links if not ((link.rel == pystac.RelType.CHILD) and (link.href == collection_link))\n        ]\n\n        # We need to clear the cache for this and parent \"get_collection\" methods\n        # because their returned value must be updated.\n        self.get_collection.cache_clear()\n        Client.get_collection.cache_clear()\n\n        # Remove the collection from the server catalog\n        return requests.delete(\n            f\"{self.href_catalog}/catalog/collections/{full_collection_id}\",\n            **self.apikey_headers,\n            timeout=timeout,\n        )\n\n    def add_item(  # type: ignore # pylint: disable=arguments-renamed\n        self,\n        collection_id: str,\n        item: Item,\n        owner_id: str | None = None,\n        timeout: int = TIMEOUT,\n    ) -&gt; Response:\n        \"\"\"Update the item links, then post the item into the catalog.\n\n        Args:\n            collection_id (str): The collection id.\n            item (Item): STAC item to update and post\n            owner_id (str, optional): Collection owner ID. If missing, we use self.owner_id.\n            timeout (int): The timeout duration for the HTTP request.\n\n        Returns:\n            JSONResponse: The response of the request.\n        \"\"\"\n        # owner_id:collection_id\n        full_collection_id = self.full_collection_id(owner_id, collection_id)\n\n        # Get the collection from the catalog\n        collection = self.get_collection(collection_id, owner_id)\n\n        # Update the item  contents\n        collection.add_item(item)\n\n        # Post the item to the catalog\n        return requests.post(\n            f\"{self.href_catalog}/catalog/collections/{full_collection_id}/items\",\n            json=item.to_dict(),\n            **self.apikey_headers,\n            timeout=timeout,\n        )\n\n    def remove_item(  # type: ignore # pylint: disable=arguments-differ\n        self,\n        collection_id: str,\n        item_id: str,\n        owner_id: str | None = None,\n        timeout: int = TIMEOUT,\n    ) -&gt; Response:\n        \"\"\"Remove/delete an item from a collection.\n\n        Args:\n            collection_id (str): The collection id.\n            item_id (str): The item id.\n            owner_id (str, optional): Collection owner ID. If missing, we use self.owner_id.\n            timeout (int): The timeout duration for the HTTP request.\n\n        Returns:\n            JSONResponse: The response of the request.\n        \"\"\"\n        # owner_id:collection_id\n        full_collection_id = self.full_collection_id(owner_id, collection_id)\n\n        # Remove the collection from the server catalog\n        return requests.delete(\n            f\"{self.href_catalog}/catalog/collections/{full_collection_id}/items/{item_id}\",\n            **self.apikey_headers,\n            timeout=timeout,\n        )\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.apikey_config","title":"<code>apikey_config: dict</code>  <code>property</code>","text":"<p>Return the config from the keycloak account, associated to the api key.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.apikey_iam_roles","title":"<code>apikey_iam_roles: list[str]</code>  <code>property</code>","text":"<p>Return the IAM (Identity and Access Management) roles from the keycloak account, associated to the api key.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.apikey_user_login","title":"<code>apikey_user_login: str</code>  <code>property</code>","text":"<p>Return the user login from the keycloak account, associated to the api key.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.href_search","title":"<code>href_search: str</code>  <code>property</code>","text":"<p>Implemented by AuxipClient and CadipClient.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.href_staging","title":"<code>href_staging: str</code>  <code>property</code>","text":"<p>Implemented by AuxipClient and CadipClient.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.href_status","title":"<code>href_status: str</code>  <code>property</code>","text":"<p>Implemented by AuxipClient and CadipClient.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.__init__","title":"<code>__init__(rs_server_href, rs_server_api_key=None, owner_id=None, logger=None)</code>","text":"<p>RsClient class constructor.</p> Source code in <code>docs/rs-client-libraries/rs_client/rs_client.py</code> <pre><code>def __init__(\n    self,\n    rs_server_href: str | None,\n    rs_server_api_key: str | None = None,\n    owner_id: str | None = None,\n    logger: logging.Logger | None = None,\n):\n    \"\"\"RsClient class constructor.\"\"\"\n    self.rs_server_href: str | None = rs_server_href\n    self.rs_server_api_key: str | None = rs_server_api_key\n    self.owner_id: str = owner_id or \"\"\n    self.logger: logging.Logger = logger or Logging.default(__name__)\n\n    # Remove trailing / character(s) from the URL\n    if self.rs_server_href:\n        self.rs_server_href = self.rs_server_href.strip().rstrip(\"/\").strip()\n\n    # We are in local mode if the URL is undefined.\n    # Env vars are used instead to determine the different services URL.\n    self.local_mode = not bool(self.rs_server_href)\n\n    # If the API key is not set, we try to read it from the RSPY_APIKEY environment variable.\n    if not self.rs_server_api_key:\n        self.rs_server_api_key = os.getenv(\"RSPY_APIKEY\")  # None if the env var is not set\n\n    if (not self.local_mode) and (not self.rs_server_api_key):\n        raise RuntimeError(\"API key is mandatory for RS-Server authentication\")\n\n    # For HTTP request headers\n    self.apikey_headers: dict = (\n        {\"headers\": {APIKEY_HEADER: self.rs_server_api_key}} if self.rs_server_api_key else {}\n    )\n\n    # Determine automatically the owner id\n    if not self.owner_id:\n        # In local mode, we use the local system username\n        if self.local_mode:\n            self.owner_id = getpass.getuser()\n\n        # In hybrid/cluster mode, we retrieve the API key login\n        else:\n            self.owner_id = self.apikey_user_login\n\n    # Remove special characters\n    self.owner_id = re.sub(r\"[^a-zA-Z0-9]+\", \"\", self.owner_id)\n\n    if not self.owner_id:\n        raise RuntimeError(\"The owner ID is empty or only contains special characters\")\n\n    self.logger.debug(f\"Owner ID: {self.owner_id!r}\")\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.apikey_security","title":"<code>apikey_security()</code>","text":"<p>Check the api key validity. Cache an infinite (sys.maxsize) number of results for 120 seconds.</p> <p>Returns:</p> Type Description <code>tuple[list[str], dict, str]</code> <p>Tuple of (IAM roles, config, user login) information from the keycloak account, associated to the api key.</p> Source code in <code>docs/rs-client-libraries/rs_client/rs_client.py</code> <pre><code>@cached(cache=apikey_security_cache)\ndef apikey_security(self) -&gt; tuple[list[str], dict, str]:\n    \"\"\"\n    Check the api key validity. Cache an infinite (sys.maxsize) number of results for 120 seconds.\n\n    Returns:\n        Tuple of (IAM roles, config, user login) information from the keycloak account, associated to the api key.\n    \"\"\"\n\n    # In local mode, we have no API key, so return empty results\n    if self.local_mode:\n        return [], {}, \"\"\n\n    # self.logger.warning(\n    #     f\"TODO: use {self.rs_server_href}/apikeymanager/check/api_key instead, see: \"\n    #     \"https://pforge-exchange2.astrium.eads.net/jira/browse/RSPY-257\",\n    # )\n    # Does not work in hybrid mode for now because this URL is not exposed.\n    check_url = os.environ[\"RSPY_UAC_CHECK_URL\"]\n\n    # Request the API key manager, pass user-defined api key in http header\n    # check_url = f\"{self.rs_server_href}/apikeymanager/check/api_key\"\n    self.logger.debug(\"Call the API key manager\")\n    response = requests.get(check_url, **self.apikey_headers, timeout=TIMEOUT)\n\n    # Read the api key info\n    if response.ok:\n        contents = response.json()\n        # Note: for now, config is an empty dict\n        return contents[\"iam_roles\"], contents[\"config\"], contents[\"user_login\"]\n\n    # Try to read the response detail or error\n    try:\n        json = response.json()\n        if \"detail\" in json:\n            detail = json[\"detail\"]\n        else:\n            detail = json[\"error\"]\n\n    # If this fail, get the full response content\n    except Exception:  # pylint: disable=broad-exception-caught\n        detail = response.content\n\n    raise RuntimeError(f\"API key manager status code {response.status_code}: {detail}\")\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.get_auxip_client","title":"<code>get_auxip_client()</code>","text":"<p>Return an instance of the child class AuxipClient, with the same attributes as this \"self\" instance.</p> Source code in <code>docs/rs-client-libraries/rs_client/rs_client.py</code> <pre><code>def get_auxip_client(self) -&gt; \"AuxipClient\":  # type: ignore # noqa: F821\n    \"\"\"\n    Return an instance of the child class AuxipClient, with the same attributes as this \"self\" instance.\n    \"\"\"\n    from rs_client.auxip_client import (  # pylint: disable=import-outside-toplevel,cyclic-import\n        AuxipClient,\n    )\n\n    return AuxipClient(self.rs_server_href, self.rs_server_api_key, self.owner_id, self.logger)\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.get_cadip_client","title":"<code>get_cadip_client(station)</code>","text":"<p>Return an instance of the child class CadipClient, with the same attributes as this \"self\" instance.</p> <p>Parameters:</p> Name Type Description Default <code>station</code> <code>ECadipStation</code> <p>Cadip station</p> required Source code in <code>docs/rs-client-libraries/rs_client/rs_client.py</code> <pre><code>def get_cadip_client(\n    self,\n    station: ECadipStation,\n) -&gt; \"CadipClient\":  # type: ignore # noqa: F821\n    \"\"\"\n    Return an instance of the child class CadipClient, with the same attributes as this \"self\" instance.\n\n    Args:\n        station (ECadipStation): Cadip station\n    \"\"\"\n    from rs_client.cadip_client import (  # pylint: disable=import-outside-toplevel,cyclic-import\n        CadipClient,\n    )\n\n    return CadipClient(self.rs_server_href, self.rs_server_api_key, self.owner_id, station, self.logger)\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.get_stac_client","title":"<code>get_stac_client(*args, **kwargs)</code>","text":"<p>Return an instance of the child class StacClient, with the same attributes as this \"self\" instance.</p> Source code in <code>docs/rs-client-libraries/rs_client/rs_client.py</code> <pre><code>def get_stac_client(self, *args, **kwargs) -&gt; \"StacClient\":  # type: ignore # noqa: F821\n    \"\"\"\n    Return an instance of the child class StacClient, with the same attributes as this \"self\" instance.\n    \"\"\"\n    from rs_client.stac_client import (  # pylint: disable=import-outside-toplevel,cyclic-import\n        StacClient,\n    )\n\n    return StacClient.open(self.rs_server_href, self.rs_server_api_key, self.owner_id, self.logger, *args, **kwargs)\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.search_stations","title":"<code>search_stations(start_date, stop_date, limit=None, sortby=None, timeout=TIMEOUT)</code>","text":"<p>Retrieve a list of files from the specified endpoint.</p> <p>This function queries the specified endpoint to retrieve a list of files available in the station (CADIP, ADGS, LTA ...) that were collected by the satellite within the provided time range, starting from 'start_date' up to 'stop_date' (inclusive).</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>datetime</code> <p>The start date of the time range.</p> required <code>stop_date</code> <code>datetime</code> <p>The stop date of the time range.</p> required <code>timeout</code> <code>int</code> <p>The timeout duration for the HTTP request.</p> <code>TIMEOUT</code> <code>limit</code> <code>int</code> <p>The maximum number of results to return. Defaults to None.</p> <code>None</code> <code>sortby</code> <code>str</code> <p>The attribute to sort the results by. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>files</code> <code>list</code> <p>The list of files available at the station within the specified time range.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the endpoint can't be reached</p> Notes <ul> <li>This function queries the specified endpoint with a time range to retrieve information about available files.</li> <li>It constructs a payload with the start and stop dates in ISO 8601 format and sends a GET request to the endpoint.</li> <li>The response is expected to be a STAC Compatible formatted JSONResponse, containing information about  available files.</li> <li>The function converts a STAC FeatureCollection to a Python list.</li> </ul> Source code in <code>docs/rs-client-libraries/rs_client/rs_client.py</code> <pre><code>def search_stations(  # pylint: disable=too-many-arguments\n    self,\n    start_date: datetime,\n    stop_date: datetime,\n    limit: Union[int, None] = None,\n    sortby: Union[str, None] = None,\n    timeout: int = TIMEOUT,\n) -&gt; list:\n    \"\"\"Retrieve a list of files from the specified endpoint.\n\n    This function queries the specified endpoint to retrieve a list of files available in the\n    station (CADIP, ADGS, LTA ...) that were collected by the satellite within the provided time range,\n    starting from 'start_date' up to 'stop_date' (inclusive).\n\n    Args:\n        start_date (datetime): The start date of the time range.\n        stop_date (datetime): The stop date of the time range.\n        timeout (int): The timeout duration for the HTTP request.\n        limit (int, optional): The maximum number of results to return. Defaults to None.\n        sortby (str, optional): The attribute to sort the results by. Defaults to None.\n\n    Returns:\n        files (list): The list of files available at the station within the specified time range.\n\n    Raises:\n        RuntimeError: if the endpoint can't be reached\n\n    Notes:\n        - This function queries the specified endpoint with a time range to retrieve information about\n        available files.\n        - It constructs a payload with the start and stop dates in ISO 8601 format and sends a GET\n        request to the endpoint.\n        - The response is expected to be a STAC Compatible formatted JSONResponse, containing information about\n         available files.\n        - The function converts a STAC FeatureCollection to a Python list.\n    \"\"\"\n\n    payload = {\n        \"datetime\": start_date.strftime(DATETIME_FORMAT)\n        + \"/\"  # 2014-01-01T12:00:00Z/2023-12-30T12:00:00Z\",\n        + stop_date.strftime(DATETIME_FORMAT),\n    }\n    if limit:\n        payload[\"limit\"] = str(limit)\n    if sortby:\n        payload[\"sortby\"] = str(sortby)\n    try:\n        response = requests.get(\n            self.href_search,  # pylint: disable=no-member #\u00a0(\"self\" is AuxipClient or CadipClient)\n            params=payload,\n            timeout=timeout,\n            **self.apikey_headers,\n        )\n    except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:\n        self.logger.exception(f\"Could not get the response from the station search endpoint: {e}\")\n        raise RuntimeError(\"Could not get the response from the station search endpoint\") from e\n\n    files = []\n    try:\n        if response.ok:\n            for file_info in response.json()[\"features\"]:\n                files.append(file_info)\n        else:\n            self.logger.error(f\"Error: {response.status_code} : {response.json()}\")\n    except KeyError as e:\n        raise RuntimeError(\"Wrong format of search endpoint answer\") from e\n\n    return files\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.staging","title":"<code>staging(filename, s3_path='', tmp_download_path='', timeout=TIMEOUT)</code>","text":"<p>Stage a file for download.</p> <p>This method stages a file for download by sending a request to the staging endpoint with optional parameters for S3 path and temporary download path.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to be staged for download.</p> required <code>timeout</code> <code>int</code> <p>The timeout duration for the HTTP request.</p> <code>TIMEOUT</code> <code>s3_path</code> <code>str</code> <p>The S3 path where the file will be stored after download. Defaults to an empty string.</p> <code>''</code> <code>tmp_download_path</code> <code>str</code> <p>The temporary download path for the file. Defaults to an empty string.</p> <code>''</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If an error occurs while staging the file.</p> Source code in <code>docs/rs-client-libraries/rs_client/rs_client.py</code> <pre><code>def staging(self, filename: str, s3_path: str = \"\", tmp_download_path: str = \"\", timeout: int = TIMEOUT):\n    \"\"\"Stage a file for download.\n\n    This method stages a file for download by sending a request to the staging endpoint\n    with optional parameters for S3 path and temporary download path.\n\n    Args:\n        filename (str): The name of the file to be staged for download.\n        timeout (int): The timeout duration for the HTTP request.\n        s3_path (str, optional): The S3 path where the file will be stored after download.\n            Defaults to an empty string.\n        tmp_download_path (str, optional): The temporary download path for the file.\n            Defaults to an empty string.\n\n    Raises:\n        RuntimeError: If an error occurs while staging the file.\n\n    \"\"\"\n\n    # dictionary to be used for payload request\n    payload = {}\n    # some protections for the optional args\n    if s3_path:\n        payload[\"obs\"] = s3_path\n    if tmp_download_path:\n        payload[\"local\"] = tmp_download_path\n\n    # update the filename to be ingested\n    payload[\"name\"] = filename\n    try:\n        # logger.debug(f\"Calling  {endpoint} with payload {payload}\")\n        response = requests.get(\n            self.href_staging,  # pylint: disable=no-member #\u00a0(\"self\" is AuxipClient or CadipClient)\n            params=payload,\n            timeout=timeout,\n            **self.apikey_headers,\n        )\n        self.logger.debug(f\"Download start endpoint returned in {response.elapsed.total_seconds()}\")\n        if not response.ok:\n            self.logger.error(f\"The download endpoint returned error for file {filename}\\n\")\n            raise RuntimeError(f\"The download endpoint returned error for file {filename}\")\n    except (\n        requests.exceptions.RequestException,\n        requests.exceptions.Timeout,\n        requests.exceptions.ReadTimeout,\n    ) as e:\n        self.logger.exception(f\"Staging file exception for {filename}:\", e)\n        raise RuntimeError(f\"Staging file exception for {filename}\") from e\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.rs_client.RsClient.staging_status","title":"<code>staging_status(filename, timeout=TIMEOUT)</code>","text":"<p>Check the status of a file download from the specified rs-server endpoint.</p> <p>This function sends a GET request to the rs-server endpoint with the filename as a query parameter to retrieve the status of the file download. If the response is successful and contains a 'status' key in the JSON response, the function returns the corresponding download status enum value. If the response is not successful or does not contain the 'status' key, the function returns a FAILED status.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file for which to check the status.</p> required <code>timeout</code> <code>int</code> <p>The timeout duration for the HTTP request.</p> <code>TIMEOUT</code> <p>Returns:</p> Name Type Description <code>EDownloadStatus</code> <code>EDownloadStatus</code> <p>The download status enum value based on the response from the endpoint.</p> Source code in <code>docs/rs-client-libraries/rs_client/rs_client.py</code> <pre><code>def staging_status(self, filename, timeout: int = TIMEOUT) -&gt; EDownloadStatus:\n    \"\"\"Check the status of a file download from the specified rs-server endpoint.\n\n    This function sends a GET request to the rs-server endpoint with the filename as a query parameter\n    to retrieve the status of the file download. If the response is successful and contains a 'status'\n    key in the JSON response, the function returns the corresponding download status enum value. If the\n    response is not successful or does not contain the 'status' key, the function returns a FAILED status.\n\n    Args:\n        filename (str): The name of the file for which to check the status.\n        timeout (int): The timeout duration for the HTTP request.\n\n    Returns:\n        EDownloadStatus: The download status enum value based on the response from the endpoint.\n    \"\"\"\n\n    # TODO: check the status for a certain timeout if http returns NOK ?\n    try:\n        response = requests.get(\n            self.href_status,  # pylint: disable=no-member # (\"self\" is AuxipClient or CadipClient)\n            params={\"name\": filename},\n            timeout=timeout,\n            **self.apikey_headers,\n        )\n\n        eval_response = response.json()\n        if (\n            response.ok\n            and \"name\" in eval_response.keys()\n            and filename == eval_response[\"name\"]\n            and \"status\" in eval_response.keys()\n        ):\n            return EDownloadStatus(eval_response[\"status\"])\n\n    except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:\n        self.logger.exception(f\"Status endpoint exception: {e}\")\n\n    return EDownloadStatus.FAILED\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.auxip_client.AuxipClient.href_adgs","title":"<code>href_adgs: str</code>  <code>property</code>","text":"<p>Return the RS-Server ADGS URL hostname. This URL can be overwritten using the RSPY_HOST_ADGS env variable (used e.g. for local mode). Either it should just be the RS-Server URL.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.auxip_client.AuxipClient.href_search","title":"<code>href_search: str</code>  <code>property</code>","text":"<p>Return the RS-Server hostname and path where the ADGS search endpoint is deployed.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.auxip_client.AuxipClient.href_staging","title":"<code>href_staging: str</code>  <code>property</code>","text":"<p>Return the RS-Server hostname and path where the ADGS staging endpoint is deployed.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.auxip_client.AuxipClient.href_status","title":"<code>href_status: str</code>  <code>property</code>","text":"<p>Return the RS-Server hostname and path where the ADGS status endpoint is deployed.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.auxip_client.AuxipClient.station_name","title":"<code>station_name: str</code>  <code>property</code>","text":"<p>Return \"AUXIP\".</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.cadip_client.CadipClient.href_cadip","title":"<code>href_cadip: str</code>  <code>property</code>","text":"<p>Return the RS-Server CADIP URL hostname. This URL can be overwritten using the RSPY_HOST_CADIP env variable (used e.g. for local mode). Either it should just be the RS-Server URL.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.cadip_client.CadipClient.href_search","title":"<code>href_search: str</code>  <code>property</code>","text":"<p>Return the RS-Server hostname and path where the CADIP search endpoint is deployed.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.cadip_client.CadipClient.href_session","title":"<code>href_session: str</code>  <code>property</code>","text":"<p>Return the RS-Server hostname and path where the CADIP search session endpoint is deployed.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.cadip_client.CadipClient.href_staging","title":"<code>href_staging: str</code>  <code>property</code>","text":"<p>Return the RS-Server hostname and path where the CADIP staging endpoint is deployed.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.cadip_client.CadipClient.href_status","title":"<code>href_status: str</code>  <code>property</code>","text":"<p>Return the RS-Server hostname and path where the CADIP status endpoint is deployed.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.cadip_client.CadipClient.station_name","title":"<code>station_name: str</code>  <code>property</code>","text":"<p>Return the station name.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.cadip_client.CadipClient.__init__","title":"<code>__init__(rs_server_href, rs_server_api_key, owner_id, station, logger=None)</code>","text":"<p>CadipClient class constructor.</p> Source code in <code>docs/rs-client-libraries/rs_client/cadip_client.py</code> <pre><code>def __init__(  # pylint: disable=too-many-arguments\n    self,\n    rs_server_href: str | None,\n    rs_server_api_key: str | None,\n    owner_id: str,\n    station: ECadipStation,\n    logger: logging.Logger | None = None,\n):\n    \"\"\"CadipClient class constructor.\"\"\"\n    super().__init__(rs_server_href, rs_server_api_key, owner_id, logger)\n    try:\n        self.station: ECadipStation = ECadipStation[station]\n    except KeyError as e:\n        self.logger.exception(f\"There is no such CADIP station: {station}\")\n        raise RuntimeError(f\"There is no such CADIP station: {station}\") from e\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.cadip_client.CadipClient.search_sessions","title":"<code>search_sessions(session_ids=None, start_date=None, stop_date=None, platforms=None, timeout=TIMEOUT)</code>","text":"<p>Endpoint to retrieve list of sessions from any CADIP station.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>The timeout duration for the HTTP request.</p> <code>TIMEOUT</code> <code>session_ids</code> <code>list[str]</code> <p>Session identifiers (eg: [\"S1A_20170501121534062343\"] or [\"S1A_20170501121534062343, S1A_20240328185208053186\"])</p> <code>None</code> <code>start_date</code> <code>datetime</code> <p>Start date of the time interval</p> <code>None</code> <code>stop_date</code> <code>datetime</code> <p>Stop date of the time interval</p> <code>None</code> <code>platforms</code> <code>list[PlatformEnum]</code> <p>platform list</p> <code>None</code> Source code in <code>docs/rs-client-libraries/rs_client/cadip_client.py</code> <pre><code>def search_sessions(  # pylint: disable=too-many-arguments\n    self,\n    session_ids: list[str] | None = None,\n    start_date: datetime | None = None,\n    stop_date: datetime | None = None,\n    platforms: list[EPlatform] | None = None,\n    timeout: int = TIMEOUT,\n) -&gt; list[dict]:  # TODO return pystac.ItemCollection instead\n    \"\"\"Endpoint to retrieve list of sessions from any CADIP station.\n\n    Args:\n        timeout (int): The timeout duration for the HTTP request.\n        session_ids (list[str]): Session identifiers\n            (eg: [\"S1A_20170501121534062343\"] or [\"S1A_20170501121534062343, S1A_20240328185208053186\"])\n        start_date (datetime): Start date of the time interval\n        stop_date (datetime): Stop date of the time interval\n        platforms (list[PlatformEnum]): platform list\n    \"\"\"\n\n    payload = {}\n    if session_ids:\n        payload[\"id\"] = \",\".join(session_ids)\n    if platforms:\n        payload[\"platform\"] = \",\".join([platform.value for platform in platforms])\n    if start_date:\n        payload[\"start_date\"] = start_date.strftime(DATETIME_FORMAT)\n    if stop_date:\n        payload[\"stop_date\"] = stop_date.strftime(DATETIME_FORMAT)\n    try:\n        response = requests.get(\n            self.href_session,\n            params=payload,\n            timeout=timeout,\n            **self.apikey_headers,\n        )\n    except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:\n        self.logger.exception(f\"Could not get the response from the session search endpoint: {e}\")\n        raise RuntimeError(\"Could not get the response from the session search endpoint\") from e\n\n    sessions = []\n    try:\n        if response.ok:\n            for session_info in response.json()[\"features\"]:\n                sessions.append(session_info)\n        else:\n            self.logger.error(f\"Error: {response.status_code} : {response.json()}\")\n    except KeyError as e:\n        raise RuntimeError(\"Wrong format of session search endpoint answer\") from e\n\n    return sessions\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.stac_client.StacClient.href_catalog","title":"<code>href_catalog: str</code>  <code>property</code>","text":"<p>Return the RS-Server catalog URL hostname. This URL can be overwritten using the RSPY_HOST_CATALOG env variable (used e.g. for local mode). Either it should just be the RS-Server URL.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.stac_client.StacClient.__init__","title":"<code>__init__(id, description, title=None, stac_extensions=None, extra_fields=None, href=None, catalog_type=CatalogType.ABSOLUTE_PUBLISHED, strategy=None, *, modifier=None, **kwargs)</code>","text":"<p>Constructor. Called only by pystac. As an user: don't use this directly, call the open(...) class method instead or RsClient.get_stac_client(...).</p> Source code in <code>docs/rs-client-libraries/rs_client/stac_client.py</code> <pre><code>def __init__(  # pylint: disable=too-many-arguments,super-init-not-called # super-init is called in .open(...)\n    self,\n    id: str,  # pylint: disable=redefined-builtin\n    description: str,\n    title: Optional[str] = None,\n    stac_extensions: Optional[List[str]] = None,\n    extra_fields: Optional[Dict[str, Any]] = None,\n    href: Optional[str] = None,\n    catalog_type: CatalogType = CatalogType.ABSOLUTE_PUBLISHED,\n    strategy: Optional[HrefLayoutStrategy] = None,\n    *,\n    modifier: Optional[Callable[[Modifiable], None]] = None,\n    **kwargs: Dict[str, Any],\n):\n    \"\"\"\n    Constructor. Called only by pystac.\n    As an user: don't use this directly, call the open(...) class method instead or RsClient.get_stac_client(...).\n    \"\"\"\n\n    # Call manually the parent pystac Client constructor.\n    # The RsClient constructor will be called manually later.\n    Client.__init__(\n        self,\n        id=id,\n        description=description,\n        title=title,\n        stac_extensions=stac_extensions,\n        extra_fields=extra_fields,\n        href=href,\n        catalog_type=catalog_type,\n        strategy=strategy,\n        modifier=modifier,\n        **kwargs,\n    )\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.stac_client.StacClient.add_collection","title":"<code>add_collection(collection, add_public_license=True, owner_id=None, timeout=TIMEOUT)</code>","text":"<p>Update the collection links, then post the collection into the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>Collection</code> <p>STAC collection</p> required <code>add_public_license</code> <code>bool</code> <p>If True, add a public domain license field and link.</p> <code>True</code> <code>owner_id</code> <code>str</code> <p>Collection owner ID. If missing, we use self.owner_id.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The timeout duration for the HTTP request.</p> <code>TIMEOUT</code> <p>Returns:</p> Name Type Description <code>JSONResponse</code> <code>json</code> <p>The response of the request.</p> Source code in <code>docs/rs-client-libraries/rs_client/stac_client.py</code> <pre><code>def add_collection(\n    self,\n    collection: Collection,\n    add_public_license: bool = True,\n    owner_id: str | None = None,\n    timeout: int = TIMEOUT,\n) -&gt; Response:\n    \"\"\"Update the collection links, then post the collection into the catalog.\n\n    Args:\n        collection (Collection): STAC collection\n        add_public_license (bool): If True, add a public domain license field and link.\n        owner_id (str, optional): Collection owner ID. If missing, we use self.owner_id.\n        timeout (int): The timeout duration for the HTTP request.\n\n    Returns:\n        JSONResponse (json): The response of the request.\n    \"\"\"\n\n    full_owner_id = owner_id or self.owner_id\n\n    # Use owner_id:collection_id instead of just the collection ID, before adding the links,\n    # so the links contain the full owner_id:collection_id\n    short_collection_id = collection.id\n    full_collection_id = self.full_collection_id(owner_id, short_collection_id)\n    collection.id = full_collection_id\n\n    # Default description\n    if not collection.description:\n        collection.description = f\"This is the collection {short_collection_id} from user {full_owner_id}.\"\n\n    # Add the owner_id as an extra field\n    collection.extra_fields[\"owner\"] = full_owner_id\n\n    # Add public domain license\n    if add_public_license:\n        collection.license = \"public-domain\"\n        collection.add_link(\n            Link(\n                rel=RelType.LICENSE,\n                target=\"https://creativecommons.org/licenses/publicdomain/\",\n                title=\"public-domain\",\n            ),\n        )\n\n    # Update the links\n    self.add_child(collection)\n\n    # Restore the short collection_id at the root of the collection\n    collection.id = short_collection_id\n\n    # Check that the collection is compliant to STAC\n    collection.validate_all()\n\n    # Post the collection to the catalog\n    return requests.post(\n        f\"{self.href_catalog}/catalog/collections\",\n        json=collection.to_dict(),\n        **self.apikey_headers,\n        timeout=timeout,\n    )\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.stac_client.StacClient.add_item","title":"<code>add_item(collection_id, item, owner_id=None, timeout=TIMEOUT)</code>","text":"<p>Update the item links, then post the item into the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>collection_id</code> <code>str</code> <p>The collection id.</p> required <code>item</code> <code>Item</code> <p>STAC item to update and post</p> required <code>owner_id</code> <code>str</code> <p>Collection owner ID. If missing, we use self.owner_id.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The timeout duration for the HTTP request.</p> <code>TIMEOUT</code> <p>Returns:</p> Name Type Description <code>JSONResponse</code> <code>Response</code> <p>The response of the request.</p> Source code in <code>docs/rs-client-libraries/rs_client/stac_client.py</code> <pre><code>def add_item(  # type: ignore # pylint: disable=arguments-renamed\n    self,\n    collection_id: str,\n    item: Item,\n    owner_id: str | None = None,\n    timeout: int = TIMEOUT,\n) -&gt; Response:\n    \"\"\"Update the item links, then post the item into the catalog.\n\n    Args:\n        collection_id (str): The collection id.\n        item (Item): STAC item to update and post\n        owner_id (str, optional): Collection owner ID. If missing, we use self.owner_id.\n        timeout (int): The timeout duration for the HTTP request.\n\n    Returns:\n        JSONResponse: The response of the request.\n    \"\"\"\n    # owner_id:collection_id\n    full_collection_id = self.full_collection_id(owner_id, collection_id)\n\n    # Get the collection from the catalog\n    collection = self.get_collection(collection_id, owner_id)\n\n    # Update the item  contents\n    collection.add_item(item)\n\n    # Post the item to the catalog\n    return requests.post(\n        f\"{self.href_catalog}/catalog/collections/{full_collection_id}/items\",\n        json=item.to_dict(),\n        **self.apikey_headers,\n        timeout=timeout,\n    )\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.stac_client.StacClient.full_collection_id","title":"<code>full_collection_id(owner_id, collection_id)</code>","text":"<p>Return the full collection name as: : <p>Parameters:</p> Name Type Description Default <code>owner_id</code> <code>str</code> <p>Collection owner ID. If missing, we use self.owner_id.</p> required <code>collection_id</code> <code>str</code> <p>Collection name</p> required Source code in <code>docs/rs-client-libraries/rs_client/stac_client.py</code> <pre><code>def full_collection_id(self, owner_id: str | None, collection_id: str):\n    \"\"\"\n    Return the full collection name as: &lt;owner_id&gt;:&lt;collection_id&gt;\n\n    Args:\n        owner_id (str): Collection owner ID. If missing, we use self.owner_id.\n        collection_id (str): Collection name\n    \"\"\"\n    return f\"{owner_id or self.owner_id}:{collection_id}\"\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.stac_client.StacClient.get_collection","title":"<code>get_collection(collection_id, owner_id=None)</code>  <code>cached</code>","text":"<p>Get the requested collection as : Source code in <code>docs/rs-client-libraries/rs_client/stac_client.py</code> <pre><code>@lru_cache()\ndef get_collection(self, collection_id: str, owner_id: str | None = None) -&gt; Union[Collection, CollectionClient]:\n    \"\"\"Get the requested collection as &lt;owner_id&gt;:&lt;collection_id&gt;\"\"\"\n    full_collection_id = self.full_collection_id(owner_id, collection_id)\n    return Client.get_collection(self, full_collection_id)\n</code></pre> <p>This client allows you to interact with the STAC service, making it easy to search, retrieve, and manage spatio-temporal asset catalog data.</p> <p>For detailed usage instructions and examples for each client, please refer to the respective sections.</p>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.stac_client.StacClient.open","title":"<code>open(rs_server_href, rs_server_api_key, owner_id, logger=None, headers=None, parameters=None, ignore_conformance=None, modifier=None, request_modifier=None, stac_io=None, timeout=TIMEOUT)</code>  <code>classmethod</code>","text":"<p>Create a new StacClient instance.</p> Source code in <code>docs/rs-client-libraries/rs_client/stac_client.py</code> <pre><code>@classmethod\ndef open(  # type: ignore  # pylint: disable=arguments-renamed, too-many-arguments\n    cls,\n    # RsClient parameters\n    rs_server_href: str | None,\n    rs_server_api_key: str | None,\n    owner_id: str | None,\n    logger: logging.Logger | None = None,\n    # pystac Client parameters\n    headers: Optional[Dict[str, str]] = None,\n    parameters: Optional[Dict[str, Any]] = None,\n    ignore_conformance: Optional[bool] = None,\n    modifier: Optional[Callable[[Modifiable], None]] = None,\n    request_modifier: Optional[Callable[[Request], Union[Request, None]]] = None,\n    stac_io: Optional[StacApiIO] = None,\n    timeout: Optional[Timeout] = TIMEOUT,\n) -&gt; StacClient:\n    \"\"\"Create a new StacClient instance.\"\"\"\n\n    if rs_server_api_key:\n        if headers is None:\n            headers = {}\n        headers[APIKEY_HEADER] = rs_server_api_key\n\n    client: StacClient = super().open(  # type: ignore\n        cls.__href_catalog(rs_server_href) + \"/catalog/\",\n        headers,\n        parameters,\n        ignore_conformance,\n        modifier,\n        request_modifier,\n        stac_io,\n        timeout,\n    )\n\n    # Manual call to the parent RsClient constructor\n    RsClient.__init__(\n        client,\n        rs_server_href=rs_server_href,\n        rs_server_api_key=rs_server_api_key,\n        owner_id=owner_id,\n        logger=logger,\n    )\n\n    return client\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.stac_client.StacClient.remove_collection","title":"<code>remove_collection(collection_id, owner_id=None, timeout=TIMEOUT)</code>","text":"<p>Remove/delete a collection from the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>collection_id</code> <code>str</code> <p>The collection id.</p> required <code>owner_id</code> <code>str</code> <p>Collection owner ID. If missing, we use self.owner_id.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The timeout duration for the HTTP request.</p> <code>TIMEOUT</code> <p>Returns:</p> Name Type Description <code>JSONResponse</code> <code>Response</code> <p>The response of the request.</p> Source code in <code>docs/rs-client-libraries/rs_client/stac_client.py</code> <pre><code>def remove_collection(\n    self,\n    collection_id: str,\n    owner_id: str | None = None,\n    timeout: int = TIMEOUT,\n) -&gt; Response:\n    \"\"\"Remove/delete a collection from the catalog.\n\n    Args:\n        collection_id (str): The collection id.\n        owner_id (str, optional): Collection owner ID. If missing, we use self.owner_id.\n        timeout (int): The timeout duration for the HTTP request.\n\n    Returns:\n        JSONResponse: The response of the request.\n    \"\"\"\n    # owner_id:collection_id\n    full_collection_id = self.full_collection_id(owner_id, collection_id)\n\n    # Remove the collection from the \"child\" links of the local catalog instance\n    collection_link = f\"{self.self_href.rstrip('/')}/collections/{full_collection_id}\"\n    self.links = [\n        link for link in self.links if not ((link.rel == pystac.RelType.CHILD) and (link.href == collection_link))\n    ]\n\n    # We need to clear the cache for this and parent \"get_collection\" methods\n    # because their returned value must be updated.\n    self.get_collection.cache_clear()\n    Client.get_collection.cache_clear()\n\n    # Remove the collection from the server catalog\n    return requests.delete(\n        f\"{self.href_catalog}/catalog/collections/{full_collection_id}\",\n        **self.apikey_headers,\n        timeout=timeout,\n    )\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client/#rs_client.stac_client.StacClient.remove_item","title":"<code>remove_item(collection_id, item_id, owner_id=None, timeout=TIMEOUT)</code>","text":"<p>Remove/delete an item from a collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_id</code> <code>str</code> <p>The collection id.</p> required <code>item_id</code> <code>str</code> <p>The item id.</p> required <code>owner_id</code> <code>str</code> <p>Collection owner ID. If missing, we use self.owner_id.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The timeout duration for the HTTP request.</p> <code>TIMEOUT</code> <p>Returns:</p> Name Type Description <code>JSONResponse</code> <code>Response</code> <p>The response of the request.</p> Source code in <code>docs/rs-client-libraries/rs_client/stac_client.py</code> <pre><code>def remove_item(  # type: ignore # pylint: disable=arguments-differ\n    self,\n    collection_id: str,\n    item_id: str,\n    owner_id: str | None = None,\n    timeout: int = TIMEOUT,\n) -&gt; Response:\n    \"\"\"Remove/delete an item from a collection.\n\n    Args:\n        collection_id (str): The collection id.\n        item_id (str): The item id.\n        owner_id (str, optional): Collection owner ID. If missing, we use self.owner_id.\n        timeout (int): The timeout duration for the HTTP request.\n\n    Returns:\n        JSONResponse: The response of the request.\n    \"\"\"\n    # owner_id:collection_id\n    full_collection_id = self.full_collection_id(owner_id, collection_id)\n\n    # Remove the collection from the server catalog\n    return requests.delete(\n        f\"{self.href_catalog}/catalog/collections/{full_collection_id}/items/{item_id}\",\n        **self.apikey_headers,\n        timeout=timeout,\n    )\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client_common/","title":"Common source code","text":"<p>The following source code is used in both packages rs_client and rs_workflows: </p> <p>We can't pass a RsClient instance to the workflow because it causes (de-)serialization issues. So instead we pass the instance parameters, that will be used to recreate a new instance from the workflow.</p> <p>TODO: also (de-)serialize the pystac_client.Client.open(...) parameters ?</p> <p>Attributes:</p> Name Type Description <code>cls</code> <code>Class</code> <p>RsClient child class type</p> <code>rs_server_href</code> <code>str</code> <p>RS-Server URL. In local mode, pass None.</p> <code>rs_server_api_key</code> <code>str</code> <p>API key for RS-Server authentication.</p> <code>owner_id</code> <code>str</code> <p>ID of the owner of the STAC catalog collections (no special characters allowoed).</p> <code>station</code> <code>ECadipStation</code> <p>Cadip station (if applicable)</p> Source code in <code>docs/rs-client-libraries/rs_common/serialization.py</code> <pre><code>class RsClientSerialization:  # pylint: disable=too-few-public-methods\n    \"\"\"\n    We can't pass a RsClient instance to the workflow because it causes (de-)serialization issues.\n    So instead we pass the instance parameters, that will be used to recreate a new instance\n    from the workflow.\n\n    TODO: also (de-)serialize the pystac_client.Client.open(...) parameters ?\n\n    Attributes:\n        cls (Class): RsClient child class type\n        rs_server_href (str): RS-Server URL. In local mode, pass None.\n        rs_server_api_key (str): API key for RS-Server authentication.\n        owner_id (str): ID of the owner of the STAC catalog collections (no special characters allowoed).\n        station (ECadipStation): Cadip station (if applicable)\n\n    \"\"\"\n\n    def __init__(self, client: RsClient):\n        \"\"\"\n        Serialize a RsClient instance.\n\n        Args:\n            client (RsClient): RsClient instance\n        \"\"\"\n\n        # Save the parameters, except the logging (which should not be (de-)serialized neither.\n        self.cls: Type[RsClient] = type(client)\n        self.rs_server_href: str | None = client.rs_server_href\n        self.rs_server_api_key: str | None = client.rs_server_api_key\n        self.owner_id: str | None = client.owner_id\n\n        self.station: ECadipStation | None = None\n        if self.cls == CadipClient:\n            self.station = cast(CadipClient, client).station\n\n    def deserialize(self, logger=None) -&gt; RsClient:\n        \"\"\"\n        Recreate a new RsClient instance from the serialized parameters.\n\n        Return:\n            client (RsClient): RsClient instance\n            logger (logging.Logger): Logging instance.\n        \"\"\"\n\n        # Init parent class\n        client = RsClient(self.rs_server_href, self.rs_server_api_key, self.owner_id, logger)\n\n        # Return child instance\n        if self.cls == AuxipClient:\n            return client.get_auxip_client()\n        if self.cls == CadipClient:\n            return client.get_cadip_client(self.station)  # type: ignore\n        if self.cls == StacClient:\n            return client.get_stac_client()\n        raise ValueError(f\"Unknown RsClient type: {self.cls}\")\n\n    @property\n    def get_flow_name(self):\n        \"\"\"Return a unique flow name for Prefect.\"\"\"\n        return f\"{self.owner_id}_{datetime.now(timezone.utc).strftime(DATETIME_FORMAT)}\"\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client_common/#rs_common.serialization.RsClientSerialization.get_flow_name","title":"<code>get_flow_name</code>  <code>property</code>","text":"<p>Return a unique flow name for Prefect.</p>"},{"location":"generate_src_doc/rs-client/rs_client_common/#rs_common.serialization.RsClientSerialization.__init__","title":"<code>__init__(client)</code>","text":"<p>Serialize a RsClient instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>RsClient</code> <p>RsClient instance</p> required Source code in <code>docs/rs-client-libraries/rs_common/serialization.py</code> <pre><code>def __init__(self, client: RsClient):\n    \"\"\"\n    Serialize a RsClient instance.\n\n    Args:\n        client (RsClient): RsClient instance\n    \"\"\"\n\n    # Save the parameters, except the logging (which should not be (de-)serialized neither.\n    self.cls: Type[RsClient] = type(client)\n    self.rs_server_href: str | None = client.rs_server_href\n    self.rs_server_api_key: str | None = client.rs_server_api_key\n    self.owner_id: str | None = client.owner_id\n\n    self.station: ECadipStation | None = None\n    if self.cls == CadipClient:\n        self.station = cast(CadipClient, client).station\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_client_common/#rs_common.serialization.RsClientSerialization.deserialize","title":"<code>deserialize(logger=None)</code>","text":"<p>Recreate a new RsClient instance from the serialized parameters.</p> Return <p>client (RsClient): RsClient instance logger (logging.Logger): Logging instance.</p> Source code in <code>docs/rs-client-libraries/rs_common/serialization.py</code> <pre><code>def deserialize(self, logger=None) -&gt; RsClient:\n    \"\"\"\n    Recreate a new RsClient instance from the serialized parameters.\n\n    Return:\n        client (RsClient): RsClient instance\n        logger (logging.Logger): Logging instance.\n    \"\"\"\n\n    # Init parent class\n    client = RsClient(self.rs_server_href, self.rs_server_api_key, self.owner_id, logger)\n\n    # Return child instance\n    if self.cls == AuxipClient:\n        return client.get_auxip_client()\n    if self.cls == CadipClient:\n        return client.get_cadip_client(self.station)  # type: ignore\n    if self.cls == StacClient:\n        return client.get_stac_client()\n    raise ValueError(f\"Unknown RsClient type: {self.cls}\")\n</code></pre>"},{"location":"generate_src_doc/rs-client/rs_workflows/","title":"Worflows","text":"<p>Following is the documentation generated from the source code of the already implemented workflows</p> <p>Example</p> <p>Staging</p> <p>S1 L0</p>"},{"location":"generate_src_doc/rs-client/s1_l0/","title":"S1 L0","text":"<p>Prefect flow for processing a S1 L0 product</p>"},{"location":"generate_src_doc/rs-client/s1_l0/#rs_workflows.s1_l0.PrefectS1L0FlowConfig","title":"<code>PrefectS1L0FlowConfig</code>","text":"<p>Configuration for Prefect flow related to S1 Level 0 data.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/s1_l0.py</code> <pre><code>class PrefectS1L0FlowConfig:  # pylint: disable=too-few-public-methods, too-many-instance-attributes\n    \"\"\"Configuration for Prefect flow related to S1 Level 0 data.\"\"\"\n\n    def __init__(  # pylint: disable=too-many-arguments\n        self,\n        stac_client: StacClient,\n        url_dpr: str,\n        mission: str,\n        cadip_session_id: str,\n        product_types: list,\n        adgs_files: list,\n        s3_path: str,\n        temp_s3_path: str,\n    ):\n        \"\"\"\n        Initialize the PrefectS1L0FlowConfig object with provided parameters.\n\n        Args:\n            stac_client (StacClient): StacClient instance\n            url_dpr (str): The URL of the dpr endpoint\n            mission (str): The mission name.\n            cadip_session_id (str): The CADIP session ID.\n            product_types (list): The list with the products types to be processed by the DPR\n            adgs_files (list): ADGS files in the catalog.\n            s3_path (str): The S3 path.\n            temp_s3_path (str): The temporary S3 path.\n        \"\"\"\n        self.stac_client: StacClient | None = None  # don't save this instance\n        self.rs_client_serialization = RsClientSerialization(stac_client)  # save the serialization parameters instead\n        self.url_dpr = url_dpr\n        self.mission = mission\n        self.cadip_session_id = cadip_session_id\n        self.product_types = product_types\n        self.adgs_files = adgs_files\n        self.s3_path = s3_path\n        self.temp_s3_path = temp_s3_path\n</code></pre>"},{"location":"generate_src_doc/rs-client/s1_l0/#rs_workflows.s1_l0.PrefectS1L0FlowConfig.__init__","title":"<code>__init__(stac_client, url_dpr, mission, cadip_session_id, product_types, adgs_files, s3_path, temp_s3_path)</code>","text":"<p>Initialize the PrefectS1L0FlowConfig object with provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>stac_client</code> <code>StacClient</code> <p>StacClient instance</p> required <code>url_dpr</code> <code>str</code> <p>The URL of the dpr endpoint</p> required <code>mission</code> <code>str</code> <p>The mission name.</p> required <code>cadip_session_id</code> <code>str</code> <p>The CADIP session ID.</p> required <code>product_types</code> <code>list</code> <p>The list with the products types to be processed by the DPR</p> required <code>adgs_files</code> <code>list</code> <p>ADGS files in the catalog.</p> required <code>s3_path</code> <code>str</code> <p>The S3 path.</p> required <code>temp_s3_path</code> <code>str</code> <p>The temporary S3 path.</p> required Source code in <code>docs/rs-client-libraries/rs_workflows/s1_l0.py</code> <pre><code>def __init__(  # pylint: disable=too-many-arguments\n    self,\n    stac_client: StacClient,\n    url_dpr: str,\n    mission: str,\n    cadip_session_id: str,\n    product_types: list,\n    adgs_files: list,\n    s3_path: str,\n    temp_s3_path: str,\n):\n    \"\"\"\n    Initialize the PrefectS1L0FlowConfig object with provided parameters.\n\n    Args:\n        stac_client (StacClient): StacClient instance\n        url_dpr (str): The URL of the dpr endpoint\n        mission (str): The mission name.\n        cadip_session_id (str): The CADIP session ID.\n        product_types (list): The list with the products types to be processed by the DPR\n        adgs_files (list): ADGS files in the catalog.\n        s3_path (str): The S3 path.\n        temp_s3_path (str): The temporary S3 path.\n    \"\"\"\n    self.stac_client: StacClient | None = None  # don't save this instance\n    self.rs_client_serialization = RsClientSerialization(stac_client)  # save the serialization parameters instead\n    self.url_dpr = url_dpr\n    self.mission = mission\n    self.cadip_session_id = cadip_session_id\n    self.product_types = product_types\n    self.adgs_files = adgs_files\n    self.s3_path = s3_path\n    self.temp_s3_path = temp_s3_path\n</code></pre>"},{"location":"generate_src_doc/rs-client/s1_l0/#rs_workflows.s1_l0.build_eopf_triggering_yaml","title":"<code>build_eopf_triggering_yaml(cadip_files, adgs_files, product_types, temp_s3_path)</code>","text":"<p>Builds the EOPF triggering YAML file using CADIP and ADGS file paths.</p> <p>Parameters:</p> Name Type Description Default <code>cadip_files</code> <code>dict</code> <p>CADIP files metadata.</p> required <code>adgs_files</code> <code>dict</code> <p>ADGS files metadata.</p> required <code>product_types</code> <code>list</code> <p>The types for the output products</p> required <code>temp_s3_path</code> <code>str</code> <p>Temporary S3 path.</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>dict</code> <p>The generated YAML template with updated inputs and I/O products, or None if unsuccessful.</p> <p>Raises:     FileNotFoundError: If the YAML template file is not found.     yaml.YAMLError: If there is an error loading the YAML template file.     IOError: If there is an input/output error while accessing the YAML template file.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/s1_l0.py</code> <pre><code>@task\ndef build_eopf_triggering_yaml(cadip_files: dict, adgs_files: dict, product_types: list, temp_s3_path: str):\n    \"\"\"Builds the EOPF triggering YAML file using CADIP and ADGS file paths.\n\n    Args:\n        cadip_files (dict): CADIP files metadata.\n        adgs_files (dict): ADGS files metadata.\n        product_types (list): The types for the output products\n        temp_s3_path (str): Temporary S3 path.\n\n    Returns:\n        response (dict): The generated YAML template with updated inputs and I/O products, or None if unsuccessful.\n    Raises:\n        FileNotFoundError: If the YAML template file is not found.\n        yaml.YAMLError: If there is an error loading the YAML template file.\n        IOError: If there is an input/output error while accessing the YAML template file.\n    \"\"\"\n    logger = Logging.default(LOGGER_NAME)\n    logger.debug(\"Task build_eopf_triggering_yaml STARTED\")\n    # Load YAML template\n    try:\n        with open(CONFIG_DIR / YAML_TEMPLATE_FILE, encoding=\"utf-8\") as yaml_file:\n            yaml_template = yaml.safe_load(yaml_file)\n    except FileNotFoundError as e:\n        logger.exception(f\"Could not find the YAML template file: {e}\")\n        return None\n    except yaml.YAMLError as e:\n        logger.exception(f\"Could not load the YAML template file: {e}\")\n        return None\n    except IOError as e:\n        logger.exception(f\"Could not find the YAML template file: {e}\")\n        return None\n\n    # Extract paths for CADIP and ADGS files\n    cadip_paths = [file_prop[\"assets\"][\"file\"][\"alternate\"][\"s3\"][\"href\"] for file_prop in cadip_files]\n    adgs_paths = [file_prop[\"assets\"][\"file\"][\"alternate\"][\"s3\"][\"href\"] for file_prop in adgs_files]\n    # create the dictionaries to insert within the yaml template\n\n    # Update the YAML template with inputs and I/O products\n    yaml_template[\"workflow\"][0][\"inputs\"], yaml_template[\"I/O\"][\"inputs_products\"] = gen_payload_inputs(\n        cadip_paths,\n        adgs_paths,\n    )\n\n    # Update the YAML  with the outpurs\n    yaml_template[\"workflow\"][0][\"parameters\"][\"product_types\"] = product_types\n    yaml_template[\"workflow\"][0][\"outputs\"], yaml_template[\"I/O\"][\"output_products\"] = gen_payload_outputs(\n        product_types,\n        temp_s3_path,\n    )\n\n    logger.debug(\"Task build_eopf_triggering_yaml FINISHED\")\n    return yaml_template\n</code></pre>"},{"location":"generate_src_doc/rs-client/s1_l0/#rs_workflows.s1_l0.create_cql2_filter","title":"<code>create_cql2_filter(properties, op='and')</code>","text":"<p>Create a CQL2 filter based on provided properties, see: https://pystac-client.readthedocs.io/en/stable/tutorials/cql2-filter.html</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>dict</code> <p>Dictionary containing field-value pairs for filtering.</p> required <code>op</code> <code>str</code> <p>Logical operator to combine filter conditions. Defaults to \"and\".</p> <code>'and'</code> <p>Returns:</p> Name Type Description <code>ret</code> <code>dict</code> <p>CQL2 filter.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/s1_l0.py</code> <pre><code>def create_cql2_filter(properties: dict, op: str = \"and\"):\n    \"\"\"\n    Create a CQL2 filter based on provided properties,\n    see: https://pystac-client.readthedocs.io/en/stable/tutorials/cql2-filter.html\n\n    Args:\n        properties (dict): Dictionary containing field-value pairs for filtering.\n        op (str, optional): Logical operator to combine filter conditions. Defaults to \"and\".\n\n    Returns:\n        ret (dict): CQL2 filter.\n    \"\"\"\n    args = [{\"op\": \"=\", \"args\": [{\"property\": field}, value]} for field, value in properties.items()]\n    # args.append(\"collecttion=test_user_s1_chunk\")\n    # this filter is used for getting the files for one CADIP seesion id\n    # There can't be more than 60 files for a cadip session, so a hardcoded limit of 1000 seems\n    # to be ok for the time being\n    return {\"filter-lang\": \"cql2-json\", \"limit\": \"1000\", \"filter\": {\"op\": op, \"args\": args}}\n</code></pre>"},{"location":"generate_src_doc/rs-client/s1_l0/#rs_workflows.s1_l0.gen_payload_inputs","title":"<code>gen_payload_inputs(cadu_list, adgs_list)</code>","text":"<p>Generate payload inputs for the EOPF triggering YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>cadu_list</code> <code>list</code> <p>List of CADU file paths.</p> required <code>adgs_list</code> <code>list</code> <p>List of ADGS file paths.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>[list, list]</code> <p>A tuple containing the composer dictionary and the YAML content list.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/s1_l0.py</code> <pre><code>def gen_payload_inputs(cadu_list: list, adgs_list: list):\n    \"\"\"Generate payload inputs for the EOPF triggering YAML file.\n\n    Args:\n        cadu_list (list): List of CADU file paths.\n        adgs_list (list): List of ADGS file paths.\n\n    Returns:\n        tuple ([list, list]): A tuple containing the composer dictionary and the YAML content list.\n    \"\"\"\n    input_body = []\n    composer = []\n\n    def add_input(file_list, prefix, start_cnt):\n        nonlocal composer\n        nonlocal input_body\n\n        for cnt, file in enumerate(file_list, start=start_cnt):\n            file_id = f\"in{cnt}\"\n            input_id = f\"{prefix}{cnt}\"\n            composer.append({file_id: input_id})\n            yaml_template = {\"id\": input_id, \"path\": file, \"store_type\": file.split(\".\")[-1], \"store_params\": {}}\n            input_body.append(yaml_template)\n\n    add_input(cadu_list, \"CADU\", 1)\n    add_input(adgs_list, \"ADGS\", len(cadu_list) + 1)\n\n    return composer, input_body\n</code></pre>"},{"location":"generate_src_doc/rs-client/s1_l0/#rs_workflows.s1_l0.gen_payload_outputs","title":"<code>gen_payload_outputs(product_types, temp_s3_path)</code>","text":"<p>Generate payload outputs for product types.</p> <p>This function generates composer and output_body payloads for a list of product types, each associated with a temporary S3 path.</p> <p>Parameters:</p> Name Type Description Default <code>product_types</code> <code>list</code> <p>A list of product types.</p> required <code>temp_s3_path</code> <code>str</code> <p>The temporary S3 path where the products will be stored.</p> required <p>Returns:</p> Name Type Description <code>Tuple</code> <code>[list, list]</code> <p>A tuple containing composer and output_body payloads.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/s1_l0.py</code> <pre><code>def gen_payload_outputs(product_types, temp_s3_path: str):\n    \"\"\"Generate payload outputs for product types.\n\n    This function generates composer and output_body payloads for a list of product types,\n    each associated with a temporary S3 path.\n\n    Args:\n        product_types (list): A list of product types.\n        temp_s3_path (str): The temporary S3 path where the products will be stored.\n\n    Returns:\n        Tuple ([list, list]): A tuple containing composer and output_body payloads.\n\n    \"\"\"\n    composer = []\n    output_body = []\n\n    temp_s3_path = temp_s3_path.rstrip(\"/\")\n    for typecnt, ptype in enumerate(product_types):\n        composer.append({f\"out{typecnt}\": ptype})\n        output_body.append(\n            {\n                \"id\": ptype,\n                \"path\": f\"{temp_s3_path}/{ptype}/\",\n                \"type\": \"folder|zip\",\n                \"store_type\": \"zarr\",\n                \"store_params\": {},\n            },\n        )\n\n    return composer, output_body\n</code></pre>"},{"location":"generate_src_doc/rs-client/s1_l0/#rs_workflows.s1_l0.get_adgs_catalog_data","title":"<code>get_adgs_catalog_data(stac_client, collection, files)</code>","text":"<p>Prefect task to retrieve ADGS catalog data for specific files in a collection.</p> <p>This task retrieves ADGS catalog data by sending a request to the ADGS catalog search endpoint with filters based on the collection and file IDs.</p> <p>Parameters:</p> Name Type Description Default <code>stac_client</code> <code>StacClient</code> <p>The StacClient instance for accessing the ADGS catalog.</p> required <code>collection</code> <code>str</code> <p>The name of the collection.</p> required <code>files</code> <code>list</code> <p>A list of file IDs to retrieve from the catalog.</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>dict</code> <p>The ADGS catalog data if retrieval is successful, otherwise None.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/s1_l0.py</code> <pre><code>@task\ndef get_adgs_catalog_data(\n    stac_client: StacClient,  # NOTE: maybe use RsClientSerialization instead\n    collection: str,\n    files: list,\n):\n    \"\"\"Prefect task to retrieve ADGS catalog data for specific files in a collection.\n\n    This task retrieves ADGS catalog data by sending a request to the ADGS\n    catalog search endpoint with filters based on the collection and file IDs.\n\n    Args:\n        stac_client (StacClient): The StacClient instance for accessing the ADGS catalog.\n        collection (str): The name of the collection.\n        files (list): A list of file IDs to retrieve from the catalog.\n\n    Returns:\n        response (dict): The ADGS catalog data if retrieval is successful, otherwise None.\n\n    \"\"\"\n\n    logger = stac_client.logger\n    logger.debug(\"Task get_adgs_catalog_data STARTED\")\n\n    _filter = {\n        \"op\": \"and\",\n        \"args\": [\n            {\"op\": \"=\", \"args\": [{\"property\": \"collection\"}, f\"{stac_client.owner_id}_{collection}\"]},\n            {\"op\": \"=\", \"args\": [{\"property\": \"owner\"}, stac_client.owner_id]},\n            {\"op\": \"in\", \"args\": [{\"property\": \"id\"}, files]},\n        ],\n    }\n    try:\n        search = stac_client.search(filter=_filter)\n        data = list(search.items_as_dicts())\n        logger.debug(\"Task get_adgs_catalog_data FINISHED\")\n        return data\n    except NotImplementedError as e:\n        logger.exception(\"Search exception caught: %s\", e)\n        return None\n</code></pre>"},{"location":"generate_src_doc/rs-client/s1_l0/#rs_workflows.s1_l0.get_cadip_catalog_data","title":"<code>get_cadip_catalog_data(stac_client, collection, session_id)</code>","text":"<p>Prefect task to retrieve CADIP catalog data for a specific collection and session ID.</p> <p>This task retrieves CADIP catalog data by sending a request to the CADIP catalog search endpoint with a filter based on the collection and session ID.</p> <p>Parameters:</p> Name Type Description Default <code>stac_client</code> <code>StacClient</code> <p>The StacClient instance for accessing the CADIP catalog.</p> required <code>collection</code> <code>str</code> <p>The name of the collection.</p> required <code>session_id</code> <code>str</code> <p>The session ID associated with the collection.</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>dict</code> <p>The CADIP catalog data if retrieval is successful, otherwise None.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/s1_l0.py</code> <pre><code>@task\ndef get_cadip_catalog_data(\n    stac_client: StacClient,  # NOTE: maybe use RsClientSerialization instead\n    collection: str,\n    session_id: str,\n):\n    \"\"\"Prefect task to retrieve CADIP catalog data for a specific collection and session ID.\n\n    This task retrieves CADIP catalog data by sending a request to the CADIP\n    catalog search endpoint with a filter based on the collection and session ID.\n\n    Args:\n        stac_client (StacClient): The StacClient instance for accessing the CADIP catalog.\n        collection (str): The name of the collection.\n        session_id (str): The session ID associated with the collection.\n\n    Returns:\n        response (dict): The CADIP catalog data if retrieval is successful, otherwise None.\n\n    \"\"\"\n\n    logger = stac_client.logger\n    logger.debug(\"Task get_cadip_catalog_data STARTED\")\n\n    _filter = create_cql2_filter({\"collection\": f\"{stac_client.owner_id}_{collection}\", \"cadip:session_id\": session_id})\n\n    try:\n        search = stac_client.search(filter=_filter)\n        data = list(search.items_as_dicts())\n        logger.debug(\"Task get_cadip_catalog_data FINISHED\")\n        return data\n    except NotImplementedError as e:\n        logger.exception(\"Search exception caught: %s\", e)\n        return None\n</code></pre>"},{"location":"generate_src_doc/rs-client/s1_l0/#rs_workflows.s1_l0.get_yaml_outputs","title":"<code>get_yaml_outputs(template)</code>","text":"<p>Extract the paths of YAML outputs from the template.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>dict</code> <p>The YAML template.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of paths for YAML outputs.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/s1_l0.py</code> <pre><code>def get_yaml_outputs(template: dict):\n    \"\"\"Extract the paths of YAML outputs from the template.\n\n    Args:\n        template (dict): The YAML template.\n\n    Returns:\n        list (list): A list of paths for YAML outputs.\n    \"\"\"\n    return [out[\"path\"] for out in template[\"I/O\"][\"output_products\"]]\n</code></pre>"},{"location":"generate_src_doc/rs-client/s1_l0/#rs_workflows.s1_l0.s1_l0_flow","title":"<code>s1_l0_flow(config)</code>","text":"<p>Constructs a Prefect Flow for Sentinel-1 Level 0 processing.</p> <p>This flow oversees the execution of tasks involved in processing Sentinel-1 Level 0 data. It coordinates sequential and parallel tasks, including retrieving catalog data concurrently from CADIP and ADGS stations, generating YAML configuration based on the retrieved data, initiating the processing based on the YAML configuration, updating the STAC catalog according to processing results, and publishing the processed files contingent upon catalog updates.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PrefectS1L0FlowConfig</code> <p>Configuration for the flow.</p> required Source code in <code>docs/rs-client-libraries/rs_workflows/s1_l0.py</code> <pre><code>@flow(\n    flow_run_name=\"{config.rs_client_serialization.get_flow_name}\",\n    task_runner=DaskTaskRunner(cluster_kwargs={\"n_workers\": 2, \"threads_per_worker\": 1}),\n)\ndef s1_l0_flow(config: PrefectS1L0FlowConfig):  # pylint: disable=too-many-locals\n    \"\"\"Constructs a Prefect Flow for Sentinel-1 Level 0 processing.\n\n    This flow oversees the execution of tasks involved in processing Sentinel-1 Level 0 data. It coordinates sequential\n    and parallel tasks, including retrieving catalog data concurrently from CADIP and ADGS stations, generating YAML\n    configuration based on the retrieved data, initiating the processing based on the YAML configuration, updating the\n    STAC catalog according to processing results, and publishing the processed files contingent upon catalog updates.\n\n    Args:\n        config (PrefectS1L0FlowConfig): Configuration for the flow.\n\n    \"\"\"\n    logger = get_prefect_logger(LOGGER_NAME)\n\n    # Deserialize the RsClient instance\n    config.stac_client = config.rs_client_serialization.deserialize(logger)  # type: ignore\n\n    # Check the product types\n    ok_types = [\"S1SEWRAW\", \"S1SIWRAW\", \"S1SSMRAW\", \"S1SWVRAW\"]\n    for product_type in config.product_types:\n        if product_type not in ok_types:\n            raise RuntimeError(\n                f\"Unrecognized product type: {product_type}\\n\" \"It should be one of: \\n\" + \"\\n\".join(ok_types),\n            )\n\n    # TODO: the station for CADIP should come as an input\n    cadip_collection = create_collection_name(config.mission, ECadipStation[\"CADIP\"])\n    adgs_collection = create_collection_name(config.mission, AUXIP_STATION)\n    logger.debug(f\"Collections: {cadip_collection} | {adgs_collection}\")\n\n    # gather the data for cadip session id\n    logger.debug(\"Starting task get_cadip_catalog_data\")\n    cadip_catalog_data = get_cadip_catalog_data.submit(\n        config.stac_client,  # type: ignore # NOTE: maybe use RsClientSerialization instead\n        cadip_collection,\n        config.cadip_session_id,\n    )\n    # logger.debug(f\"cadip_catalog_data = {cadip_catalog_data} | {cadip_catalog_data.result()}\")\n    logger.debug(\"Starting task get_adgs_catalog_data\")\n    adgs_catalog_data = get_adgs_catalog_data.submit(\n        config.stac_client,  # type: ignore # NOTE: maybe use RsClientSerialization instead\n        adgs_collection,\n        config.adgs_files,\n    )\n\n    # the previous tasks may be launched in parallel. The next task depends on the results from these previous tasks\n    if not cadip_catalog_data.result():\n        logger.error(f\"No Cadip files were found in the catalog for Cadip session ID: {config.cadip_session_id!r}\")\n        return\n\n    if not adgs_catalog_data.result():\n        logger.error(  # pylint: disable=logging-not-lazy\n            \"None of these Auxip files were found in the catalog:\\n\" + \"\\n\".join(config.adgs_files),\n        )\n        return\n\n    logger.debug(\"Starting task build_eopf_triggering_yaml \")\n    yaml_dpr_input = build_eopf_triggering_yaml.submit(\n        cadip_catalog_data.result(),\n        adgs_catalog_data.result(),\n        config.product_types,\n        config.temp_s3_path,\n        wait_for=[cadip_catalog_data, adgs_catalog_data],\n    )\n    # the output product list should be :S1SEWRAW S1SIWRAW S1SSMRAW S1SWVRAW\n    # according to the jira story\n\n    # this task depends on the result from the previous task\n    logger.debug(\"Starting task start_dpr\")\n    files_stac = start_dpr.submit(config.url_dpr, yaml_dpr_input.result(), wait_for=[yaml_dpr_input])\n\n    if not files_stac.result():\n        logger.error(\"DPR did not processed anything\")\n        return\n\n    # The final collection where the processed products will be stored is assumed to be named \"mission_dpr\".\n    # However, it might be necessary to allow the users to input a specific collection name\n    # if they wish to do so. There is no established guide in the US for this matter.\n    collection_name = f\"{config.mission}_dpr\"\n    now = datetime.now()\n    config.stac_client.add_collection(\n        Collection(\n            id=collection_name,\n            description=None,  # rs-client will provide a default description for us\n            extent=Extent(\n                spatial=SpatialExtent(bboxes=[-180.0, -90.0, 180.0, 90.0]),\n                temporal=TemporalExtent([now, now]),\n            ),\n        ),\n        timeout=CATALOG_REQUEST_TIMEOUT,\n    )\n\n    fin_res = []\n    for output_product in get_yaml_outputs(yaml_dpr_input.result()):\n        matching_stac = next(\n            (d for d in files_stac.result() if d[\"stac_discovery\"][\"properties\"][\"eopf:type\"] in output_product),\n            None,\n        )\n\n        matching_stac[\"stac_discovery\"][\"assets\"] = {\"file\": {\"href\": \"\"}}\n\n        # Update catalog (it moves the products from temporary bucket to the final one)\n        logger.info(\"Starting task update_stac_catalog\")\n        fin_res.append(\n            (\n                update_stac_catalog.submit(\n                    config.stac_client,  # NOTE: maybe use RsClientSerialization instead\n                    collection_name,\n                    matching_stac[\"stac_discovery\"],\n                    output_product,\n                    wait_for=[files_stac],\n                ),\n                output_product,\n            ),\n        )\n    logger.debug(\"All tasks submitted\")\n    for tsk in fin_res:\n        if tsk[0].result():\n            logger.info(f\"File well published: {tsk[1]}\")\n        else:\n            logger.error(f\"File could not be published: {tsk[1]}\")\n\n    logger.info(\"S1 L0 prefect flow finished\")\n</code></pre>"},{"location":"generate_src_doc/rs-client/s1_l0/#rs_workflows.s1_l0.start_dpr","title":"<code>start_dpr(dpr_endpoint, yaml_dpr_input)</code>","text":"<p>Starts the DPR processing with the given YAML input.</p> <p>Parameters:</p> Name Type Description Default <code>dpr_endpoint</code> <code>str</code> <p>The endpoint of the DPR processor</p> required <code>yaml_dpr_input</code> <code>dict</code> <p>The YAML input for DPR processing.</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>dict</code> <p>The response JSON from the DPR simulator if successful, else None.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/s1_l0.py</code> <pre><code>@task\ndef start_dpr(dpr_endpoint: str, yaml_dpr_input: dict):\n    \"\"\"Starts the DPR processing with the given YAML input.\n\n    Args:\n        dpr_endpoint (str): The endpoint of the DPR processor\n        yaml_dpr_input (dict): The YAML input for DPR processing.\n\n    Returns:\n        response (dict): The response JSON from the DPR simulator if successful, else None.\n    \"\"\"\n    logger = Logging.default(LOGGER_NAME)\n    logger.debug(\"Task start_dpr STARTED\")\n    logger.info(\"Faking dpr processing with the following input file:\")\n    logger.debug(yaml.dump(yaml_dpr_input))\n    try:\n        response = requests.post(\n            dpr_endpoint.rstrip(\"/\") + \"/run\",\n            json=yaml.safe_load(yaml.dump(yaml_dpr_input)),\n            timeout=DPR_PROCESSING_TIMEOUT,\n        )\n    except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:\n        logger.exception(\"Calling the dpr simulater resulted in exception: %s\", e)\n        return None\n\n    if int(response.status_code) != 200:\n        logger.error(f\"The dpr simulator endpoint failed with status code: {response.status_code}\")\n        return None\n\n    logger.debug(\"DPR processor results: \\n\\n\")\n    for attr in response.json():\n        logger.debug(json.dumps(attr, indent=2))\n    logger.debug(\"Task start_dpr FINISHED\")\n    return response.json()\n</code></pre>"},{"location":"generate_src_doc/rs-client/staging/","title":"Staging","text":"<p>Common workflows for general usage</p>"},{"location":"generate_src_doc/rs-client/staging/#rs_workflows.staging.PrefectCommonConfig","title":"<code>PrefectCommonConfig</code>","text":"<p>Common configuration for Prefect tasks and flows. Base class for configuration used in Prefect task and flow used in staging the files from different stations (cadip, adgs...)</p> <p>Attributes:</p> Name Type Description <code>rs_client</code> <code>AuxipClient | CadipClient</code> <p>The client for accessing the Auxip or Cadip service.</p> <code>mission</code> <code>str</code> <p>The mission identifier.</p> <code>tmp_download_path</code> <code>str</code> <p>The local path where temporary files will be downloaded.</p> <code>s3_path</code> <code>str</code> <p>The S3 bucket path where the files will be stored.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/staging.py</code> <pre><code>class PrefectCommonConfig:  # pylint: disable=too-few-public-methods, too-many-instance-attributes,\n    \"\"\"Common configuration for Prefect tasks and flows.\n    Base class for configuration used in Prefect task and flow\n    used in staging the files from different stations (cadip, adgs...)\n\n    Attributes:\n        rs_client (AuxipClient | CadipClient): The client for accessing the Auxip or Cadip service.\n        mission (str): The mission identifier.\n        tmp_download_path (str): The local path where temporary files will be downloaded.\n        s3_path (str): The S3 bucket path where the files will be stored.\n    \"\"\"\n\n    def __init__(\n        self,\n        rs_client: AuxipClient | CadipClient,\n        mission,\n        tmp_download_path,\n        s3_path,\n    ):\n        self.rs_client: AuxipClient | CadipClient | None = None  # don't save this instance\n        self.rs_client_serialization = RsClientSerialization(rs_client)  # save the serialization parameters instead\n        self.mission: str = mission\n        self.tmp_download_path: str = tmp_download_path\n        self.s3_path: str = s3_path\n</code></pre>"},{"location":"generate_src_doc/rs-client/staging/#rs_workflows.staging.PrefectFlowConfig","title":"<code>PrefectFlowConfig</code>","text":"<p>               Bases: <code>PrefectCommonConfig</code></p> <p>Configuration class for Prefect flow.</p> <p>This class inherits the PrefectCommonConfig and represents the configuration for a Prefect flow</p> <p>see :py:class:`PrefectCommonConfig`</p> Name Type Description <code>max_workers</code> <code>int</code> <p>The maximum number of workers for the Prefect flow.</p> <code>start_datetime</code> <code>datetime</code> <p>The start datetime of the files that the station should return</p> <code>stop_datetime</code> <code>datetime</code> <p>The stop datetime of the files that the station should return</p> <code>limit</code> <p>The limit for the number of the files in the list retrieved from the ADGS/CADIP station (optional).</p> Source code in <code>docs/rs-client-libraries/rs_workflows/staging.py</code> <pre><code>class PrefectFlowConfig(PrefectCommonConfig):  # pylint: disable=too-few-public-methods\n    \"\"\"Configuration class for Prefect flow.\n\n    This class inherits the PrefectCommonConfig and represents the configuration for a\n    Prefect flow\n\n    Attributes: see :py:class:`PrefectCommonConfig`\n        max_workers (int): The maximum number of workers for the Prefect flow.\n        start_datetime (datetime): The start datetime of the files that the station should return\n        stop_datetime (datetime): The stop datetime of the files that the station should return\n        limit: The limit for the number of the files in the list retrieved from the ADGS/CADIP station (optional).\n    \"\"\"\n\n    def __init__(  # pylint: disable=too-many-arguments\n        self,\n        rs_client: AuxipClient | CadipClient,\n        mission,\n        tmp_download_path,\n        s3_path,\n        max_workers,\n        start_datetime,\n        stop_datetime,\n        limit=None,\n    ):\n        \"\"\"\n        Initialize the PrefectFlowConfig object with provided parameters.\n        \"\"\"\n        super().__init__(rs_client, mission, tmp_download_path, s3_path)\n        self.max_workers: int = max_workers\n        self.start_datetime: datetime = start_datetime\n        self.stop_datetime: datetime = stop_datetime\n        self.limit = limit\n</code></pre>"},{"location":"generate_src_doc/rs-client/staging/#rs_workflows.staging.PrefectFlowConfig.__init__","title":"<code>__init__(rs_client, mission, tmp_download_path, s3_path, max_workers, start_datetime, stop_datetime, limit=None)</code>","text":"<p>Initialize the PrefectFlowConfig object with provided parameters.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/staging.py</code> <pre><code>def __init__(  # pylint: disable=too-many-arguments\n    self,\n    rs_client: AuxipClient | CadipClient,\n    mission,\n    tmp_download_path,\n    s3_path,\n    max_workers,\n    start_datetime,\n    stop_datetime,\n    limit=None,\n):\n    \"\"\"\n    Initialize the PrefectFlowConfig object with provided parameters.\n    \"\"\"\n    super().__init__(rs_client, mission, tmp_download_path, s3_path)\n    self.max_workers: int = max_workers\n    self.start_datetime: datetime = start_datetime\n    self.stop_datetime: datetime = stop_datetime\n    self.limit = limit\n</code></pre>"},{"location":"generate_src_doc/rs-client/staging/#rs_workflows.staging.PrefectTaskConfig","title":"<code>PrefectTaskConfig</code>","text":"<p>               Bases: <code>PrefectCommonConfig</code></p> <p>Configuration class for Prefect tasks.</p> <p>This class extends the <code>PrefectCommonConfig</code> class with additional attributes specific to Prefect tasks. It includes the configuration for the task, such as the files to be processed by the task and the maximum number of retries allowed.</p> <p>see :py:class:`PrefectCommonConfig`</p> Name Type Description <code>task_files_stac</code> <code>List[Dict]</code> <p>A list of dictionaries containing information about the files to be processed                           by the task. This info is in STAC format</p> <code>max_retries</code> <code>int</code> <p>The maximum number of retries allowed for the task.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/staging.py</code> <pre><code>class PrefectTaskConfig(PrefectCommonConfig):  # pylint: disable=too-few-public-methods\n    \"\"\"Configuration class for Prefect tasks.\n\n    This class extends the `PrefectCommonConfig` class with additional attributes\n    specific to Prefect tasks. It includes the configuration for the task, such as the\n    files to be processed by the task and the maximum number of retries allowed.\n\n    Attributes: see :py:class:`PrefectCommonConfig`\n        task_files_stac (List[Dict]): A list of dictionaries containing information about the files to be processed\n                                      by the task. This info is in STAC format\n        max_retries (int): The maximum number of retries allowed for the task.\n\n    \"\"\"\n\n    def __init__(  # pylint: disable=too-many-arguments\n        self,\n        rs_client: AuxipClient | CadipClient,\n        mission,\n        tmp_download_path,\n        s3_path,\n        task_files_stac,\n        max_retries: int = 3,\n    ):\n        \"\"\"\n        Initialize the PrefectTaskConfig object with provided parameters.\n        \"\"\"\n\n        super().__init__(rs_client, mission, tmp_download_path, s3_path)\n        self.task_files_stac: list[dict] = task_files_stac\n        self.max_retries: int = max_retries\n</code></pre>"},{"location":"generate_src_doc/rs-client/staging/#rs_workflows.staging.PrefectTaskConfig.__init__","title":"<code>__init__(rs_client, mission, tmp_download_path, s3_path, task_files_stac, max_retries=3)</code>","text":"<p>Initialize the PrefectTaskConfig object with provided parameters.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/staging.py</code> <pre><code>def __init__(  # pylint: disable=too-many-arguments\n    self,\n    rs_client: AuxipClient | CadipClient,\n    mission,\n    tmp_download_path,\n    s3_path,\n    task_files_stac,\n    max_retries: int = 3,\n):\n    \"\"\"\n    Initialize the PrefectTaskConfig object with provided parameters.\n    \"\"\"\n\n    super().__init__(rs_client, mission, tmp_download_path, s3_path)\n    self.task_files_stac: list[dict] = task_files_stac\n    self.max_retries: int = max_retries\n</code></pre>"},{"location":"generate_src_doc/rs-client/staging/#rs_workflows.staging.create_collection_name","title":"<code>create_collection_name(mission, station)</code>","text":"<p>Create the name of a catalog collection</p> <p>This function constructs and returns a specific name for the catalog collection. For ADGS station type should be \"mission_name\"_aux For CADIP stations types should be \"mission_name\"_chunk</p> <p>For other values, a RuntimeError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>mission</code> <code>str</code> <p>The name of the mission.</p> required <code>station</code> <code>str</code> <p>The type of station . Supported values are \"AUX\" and \"CADIP\", \"INS\", \"MPS\", \"MTI\", \"NSG\", \"SGS\".</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the collection</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the provided station type is not supported.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/staging.py</code> <pre><code>def create_collection_name(mission: str, station: str) -&gt; str:\n    \"\"\"Create the name of a catalog collection\n\n    This function constructs and returns a specific name for the catalog collection.\n    For ADGS station type should be \"mission_name\"_aux\n    For CADIP stations types should be \"mission_name\"_chunk\n\n    For other values, a RuntimeError is raised.\n\n    Args:\n        mission (str): The name of the mission.\n        station (str): The type of station . Supported\n            values are \"AUX\" and \"CADIP\", \"INS\", \"MPS\", \"MTI\", \"NSG\", \"SGS\".\n\n    Returns:\n        str (str): The name of the collection\n\n    Raises:\n        RuntimeError: If the provided station type is not supported.\n\n    \"\"\"\n    if station == AUXIP_STATION:\n        return f\"{mission}_aux\"\n    # check CADIP\n    try:\n        ECadipStation(station)\n        return f\"{mission}_chunk\"\n    except ValueError as e:\n        raise RuntimeError(f\"Unknown station: {station}\") from e\n</code></pre>"},{"location":"generate_src_doc/rs-client/staging/#rs_workflows.staging.filter_unpublished_files","title":"<code>filter_unpublished_files(stac_client, collection_name, files_stac)</code>","text":"<p>Checks for unpublished files in the STAC catalog.</p> <p>This function takes a list of files and checks if they are already published in a specified STAC (SpatioTemporal Asset Catalog) collection. It returns a list of files that are not yet published.</p> <p>Parameters:</p> Name Type Description Default <code>stac_client</code> <code>StacClient</code> <p>An instance of <code>StacClient</code> to interact with the STAC catalog.</p> required <code>collection_name</code> <code>str</code> <p>The name of the collection in which the search is performed.</p> required <code>files_stac</code> <code>list of dict</code> <p>A list of files to be checked for publication. Each file is represented as a dictionary with at least an \"id\" key.</p> required <p>Returns:</p> Type Description <code>list</code> <p>list of dict: A list of files that are not yet published in the catalog. Each file is represented as a</p> <code>list</code> <p>dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rs_client = StacClient(...)\n&gt;&gt;&gt; collection_name = \"example_collection\"\n&gt;&gt;&gt; files_stac = [{\"id\": \"file1.raw\"}, {\"id\": \"file2.raw\"}]\n&gt;&gt;&gt; unpublished_files = filter_unpublished_files(rs_client, collection_name, files_stac)\n&gt;&gt;&gt; print(unpublished_files)\n[{\"id\": \"file1.raw\"}]\n</code></pre> Source code in <code>docs/rs-client-libraries/rs_workflows/staging.py</code> <pre><code>@task\ndef filter_unpublished_files(\n    stac_client: StacClient,  # NOTE: maybe use RsClientSerialization instead\n    collection_name: str,\n    files_stac: list,\n) -&gt; list:\n    \"\"\"Checks for unpublished files in the STAC catalog.\n\n    This function takes a list of files and checks if they are already published in a specified\n    STAC (SpatioTemporal Asset Catalog) collection. It returns a list of files that are not yet published.\n\n    Parameters:\n        stac_client (StacClient): An instance of `StacClient` to interact with the STAC catalog.\n        collection_name (str): The name of the collection in which the search is performed.\n        files_stac (list of dict): A list of files to be checked for publication. Each file is represented as a\n            dictionary with at least an \"id\" key.\n\n    Returns:\n        list of dict: A list of files that are not yet published in the catalog. Each file is represented as a\n        dictionary.\n\n    Examples:\n        &gt;&gt;&gt; rs_client = StacClient(...)\n        &gt;&gt;&gt; collection_name = \"example_collection\"\n        &gt;&gt;&gt; files_stac = [{\"id\": \"file1.raw\"}, {\"id\": \"file2.raw\"}]\n        &gt;&gt;&gt; unpublished_files = filter_unpublished_files(rs_client, collection_name, files_stac)\n        &gt;&gt;&gt; print(unpublished_files)\n        [{\"id\": \"file1.raw\"}]\n    \"\"\"\n\n    # Get files IDs (no duplicate IDs)\n    ids = set()\n    for fs in files_stac:\n        ids.add(str(fs[\"id\"]))\n    ids = list(ids)  # type: ignore # set to list conversion\n\n    # For searching, we need to prefix our collection name by &lt;owner_id&gt;_\n    owner_collection = f\"{stac_client.owner_id}_{collection_name}\"\n\n    # Search using a CQL2 filter, see: https://pystac-client.readthedocs.io/en/stable/tutorials/cql2-filter.html\n    filter_ = {\n        \"op\": \"and\",\n        \"args\": [\n            {\"op\": \"=\", \"args\": [{\"property\": \"collection\"}, owner_collection]},\n            {\"op\": \"=\", \"args\": [{\"property\": \"owner\"}, stac_client.owner_id]},\n            {\"op\": \"in\", \"args\": [{\"property\": \"id\"}, ids]},\n        ],\n    }\n    try:\n        search = stac_client.search(filter=filter_)\n        existing = list(search.items_as_dicts())\n\n    # In case of any error, try to ingest everything anyway\n    except NotImplementedError as e:\n        stac_client.logger.exception(\"Search exception caught: %s\", e)\n        return files_stac\n\n    # Only keep the files that do not alreay exist in the catalog\n    existing_ids = [item[\"id\"] for item in existing]\n    return [file_stac for file_stac in files_stac if file_stac[\"id\"] not in existing_ids]\n</code></pre>"},{"location":"generate_src_doc/rs-client/staging/#rs_workflows.staging.get_prefect_logger","title":"<code>get_prefect_logger(general_logger_name)</code>","text":"<p>It returns the Prefect logger. If this can't be taken due to the missing Prefect context (i.e. the flow/task is executed as a single function, from Pytest for example), the general logger is returned</p> <p>Parameters:</p> Name Type Description Default <code>general_logger_name</code> <code>str</code> <p>The name of the general logger if Prefect logger can't be returned.</p> required <p>Returns:</p> Name Type Description <code>logger</code> <code>logger</code> <p>A Prefect logger instance or a general logger instance.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/staging.py</code> <pre><code>def get_prefect_logger(general_logger_name):\n    \"\"\"\n    It returns the Prefect logger. If this can't be taken due to the missing\n    Prefect context (i.e. the flow/task is executed as a single function, from Pytest for example),\n    the general logger is returned\n\n    Args:\n        general_logger_name (str): The name of the general logger if Prefect logger can't be returned.\n\n    Returns:\n        logger (logger): A Prefect logger instance or a general logger instance.\n    \"\"\"\n    try:\n        logger = get_run_logger()\n        logger.setLevel(SET_PREFECT_LOGGING_LEVEL)\n    except exceptions.MissingContextError:\n        logger = Logging.default(general_logger_name)\n        logger.warning(\"Could not get the prefect logger due to missing context. Using the general one\")\n    return logger\n</code></pre>"},{"location":"generate_src_doc/rs-client/staging/#rs_workflows.staging.staging","title":"<code>staging(config)</code>","text":"<p>Prefect task function to stage (=download/ingest) files from a specified station.</p> <p>This task function stages files for processing by calling the appropriate rs-server endpoint for each requested file. It monitors the status of the file until it is completely staged before initiating the next stage request.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PrefectTaskConfig</code> <p>The configuration object containing the necessary parameters                         for staging the files.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>[dict]</code> <p>A list containing information about files that FAILED to be staged. The files within this         list don't appear in the catalog as published</p> Source code in <code>docs/rs-client-libraries/rs_workflows/staging.py</code> <pre><code>@task\ndef staging(config: PrefectTaskConfig):\n    \"\"\"Prefect task function to stage (=download/ingest) files from a specified station.\n\n    This task function stages files for processing by calling the appropriate\n    rs-server endpoint for each requested file. It monitors the status of the file until it is\n    completely staged before initiating the next stage request.\n\n    Args:\n        config (PrefectTaskConfig): The configuration object containing the necessary parameters\n                                    for staging the files.\n\n    Returns:\n        List ([dict]): A list containing information about files that FAILED to be staged. The files within this\n                    list don't appear in the catalog as published\n\n    \"\"\"\n\n    logger = get_prefect_logger(\"task_dwn\")\n\n    # Deserialize the RsClient instance\n    rs_client: AuxipClient | CadipClient = config.rs_client_serialization.deserialize(logger)  # type: ignore\n\n    # list with failed files\n    failed_files = config.task_files_stac.copy()\n\n    downloaded_files_indices = []\n\n    # Call the download endpoint for each requested file\n    for i, file_stac in enumerate(config.task_files_stac):\n        # update the filename to be ingested\n        try:\n            rs_client.staging(file_stac[\"id\"], config.s3_path, config.tmp_download_path, timeout=ENDPOINT_TIMEOUT)\n        except RuntimeError as e:\n            # TODO: Continue? Stop ?\n            logger.exception(f\"Could not stage file %s. Exception: {e}\")\n            continue\n\n        # monitor the status of the file until it is completely downloaded before initiating the next download request\n        status = rs_client.staging_status(file_stac[\"id\"], ENDPOINT_TIMEOUT)\n        # just for the demo the timeout is hardcoded, it should be otherwise defined somewhere in the configuration\n        timeout = DOWNLOAD_FILE_TIMEOUT  # 6 minutes\n        while status in [EDownloadStatus.NOT_STARTED, EDownloadStatus.IN_PROGRESS] and timeout &gt; 0:\n            logger.info(\n                \"The download progress for file %s is %s\",\n                file_stac[\"id\"],\n                status.name,\n            )\n            time.sleep(2)\n            timeout -= 2\n            status = rs_client.staging_status(file_stac[\"id\"], timeout=ENDPOINT_TIMEOUT)\n        if status == EDownloadStatus.DONE:\n            logger.info(\"File %s has been properly downloaded...\", file_stac[\"id\"])\n            # TODO: either move the code from filter_unpublished_files to RsClient\n            # or use the new PgstacClient ?\n            if update_stac_catalog.fn(\n                rs_client.get_stac_client(),  # NOTE: maybe use RsClientSerialization instead\n                create_collection_name(config.mission, rs_client.station_name),\n                file_stac,\n                config.s3_path,\n            ):\n                logger.info(f\"File well published: {file_stac['id']}\\n\")\n                # save the index of the well ingested file\n                downloaded_files_indices.append(i)\n            else:\n                logger.error(f\"Could not publish file: {file_stac['id']}\")\n        else:\n            logger.error(\n                \"Error in downloading the file %s (status %s). Timeout was %s from %s\\n\",\n                file_stac[\"id\"],\n                status.name,\n                timeout,\n                DOWNLOAD_FILE_TIMEOUT,\n            )\n    # remove all the well ingested files\n    # return only those that failed\n    for idx in sorted(downloaded_files_indices, reverse=True):\n        del failed_files[idx]\n\n    return failed_files\n</code></pre>"},{"location":"generate_src_doc/rs-client/staging/#rs_workflows.staging.staging_flow","title":"<code>staging_flow(config)</code>","text":"<p>Prefect flow for staging (downloading/ingesting) files from a station.</p> <p>This flow orchestrates the download process by obtaining the list of files from the search endpoint (provided station), splitting the list into tasks based on the number of workers, and submitting tasks for ingestion.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PrefectFlowConfig</code> <p>Configuration object containing details about the download flow, such as start and stop datetime, limit, rs_client, mission, temporary download path, S3 path, and max_workers.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the flow execution is successful, False otherwise.</p> <p>Raises:</p> Type Description <code>None</code> <p>This function does not raise any exceptions.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/staging.py</code> <pre><code>@flow(\n    flow_run_name=\"{config.rs_client_serialization.get_flow_name}\",\n    task_runner=DaskTaskRunner(cluster_kwargs={\"n_workers\": 5, \"threads_per_worker\": 1}),\n)\ndef staging_flow(config: PrefectFlowConfig):\n    \"\"\"\n    Prefect flow for staging (downloading/ingesting) files from a station.\n\n    This flow orchestrates the download process by obtaining the list of files from the search endpoint (provided\n    station), splitting the list into tasks based on the number of workers, and submitting tasks for ingestion.\n\n    Args:\n        config (PrefectFlowConfig): Configuration object containing details about the download flow, such as\n            start and stop datetime, limit, rs_client, mission, temporary download path, S3 path, and max_workers.\n\n    Returns:\n        bool (bool): True if the flow execution is successful, False otherwise.\n\n    Raises:\n        None: This function does not raise any exceptions.\n    \"\"\"\n    # get the Prefect logger\n    logger = get_prefect_logger(\"flow_dwn\")\n    logger.info(f\"The staging flow is starting. Received workers:{config.max_workers}\")\n    try:\n        # Deserialize the RsClient instance\n        rs_client: AuxipClient | CadipClient = config.rs_client_serialization.deserialize(logger)  # type: ignore\n\n        # get the list with files from the search endpoint\n        try:\n            files_stac = rs_client.search_stations(\n                config.start_datetime,\n                config.stop_datetime,\n                config.limit,\n                timeout=SEARCH_ENDPOINT_TIMEOUT,\n            )\n        except RuntimeError as e:\n            logger.exception(f\"Unable to get the list with files for staging: {e}\")\n            return False\n\n        # check if the list with files returned from the station is not empty\n        if len(files_stac) == 0:\n            logger.warning(\n                f\"The station {rs_client.station_name} did not return any \\\nelement for time interval {config.start_datetime} - {config.stop_datetime}\",\n            )\n            return True\n        # create the collection name\n\n        # filter those that are already existing\n\n        files_stac = filter_unpublished_files(  # type: ignore\n            rs_client.get_stac_client(),  # NOTE: maybe use RsClientSerialization instead\n            create_collection_name(config.mission, rs_client.station_name),\n            files_stac,\n            wait_for=[files_stac],\n        )\n        # distribute the filenames evenly in a number of lists equal with\n        # the minimum between number of runners and files to be downloaded\n        try:\n            tasks_files_stac = [\n                x.tolist() for x in [*np.array_split(files_stac, min(config.max_workers, len(files_stac)))]\n            ]\n        except ValueError:\n            logger.warning(\"No task will be started, the requested number of tasks is 0 !\")\n            tasks_files_stac = []\n        logger.info(\"List with files to be downloaded (after filtering against the catalog)\")\n        for f in files_stac:\n            logger.info(\"      %s\", f[\"id\"])\n\n        for files_stac in tasks_files_stac:\n            staging.submit(\n                PrefectTaskConfig(\n                    rs_client,\n                    config.mission,\n                    config.tmp_download_path,\n                    config.s3_path,\n                    files_stac,\n                ),\n            )\n\n    except (RuntimeError, TypeError) as e:\n        logger.error(\"Exception caught: %s\", e)\n        return False\n\n    return True\n</code></pre>"},{"location":"generate_src_doc/rs-client/staging/#rs_workflows.staging.update_stac_catalog","title":"<code>update_stac_catalog(stac_client, collection_name, stac_file_info, obs)</code>","text":"<p>Update the STAC catalog with file information.</p> <p>This task updates the STAC catalog with the information of a file that has been processed and saved to a specific location. It adds the mandatory fields such as geometry, collection name and object storage href. It sends a POST request to add the newly composed item into the STAC catalog.</p> <p>Parameters:</p> Name Type Description Default <code>stac_client</code> <code>StacClient</code> <p>The client for accessing the STAC catalog.</p> required <code>collection_name</code> <code>str</code> <p>The name of the collection in the STAC catalog.</p> required <code>stac_file_info</code> <code>dict</code> <p>Information about the STAC file to be updated.</p> required <code>obs</code> <code>str</code> <p>The bucket location where the file has been saved.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the STAC catalog is successfully updated, False otherwise.</p> Source code in <code>docs/rs-client-libraries/rs_workflows/staging.py</code> <pre><code>@task\ndef update_stac_catalog(  # pylint: disable=too-many-locals\n    stac_client: StacClient,  # NOTE: maybe use RsClientSerialization instead\n    collection_name: str,\n    stac_file_info: dict,\n    obs: str,\n):\n    \"\"\"Update the STAC catalog with file information.\n\n    This task updates the STAC catalog with the information of a file that has been processed\n    and saved to a specific location. It adds the mandatory fields such as geometry, collection name and object storage\n    href. It sends a POST request to add the newly composed item into the STAC catalog.\n\n    Args:\n        stac_client (StacClient): The client for accessing the STAC catalog.\n        collection_name (str): The name of the collection in the STAC catalog.\n        stac_file_info (dict): Information about the STAC file to be updated.\n        obs (str): The bucket location where the file has been saved.\n\n    Returns:\n        bool (bool): True if the STAC catalog is successfully updated, False otherwise.\n\n    \"\"\"\n    item_id = stac_file_info[\"id\"]\n    now = datetime.now()\n\n    # The file path from the temp s3 bucket is given in the assets\n    assets = {\"file\": Asset(href=f\"{obs.rstrip('/')}/{stac_file_info['id']}\")}\n\n    # Copy properties from the input stac file, or use default values\n    properties = stac_file_info.get(\"properties\") or {}\n    geometry = stac_file_info.get(\"geometry\") or {  # NOTE: override the geometry if it is set to None\n        \"type\": \"Polygon\",\n        \"coordinates\": [[[-180, -90], [180, -90], [180, 90], [-180, 90], [-180, -90]]],\n    }\n    bbox = stac_file_info.get(\"bbox\") or [-180.0, -90.0, 180.0, 90.0]\n    datetime_value = now\n\n    # Try to format each property date or datetime\n    for key, value in properties.items():\n        try:\n            properties[key] = dateutil.parser.parse(value).strftime(DATETIME_FORMAT_MS)\n        # If this is not a datetime, do nothing\n        except (dateutil.parser.ParserError, TypeError):\n            pass\n\n    # Add item to the STAC catalog collection, check status is OK\n    item = Item(id=item_id, geometry=geometry, bbox=bbox, datetime=datetime_value, properties=properties, assets=assets)\n    try:\n        response = stac_client.add_item(collection_name, item, timeout=CATALOG_REQUEST_TIMEOUT)\n    except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:\n        stac_client.logger.exception(\"Request exception caught: %s\", e)\n        return False\n    try:\n        if not response.ok:\n            stac_client.logger.error(f\"The catalog update endpoint: {response.json()}\")\n    except requests.exceptions.JSONDecodeError:\n        stac_client.logger.exception(f\"Could not get the json body from response: {response}\")\n    return response.status_code == 200\n</code></pre>"},{"location":"generate_src_doc/rs-server/","title":"Python API Library for RS-Server","text":""},{"location":"generate_src_doc/rs-server/#adgs","title":"ADGS","text":"<p>Documentation for ADGS station endpoints: adgs</p>"},{"location":"generate_src_doc/rs-server/#cadip","title":"CADIP","text":"<p>Documentation for CADIP station endpoints: cadip</p>"},{"location":"generate_src_doc/rs-server/#catalog","title":"Catalog","text":"<p>Documentation for catalog endpoints: catalog</p>"},{"location":"generate_src_doc/rs-server/#common","title":"Common","text":"<p>Documentation for common function: common</p>"},{"location":"generate_src_doc/rs-server/#frontend","title":"Frontend","text":"<p>Documentation for frontend functions: frontend</p>"},{"location":"generate_src_doc/rs-server/adgs/","title":"ADGS","text":"<p>Module for interacting with ADGS system through a FastAPI APIRouter.</p> <p>This module provides functionality to retrieve a list of products from the ADGS stations. It includes an API endpoint, utility functions, and initialization for accessing EODataAccessGateway.</p> <p>Module used to download AUX files from ADGS station.</p> <p>HTTP endpoints to get the downloading status from ADGS stations.</p>"},{"location":"generate_src_doc/rs-server/adgs/#rs_server_adgs.api.adgs_search.search_products","title":"<code>search_products(request, datetime, limit=1000, sortby='-created')</code>","text":"<p>Endpoint to handle the search for products in the AUX station within a specified time interval.</p> <p>This function validates the input 'datetime' format, performs a search for products using the ADGS provider, writes the search results to the database, and generates a STAC Feature Collection from the products.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The request object (unused).</p> required <code>datetime</code> <code>str</code> <p>Time interval in ISO 8601 format.</p> required <code>limit</code> <code>int</code> <p>Maximum number of products to return. Defaults to 1000.</p> <code>1000</code> <code>sortby</code> <code>str</code> <p>Sort by +/-fieldName (ascending/descending). Defaults to \"-datetime\".</p> <code>'-created'</code> <p>Returns:</p> Type Description <code>list[dict] | dict</code> <p>list[dict] | dict: A list of STAC Feature Collections or an error message.                If no products are found in the specified time range, returns an empty list.</p> <p>Raises:</p> Type Description <code>HTTPException(exceptions)</code> <p>If the pagination limit is less than 1.</p> <code>HTTPException(exceptions)</code> <p>If there is a bad station identifier (CreateProviderFailed).</p> <code>HTTPException(exceptions)</code> <p>If there is a database connection error (sqlalchemy.exc.OperationalError).</p> <code>HTTPException(exceptions)</code> <p>If there is a connection error to the station.</p> <code>HTTPException(exceptions)</code> <p>If there is a general failure during the process.</p> Source code in <code>docs/rs-server/services/adgs/rs_server_adgs/api/adgs_search.py</code> <pre><code>@router.get(\"/adgs/aux/search\")\n@apikey_validator(station=\"adgs\", access_type=\"read\")\ndef search_products(  # pylint: disable=too-many-locals\n    request: Request,  # pylint: disable=unused-argument\n    datetime: Annotated[str, Query(description='Time interval e.g. \"2024-01-01T00:00:00Z/2024-01-02T23:59:59Z\"')],\n    limit: Annotated[int, Query(description=\"Maximum number of products to return\")] = 1000,\n    sortby: Annotated[str, Query(description=\"Sort by +/-fieldName (ascending/descending)\")] = \"-created\",\n) -&gt; list[dict] | dict:\n    \"\"\"Endpoint to handle the search for products in the AUX station within a specified time interval.\n\n    This function validates the input 'datetime' format, performs a search for products using the ADGS provider,\n    writes the search results to the database, and generates a STAC Feature Collection from the products.\n\n    Args:\n        request (Request): The request object (unused).\n        datetime (str): Time interval in ISO 8601 format.\n        limit (int, optional): Maximum number of products to return. Defaults to 1000.\n        sortby (str, optional): Sort by +/-fieldName (ascending/descending). Defaults to \"-datetime\".\n\n    Returns:\n        list[dict] | dict: A list of STAC Feature Collections or an error message.\n                           If no products are found in the specified time range, returns an empty list.\n\n    Raises:\n        HTTPException (fastapi.exceptions): If the pagination limit is less than 1.\n        HTTPException (fastapi.exceptions): If there is a bad station identifier (CreateProviderFailed).\n        HTTPException (fastapi.exceptions): If there is a database connection error (sqlalchemy.exc.OperationalError).\n        HTTPException (fastapi.exceptions): If there is a connection error to the station.\n        HTTPException (fastapi.exceptions): If there is a general failure during the process.\n    \"\"\"\n\n    start_date, stop_date = validate_inputs_format(datetime)\n    if limit &lt; 1:\n        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail=\"Pagination cannot be less 0\")\n\n    try:\n        time_range = TimeRange(start_date, stop_date)\n        products = init_adgs_provider(\"adgs\").search(time_range, items_per_page=limit)\n        write_search_products_to_db(AdgsDownloadStatus, products)\n        feature_template_path = ADGS_CONFIG / \"ODataToSTAC_template.json\"\n        stac_mapper_path = ADGS_CONFIG / \"adgs_stac_mapper.json\"\n        with (\n            open(feature_template_path, encoding=\"utf-8\") as template,\n            open(stac_mapper_path, encoding=\"utf-8\") as stac_map,\n        ):\n            feature_template = json.loads(template.read())\n            stac_mapper = json.loads(stac_map.read())\n            adgs_item_collection = create_stac_collection(products, feature_template, stac_mapper)\n        logger.info(\"Succesfully listed and processed products from AUX station\")\n        return sort_feature_collection(adgs_item_collection, sortby)\n\n    # pylint: disable=duplicate-code\n    except CreateProviderFailed as exception:\n        logger.error(f\"Failed to create EODAG provider!\\n{traceback.format_exc()}\")\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=f\"Bad station identifier: {exception}\",\n        ) from exception\n\n    # pylint: disable=duplicate-code\n    except sqlalchemy.exc.OperationalError as exception:\n        logger.error(\"Failed to connect to database!\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"Database connection error: {exception}\",\n        ) from exception\n\n    except requests.exceptions.ConnectionError as exception:\n        logger.error(\"Failed to connect to station!\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"Station ADGS connection error: {exception}\",\n        ) from exception\n\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        logger.error(\"General failure!\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"General failure: {exception}\",\n        ) from exception\n</code></pre>"},{"location":"generate_src_doc/rs-server/adgs/#rs_server_adgs.api.adgs_download.AdgsDownloadResponse","title":"<code>AdgsDownloadResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Endpoint response</p> Source code in <code>docs/rs-server/services/adgs/rs_server_adgs/api/adgs_download.py</code> <pre><code>class AdgsDownloadResponse(BaseModel):\n    \"\"\"Endpoint response\"\"\"\n\n    started: bool\n</code></pre>"},{"location":"generate_src_doc/rs-server/adgs/#rs_server_adgs.api.adgs_download.download_products","title":"<code>download_products(request, name, local=None, obs=None, db=Depends(get_db))</code>","text":"<p>Initiate an asynchronous download process for an ADGS product using EODAG.</p> <p>This endpoint triggers the download of an ADGS product identified by the given name of the file. It starts the download process in a separate thread using the start_eodag_download function and updates the product's status in the database.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The request object (unused).</p> required <code>name</code> <code>str</code> <p>AUX product name.</p> required <code>local</code> <code>str</code> <p>Local download directory.</p> <code>None</code> <code>obs</code> <code>str</code> <p>Object storage path (e.g., \"s3://bucket-name/sub/dir\").</p> <code>None</code> <code>db</code> <code>Session</code> <p>The database connection object.</p> <code>Depends(get_db)</code> <p>Returns:</p> Name Type Description <code>JSONResponse</code> <code>responses</code> <p>A JSON response indicating whether the download process has started.</p> Source code in <code>docs/rs-server/services/adgs/rs_server_adgs/api/adgs_download.py</code> <pre><code>@router.get(\"/adgs/aux\", response_model=AdgsDownloadResponse)\n@apikey_validator(station=\"adgs\", access_type=\"download\")\ndef download_products(\n    request: Request,  # pylint: disable=unused-argument\n    name: Annotated[str, Query(description=\"AUX product name\")],\n    local: Annotated[str | None, Query(description=\"Local download directory\")] = None,\n    obs: Annotated[str | None, Query(description='Object storage path e.g. \"s3://bucket-name/sub/dir\"')] = None,\n    db: Session = Depends(get_db),\n):\n    \"\"\"Initiate an asynchronous download process for an ADGS product using EODAG.\n\n    This endpoint triggers the download of an ADGS product identified by the given\n    name of the file. It starts the download process in a separate thread\n    using the start_eodag_download function and updates the product's status in the database.\n\n    Args:\n        request (Request): The request object (unused).\n        name (str): AUX product name.\n        local (str, optional): Local download directory.\n        obs (str, optional): Object storage path (e.g., \"s3://bucket-name/sub/dir\").\n        db (Session): The database connection object.\n\n    Returns:\n        JSONResponse (starlette.responses): A JSON response indicating whether the download process has started.\n\n    \"\"\"\n\n    try:\n        db_product = AdgsDownloadStatus.get(db, name=name)\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        logger.error(exception)\n        return JSONResponse(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            content={\"started\": \"false\"},\n        )\n\n    # Reset status to not_started\n    db_product.not_started(db)\n\n    # start a thread to run the action in background\n    thread_started = threading.Event()\n    # fmt: off\n    eodag_args = EoDAGDownloadHandler(\n        AdgsDownloadStatus, thread_started, \"adgs\", str(db_product.product_id),\n        name, local, obs,\n    )\n    # fmt: on\n    thread = threading.Thread(\n        target=start_eodag_download,\n        args=(eodag_args,),\n    )\n    thread.start()\n\n    # check the start of the thread\n    if not thread_started.wait(timeout=DWN_THREAD_START_TIMEOUT):\n        logger.error(\"Download thread did not start !\")\n        # Try n times to update the status to FAILED in the database\n        update_db(db, db_product, EDownloadStatus.FAILED, \"Download thread did not start !\")\n        return JSONResponse(status_code=status.HTTP_408_REQUEST_TIMEOUT, content={\"started\": \"false\"})\n\n    return JSONResponse(status_code=status.HTTP_200_OK, content={\"started\": \"true\"})\n</code></pre>"},{"location":"generate_src_doc/rs-server/adgs/#rs_server_adgs.api.adgs_download.start_eodag_download","title":"<code>start_eodag_download(argument)</code>","text":"<p>Start the eodag download process.</p> <p>This function initiates the eodag download process using the provided arguments. It sets up the temporary directory where the files are to be downloaded and gets the database handler</p> <p>Parameters:</p> Name Type Description Default <code>argument</code> <code>EoDAGDownloadHandler</code> <p>An instance of EoDAGDownloadHandler containing the arguments used in the</p> required <p>downloading process</p> Source code in <code>docs/rs-server/services/adgs/rs_server_adgs/api/adgs_download.py</code> <pre><code>def start_eodag_download(argument: EoDAGDownloadHandler):\n    \"\"\"Start the eodag download process.\n\n    This function initiates the eodag download process using the provided arguments. It sets up\n    the temporary directory where the files are to be downloaded and gets the database handler\n\n    Args:\n        argument (EoDAGDownloadHandler): An instance of EoDAGDownloadHandler containing the arguments used in the\n    downloading process\n\n    \"\"\"\n    # Open a database sessions in this thread, because the session from the root thread may have closed.\n    try:\n        with tempfile.TemporaryDirectory() as default_temp_path, contextmanager(get_db)() as db:\n            eodag_download(\n                argument,\n                db,\n                init_adgs_provider,\n                default_path=default_temp_path,\n            )\n    except Exception as e:  # pylint: disable=broad-except\n        logger.error(f\"Exception caught: {e}\")\n</code></pre>"},{"location":"generate_src_doc/rs-server/adgs/#rs_server_adgs.api.adgs_status.get_download_status","title":"<code>get_download_status(request, name, db=Depends(get_db))</code>","text":"<p>Get a product download status from its ID or name.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The request object (unused).</p> required <code>name</code> <code>str</code> <p>The name of the AUX product.</p> required <code>db</code> <code>Session</code> <p>The database connection object.</p> <code>Depends(get_db)</code> <p>Returns:</p> Name Type Description <code>ReadDownloadStatus</code> <code>DownloadStatus</code> <p>The download status of the specified AUX product.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the product is not found in the database.</p> Source code in <code>docs/rs-server/services/adgs/rs_server_adgs/api/adgs_status.py</code> <pre><code>@router.get(\"/adgs/aux/status\", response_model=ReadDownloadStatus)\n@apikey_validator(station=\"adgs\", access_type=\"download\")\ndef get_download_status(\n    request: Request,  # pylint: disable=unused-argument\n    name: Annotated[str, Query(description=\"AUX product name\")],\n    db: Session = Depends(get_db),\n):\n    \"\"\"\n    Get a product download status from its ID or name.\n\n    Args:\n        request (Request): The request object (unused).\n        name (str): The name of the AUX product.\n        db (Session): The database connection object.\n\n    Returns:\n        ReadDownloadStatus (DownloadStatus): The download status of the specified AUX product.\n\n    Raises:\n        HTTPException: If the product is not found in the database.\n    \"\"\"\n\n    return AdgsDownloadStatus.get(name=name, db=db)\n</code></pre>"},{"location":"generate_src_doc/rs-server/cadip/","title":"CADIP","text":"<p>Module for interacting with CADU system through a FastAPI APIRouter.</p> <p>This module provides functionality to retrieve a list of products from the CADU system for a specified station. It includes an API endpoint, utility functions, and initialization for accessing EODataAccessGateway.</p> <p>Module used to download CADU files from CADIP stations.</p> <p>HTTP endpoints to get the downloading status from CADIP stations.</p>"},{"location":"generate_src_doc/rs-server/cadip/#rs_server_cadip.api.cadip_search.search_products","title":"<code>search_products(request, datetime='', station=FPath(description='CADIP station identifier (MTI, SGS, MPU, INU, etc)'), session_id='', limit=1000, sortby='-created')</code>","text":"<p>Endpoint to retrieve a list of products from the CADU system for a specified station.</p> <p>This function validates the input 'datetime' format, performs a search for products using the CADIP provider, writes the search results to the database, and generates a STAC Feature Collection from the products.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The request object (unused).</p> required <code>datetime</code> <code>str</code> <p>Time interval in ISO 8601 format.</p> <code>''</code> <code>station</code> <code>str</code> <p>CADIP station identifier (e.g., MTI, SGS, MPU, INU).</p> <code>Path(description='CADIP station identifier (MTI, SGS, MPU, INU, etc)')</code> <code>session_id</code> <code>str</code> <p>Session from which file belong.</p> <code>''</code> <code>limit</code> <code>int</code> <p>Maximum number of products to return. Defaults to 1000.</p> <code>1000</code> <code>sortby</code> <code>str</code> <p>Sort by +/-fieldName (ascending/descending). Defaults to \"-datetime\".</p> <code>'-created'</code> <p>Returns:</p> Type Description <code>list[dict] | dict</code> <p>list[dict] | dict: A list of STAC Feature Collections or an error message.                If no products are found in the specified time range, returns an empty list.</p> <p>Raises:</p> Type Description <code>HTTPException(exceptions)</code> <p>If the pagination limit is less than 1.</p> <code>HTTPException(exceptions)</code> <p>If there is a bad station identifier (CreateProviderFailed).</p> <code>HTTPException(exceptions)</code> <p>If there is a database connection error (sqlalchemy.exc.OperationalError).</p> <code>HTTPException(exceptions)</code> <p>If there is a connection error to the station.</p> <code>HTTPException(exceptions)</code> <p>If there is a general failure during the process.</p> Source code in <code>docs/rs-server/services/cadip/rs_server_cadip/api/cadip_search.py</code> <pre><code>@router.get(\"/cadip/{station}/cadu/search\")\n@apikey_validator(station=\"cadip\", access_type=\"read\")\ndef search_products(  # pylint: disable=too-many-locals, too-many-arguments\n    request: Request,  # pylint: disable=unused-argument\n    datetime: Annotated[str, Query(description='Time interval e.g \"2024-01-01T00:00:00Z/2024-01-02T23:59:59Z\"')] = \"\",\n    station: str = FPath(description=\"CADIP station identifier (MTI, SGS, MPU, INU, etc)\"),\n    session_id: Annotated[str, Query(description=\"Session from which file belong\")] = \"\",\n    limit: Annotated[int, Query(description=\"Maximum number of products to return\")] = 1000,\n    sortby: Annotated[str, Query(description=\"Sort by +/-fieldName (ascending/descending)\")] = \"-created\",\n) -&gt; list[dict] | dict:\n    \"\"\"Endpoint to retrieve a list of products from the CADU system for a specified station.\n\n    This function validates the input 'datetime' format, performs a search for products using the CADIP provider,\n    writes the search results to the database, and generates a STAC Feature Collection from the products.\n\n    Args:\n        request (Request): The request object (unused).\n        datetime (str): Time interval in ISO 8601 format.\n        station (str): CADIP station identifier (e.g., MTI, SGS, MPU, INU).\n        session_id (str): Session from which file belong.\n        limit (int, optional): Maximum number of products to return. Defaults to 1000.\n        sortby (str, optional): Sort by +/-fieldName (ascending/descending). Defaults to \"-datetime\".\n\n    Returns:\n        list[dict] | dict: A list of STAC Feature Collections or an error message.\n                           If no products are found in the specified time range, returns an empty list.\n\n    Raises:\n        HTTPException (fastapi.exceptions): If the pagination limit is less than 1.\n        HTTPException (fastapi.exceptions): If there is a bad station identifier (CreateProviderFailed).\n        HTTPException (fastapi.exceptions): If there is a database connection error (sqlalchemy.exc.OperationalError).\n        HTTPException (fastapi.exceptions): If there is a connection error to the station.\n        HTTPException (fastapi.exceptions): If there is a general failure during the process.\n    \"\"\"\n    if not (datetime or session_id):\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Missing search parameters\")\n    start_date, stop_date = validate_inputs_format(datetime)\n    session: Union[List[str], str] = [sid.strip() for sid in session_id.split(\",\")] if \",\" in session_id else session_id\n    if limit &lt; 1:\n        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail=\"Pagination cannot be less 0\")\n    # Init dataretriever / get products / return\n    try:\n        products = init_cadip_provider(station).search(\n            TimeRange(start_date, stop_date),\n            id=session,\n            items_per_page=limit,\n        )\n        write_search_products_to_db(CadipDownloadStatus, products)\n        feature_template_path = CADIP_CONFIG / \"ODataToSTAC_template.json\"\n        stac_mapper_path = CADIP_CONFIG / \"cadip_stac_mapper.json\"\n        with (\n            open(feature_template_path, encoding=\"utf-8\") as template,\n            open(stac_mapper_path, encoding=\"utf-8\") as stac_map,\n        ):\n            feature_template = json.loads(template.read())\n            stac_mapper = json.loads(stac_map.read())\n            cadip_item_collection = create_stac_collection(products, feature_template, stac_mapper)\n        logger.info(\"Succesfully listed and processed products from CADIP station\")\n        return sort_feature_collection(cadip_item_collection, sortby)\n\n    # pylint: disable=duplicate-code\n    except CreateProviderFailed as exception:\n        logger.error(f\"Failed to create EODAG provider!\\n{traceback.format_exc()}\")\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=f\"Bad station identifier: {exception}\",\n        ) from exception\n\n    # pylint: disable=duplicate-code\n    except sqlalchemy.exc.OperationalError as exception:\n        logger.error(\"Failed to connect to database!\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"Database connection error: {exception}\",\n        ) from exception\n\n    except requests.exceptions.ConnectionError as exception:\n        logger.error(\"Failed to connect to station!\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"Station {station} connection error: {exception}\",\n        ) from exception\n\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        logger.error(\"General failure!\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"General failure: {exception}\",\n        ) from exception\n</code></pre>"},{"location":"generate_src_doc/rs-server/cadip/#rs_server_cadip.api.cadip_search.search_session","title":"<code>search_session(request, station=FPath(description='CADIP station identifier (MTI, SGS, MPU, INU, etc)'), id=None, platform=None, start_date=None, stop_date=None)</code>","text":"<p>Endpoint to retrieve a list of sessions from any CADIP station.</p> <p>A valid session search request must contain at least a value for either id, platform, or a time interval (start_date and stop_date correctly defined).</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The request object (unused).</p> required <code>station</code> <code>str</code> <p>CADIP station identifier (e.g., MTI, SGS, MPU, INU).</p> <code>Path(description='CADIP station identifier (MTI, SGS, MPU, INU, etc)')</code> <code>id</code> <code>str</code> <p>Session identifier(s), comma-separated. Defaults to None.</p> <code>None</code> <code>platform</code> <code>str</code> <p>Satellite identifier(s), comma-separated. Defaults to None.</p> <code>None</code> <code>start_date</code> <code>str</code> <p>Start time in ISO 8601 format. Defaults to None.</p> <code>None</code> <code>stop_date</code> <code>str</code> <p>Stop time in ISO 8601 format. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A STAC Feature Collection of the sessions.</p> <p>Raises:</p> Type Description <code>HTTPException(exceptions)</code> <p>If search parameters are missing.</p> <code>HTTPException(exceptions)</code> <p>If there is a JSON mapping error.</p> <code>HTTPException(exceptions)</code> <p>If there is a value error during mapping.</p> Source code in <code>docs/rs-server/services/cadip/rs_server_cadip/api/cadip_search.py</code> <pre><code>@router.get(\"/cadip/{station}/session\")\n@apikey_validator(station=\"cadip\", access_type=\"read\")\ndef search_session(\n    request: Request,  # pylint: disable=unused-argument\n    station: str = FPath(description=\"CADIP station identifier (MTI, SGS, MPU, INU, etc)\"),\n    id: Annotated[\n        Union[str, None],\n        Query(\n            description='Session identifier eg: \"S1A_20200105072204051312\" or '\n            '\"S1A_20200105072204051312, S1A_20220715090550123456\"',\n        ),\n    ] = None,\n    platform: Annotated[Union[str, None], Query(description='Satellite identifier eg: \"S1A\" or \"S1A, S1B\"')] = None,\n    start_date: Annotated[Union[str, None], Query(description='Start time e.g. \"2024-01-01T00:00:00Z\"')] = None,\n    stop_date: Annotated[Union[str, None], Query(description='Stop time e.g. \"2024-01-01T00:00:00Z\"')] = None,\n):  # pylint: disable=too-many-arguments, too-many-locals\n    \"\"\"Endpoint to retrieve a list of sessions from any CADIP station.\n\n    A valid session search request must contain at least a value for either *id*, *platform*, or a time interval\n    (*start_date* and *stop_date* correctly defined).\n\n    Args:\n        request (Request): The request object (unused).\n        station (str): CADIP station identifier (e.g., MTI, SGS, MPU, INU).\n        id (str, optional): Session identifier(s), comma-separated. Defaults to None.\n        platform (str, optional): Satellite identifier(s), comma-separated. Defaults to None.\n        start_date (str, optional): Start time in ISO 8601 format. Defaults to None.\n        stop_date (str, optional): Stop time in ISO 8601 format. Defaults to None.\n\n    Returns:\n        dict (dict): A STAC Feature Collection of the sessions.\n\n    Raises:\n        HTTPException (fastapi.exceptions): If search parameters are missing.\n        HTTPException (fastapi.exceptions): If there is a JSON mapping error.\n        HTTPException (fastapi.exceptions): If there is a value error during mapping.\n    \"\"\"\n    session_id: Union[List[str], str, None] = [sid.strip() for sid in id.split(\",\")] if (id and \",\" in id) else id\n    satellite: Union[List[str], None] = platform.split(\",\") if platform else None\n    time_interval = validate_inputs_format(f\"{start_date}/{stop_date}\") if start_date and stop_date else (None, None)\n\n    if not (session_id or satellite or (time_interval[0] and time_interval[1])):\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Missing search parameters\")\n\n    try:\n        products = init_cadip_provider(f\"{station}_session\").search(\n            TimeRange(*time_interval),\n            id=session_id,  # pylint: disable=redefined-builtin\n            platform=satellite,\n            sessions_search=True,\n        )\n        products = validate_products(products)\n        sessions_products = from_session_expand_to_dag_serializer(products)\n        write_search_products_to_db(CadipDownloadStatus, sessions_products)\n        feature_template_path = CADIP_CONFIG / \"cadip_session_ODataToSTAC_template.json\"\n        stac_mapper_path = CADIP_CONFIG / \"cadip_sessions_stac_mapper.json\"\n        expanded_session_mapper_path = CADIP_CONFIG / \"cadip_stac_mapper.json\"\n        with (\n            open(feature_template_path, encoding=\"utf-8\") as template,\n            open(stac_mapper_path, encoding=\"utf-8\") as stac_map,\n            open(expanded_session_mapper_path, encoding=\"utf-8\") as expanded_session_mapper,\n        ):\n            feature_template = json.loads(template.read())\n            stac_mapper = json.loads(stac_map.read())\n            expanded_session_mapper = json.loads(expanded_session_mapper.read())\n            cadip_sessions_collection = create_stac_collection(products, feature_template, stac_mapper)\n            cadip_sessions_collection = from_session_expand_to_assets_serializer(\n                cadip_sessions_collection,\n                sessions_products,\n                expanded_session_mapper,\n                request,\n            )\n            return cadip_sessions_collection\n    # except [OSError, FileNotFoundError] as exception:\n    #     return HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=f\"Error: {exception}\")\n    except json.JSONDecodeError as exception:\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"JSON Map Error: {exception}\",\n        ) from exception\n    except ValueError as exception:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=\"Unable to map OData to STAC.\",\n        ) from exception\n</code></pre>"},{"location":"generate_src_doc/rs-server/cadip/#rs_server_cadip.api.cadip_download.CadipDownloadResponse","title":"<code>CadipDownloadResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Endpoint response</p> Source code in <code>docs/rs-server/services/cadip/rs_server_cadip/api/cadip_download.py</code> <pre><code>class CadipDownloadResponse(BaseModel):\n    \"\"\"Endpoint response\"\"\"\n\n    started: bool\n</code></pre>"},{"location":"generate_src_doc/rs-server/cadip/#rs_server_cadip.api.cadip_download.download_products","title":"<code>download_products(request, name, station=FPath(description='CADIP station identifier (MTI, SGS, MPU, INU, etc)'), local=None, obs=None, db=Depends(get_db))</code>","text":"<p>Initiate an asynchronous download process for a CADU product using EODAG.</p> <p>This endpoint triggers the download of a CADU product identified by the given name of the file. It starts the download process in a separate thread using the start_eodag_download function and updates the product's status in the database.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The request object (unused).</p> required <code>name</code> <code>str</code> <p>CADU product name.</p> required <code>station</code> <code>str</code> <p>CADIP station identifier (e.g., MTI, SGS, MPU, INU).</p> <code>Path(description='CADIP station identifier (MTI, SGS, MPU, INU, etc)')</code> <code>local</code> <code>str</code> <p>Local download directory. Defaults to None.</p> <code>None</code> <code>obs</code> <code>str</code> <p>Object storage path (e.g., \"s3://bucket-name/sub/dir\"). Defaults to None.</p> <code>None</code> <code>db</code> <code>Session</code> <p>The database connection object.</p> <code>Depends(get_db)</code> <p>Returns:</p> Name Type Description <code>JSONResponse</code> <code>responses</code> <p>A JSON response indicating whether the download process has started.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the product is not found in the database.</p> <code>HTTPException</code> <p>If the download thread fails to start.</p> Source code in <code>docs/rs-server/services/cadip/rs_server_cadip/api/cadip_download.py</code> <pre><code>@router.get(\"/cadip/{station}/cadu\", response_model=CadipDownloadResponse)\n@apikey_validator(station=\"cadip\", access_type=\"download\")\ndef download_products(\n    request: Request,  # pylint: disable=unused-argument\n    name: Annotated[str, Query(description=\"CADU product name\")],\n    station: str = FPath(description=\"CADIP station identifier (MTI, SGS, MPU, INU, etc)\"),\n    local: Annotated[str | None, Query(description=\"Local download directory\")] = None,\n    obs: Annotated[str | None, Query(description='Object storage path e.g. \"s3://bucket-name/sub/dir\"')] = None,\n    db: Session = Depends(get_db),\n):  # pylint: disable=too-many-arguments\n    \"\"\"Initiate an asynchronous download process for a CADU product using EODAG.\n\n    This endpoint triggers the download of a CADU product identified by the given\n    name of the file. It starts the download process in a separate thread\n    using the start_eodag_download function and updates the product's status in the database.\n\n    Args:\n        request (Request): The request object (unused).\n        name (str): CADU product name.\n        station (str): CADIP station identifier (e.g., MTI, SGS, MPU, INU).\n        local (str, optional): Local download directory. Defaults to None.\n        obs (str, optional): Object storage path (e.g., \"s3://bucket-name/sub/dir\"). Defaults to None.\n        db (Session): The database connection object.\n\n    Returns:\n        JSONResponse (starlette.responses): A JSON response indicating whether the download process has started.\n\n    Raises:\n        HTTPException: If the product is not found in the database.\n        HTTPException: If the download thread fails to start.\n    \"\"\"\n\n    # Get the product download status from database\n    try:\n        db_product = CadipDownloadStatus.get(db, name=name)\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        logger.error(exception)\n        return JSONResponse(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            content={\"started\": \"false\"},\n        )\n\n    # Reset status to not_started\n    db_product.not_started(db)\n\n    # start a thread to run the action in background\n    logger.debug(\n        \"%s : %s : %s: MAIN THREAD: Starting thread, local = %s\",\n        os.getpid(),\n        threading.get_ident(),\n        datetime.now(),\n        locals(),\n    )\n\n    thread_started = Event()\n    # fmt: off\n    # Skip this function call formatting to avoid the following error: pylint R0801: Similar lines in 2 files\n    eodag_args = EoDAGDownloadHandler(\n        CadipDownloadStatus, thread_started, station.lower(), str(db_product.product_id),\n        name, local, obs,\n    )\n    # fmt: on\n    # Big note / TODO here\n    # Is there a mechanism to catch / capture return value from a function running inside a thread?\n    # If start_eodag_download throws an error, there is no simple solution to return it with FastAPI\n    thread = threading.Thread(\n        target=start_eodag_download,\n        args=(eodag_args,),\n    )\n    thread.start()\n\n    # check the start of the thread\n    if not thread_started.wait(timeout=DWN_THREAD_START_TIMEOUT):\n        logger.error(\"Download thread did not start !\")\n        # Try n times to update the status to FAILED in the database\n        update_db(db, db_product, EDownloadStatus.FAILED, \"Download thread did not start !\")\n        return JSONResponse(status_code=status.HTTP_408_REQUEST_TIMEOUT, content={\"started\": \"false\"})\n\n    return JSONResponse(status_code=status.HTTP_200_OK, content={\"started\": \"true\"})\n</code></pre>"},{"location":"generate_src_doc/rs-server/cadip/#rs_server_cadip.api.cadip_download.start_eodag_download","title":"<code>start_eodag_download(argument)</code>","text":"<p>Start the eodag download process.</p> <p>This function initiates the eodag download process using the provided arguments. It sets up the temporary directory where the files are to be downloaded and gets the database handler</p> <p>Parameters:</p> Name Type Description Default <code>argument</code> <code>EoDAGDownloadHandler</code> <p>An instance of EoDAGDownloadHandler containing</p> required <p>the arguments used in the downloading process</p> Source code in <code>docs/rs-server/services/cadip/rs_server_cadip/api/cadip_download.py</code> <pre><code>def start_eodag_download(argument: EoDAGDownloadHandler):\n    \"\"\"Start the eodag download process.\n\n    This function initiates the eodag download process using the provided arguments. It sets up\n    the temporary directory where the files are to be downloaded and gets the database handler\n\n    Args:\n        argument (EoDAGDownloadHandler): An instance of EoDAGDownloadHandler containing\n    the arguments used in the downloading process\n\n\n    \"\"\"\n\n    # Open a database sessions in this thread, because the session from the root thread may have closed.\n    try:\n        with tempfile.TemporaryDirectory() as default_temp_path, contextmanager(get_db)() as db:\n            eodag_download(\n                argument,\n                db,\n                init_cadip_provider,\n                default_path=default_temp_path,\n            )\n    except Exception as e:  # pylint: disable=broad-except\n        logger.error(f\"Exception caught: {e}\")\n</code></pre>"},{"location":"generate_src_doc/rs-server/cadip/#rs_server_cadip.api.cadip_status.get_download_status","title":"<code>get_download_status(request, name, db=Depends(get_db), station=FPath(description='CADIP station identifier (MTI, SGS, MPU, INU, etc)'))</code>","text":"<p>Get the download status of a CADU product by its name.</p> <p>This endpoint retrieves the download status of a CADU product from the database using the provided product name.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The request object (unused).</p> required <code>name</code> <code>str</code> <p>CADU product name.</p> required <code>db</code> <code>Session</code> <p>The database connection object.</p> <code>Depends(get_db)</code> <code>station</code> <code>str</code> <p>CADIP station identifier (e.g., MTI, SGS, MPU, INU).</p> <code>Path(description='CADIP station identifier (MTI, SGS, MPU, INU, etc)')</code> <p>Returns:</p> Name Type Description <code>ReadDownloadStatus</code> <code>DownloadStatus</code> <p>The download status of the product.</p> Source code in <code>docs/rs-server/services/cadip/rs_server_cadip/api/cadip_status.py</code> <pre><code>@router.get(\"/cadip/{station}/cadu/status\", response_model=ReadDownloadStatus)\n@apikey_validator(station=\"cadip\", access_type=\"download\")\ndef get_download_status(\n    request: Request,  # pylint: disable=unused-argument\n    name: Annotated[str, Query(description=\"CADU product name\")],\n    db: Session = Depends(get_db),\n    station: str = FPath(  # pylint: disable=unused-argument\n        description=\"CADIP station identifier (MTI, SGS, MPU, INU, etc)\",\n    ),\n):\n    \"\"\"\n    Get the download status of a CADU product by its name.\n\n    This endpoint retrieves the download status of a CADU product from the database\n    using the provided product name.\n\n    Args:\n        request (Request): The request object (unused).\n        name (str): CADU product name.\n        db (Session): The database connection object.\n        station (str): CADIP station identifier (e.g., MTI, SGS, MPU, INU).\n\n    Returns:\n        ReadDownloadStatus (DownloadStatus): The download status of the product.\n    \"\"\"\n\n    return CadipDownloadStatus.get(name=name, db=db)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/","title":"Common","text":"<p>Authentication functions implementation.</p> <p>Note: calls https://gitlab.si.c-s.fr/space_applications/eoservices/apikey-manager</p> <p>Database connection.</p> <p>Taken from: https://praciano.com.br/fastapi-and-async-sqlalchemy-20-with-pytest-done-right.html</p> <p>Logging utility.</p> <p>OpenTelemetry utility</p> <p>This module is used to share common functions between apis endpoints</p> <p>EODAG Provider.</p> <p>Provider mechanism.</p> <p>TODO Docstring to be added.</p>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.authentication.__apikey_security_cached","title":"<code>__apikey_security_cached(apikey_value)</code>  <code>async</code>","text":"<p>Cached version of apikey_security. Cache an infinite (sys.maxsize) number of results for 120 seconds.</p> <p>This function serves as a cached version of apikey_security. It retrieves user access control information from the User Authentication and Authorization Control (UAC) manager and caches the result for performance optimization.</p> <p>Parameters:</p> Name Type Description Default <code>apikey_value</code> <code>str</code> <p>The API key value.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[list[str], dict, str]</code> <p>A tuple containing user IAM roles, configuration data, and user login information.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If there is an error connecting to the UAC manager or if the UAC manager returns an error.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/authentication.py</code> <pre><code>@cached(cache=ttl_cache)\nasync def __apikey_security_cached(apikey_value) -&gt; tuple[list[str], dict, str]:\n    \"\"\"\n    Cached version of apikey_security. Cache an infinite (sys.maxsize) number of results for 120 seconds.\n\n    This function serves as a cached version of apikey_security. It retrieves user access control information\n    from the User Authentication and Authorization Control (UAC) manager and caches the result for performance\n    optimization.\n\n    Args:\n        apikey_value (str): The API key value.\n\n    Returns:\n        tuple: A tuple containing user IAM roles, configuration data, and user login information.\n\n    Raises:\n        HTTPException: If there is an error connecting to the UAC manager or if the UAC manager returns an error.\n    \"\"\"\n\n    # The uac manager check url is passed as an environment variable\n    try:\n        check_url = env[\"RSPY_UAC_CHECK_URL\"]\n    except KeyError:\n        raise HTTPException(HTTP_400_BAD_REQUEST, \"UAC manager URL is undefined\")  # pylint: disable=raise-missing-from\n\n    # Request the uac, pass user-defined api key in http header\n    try:\n        response = await settings.http_client().get(check_url, headers={APIKEY_HEADER: apikey_value or \"\"})\n    except httpx.HTTPError as error:\n        message = \"Error connecting to the UAC manager\"\n        logger.error(f\"{message}\\n{traceback.format_exc()}\")\n        raise HTTPException(HTTP_500_INTERNAL_SERVER_ERROR, message) from error\n\n    # Read the api key info\n    if response.is_success:\n        contents = response.json()\n        # Note: for now, config is an empty dict\n        return contents[\"iam_roles\"], contents[\"config\"], contents[\"user_login\"]\n\n    # Try to read the response detail or error\n    try:\n        json = response.json()\n        if \"detail\" in json:\n            detail = json[\"detail\"]\n        else:\n            detail = json[\"error\"]\n\n    # If this fail, get the full response content\n    except Exception:  # pylint: disable=broad-exception-caught\n        detail = response.read().decode(\"utf-8\")\n\n    # Forward error\n    raise HTTPException(response.status_code, f\"UAC manager: {detail}\")\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.authentication.apikey_security","title":"<code>apikey_security(request, apikey_header)</code>  <code>async</code>","text":"<p>FastAPI Security dependency for the cluster mode. Check the api key validity, passed as an HTTP header.</p> <p>Parameters:</p> Name Type Description Default <code>apikey_header</code> <code>Security</code> <p>API key passed in HTTP header</p> required <p>Returns:</p> Type Description <code>tuple[list[str], dict, str]</code> <p>Tuple of (IAM roles, config, user login) information from the keycloak account, associated to the api key.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/authentication.py</code> <pre><code>async def apikey_security(\n    request: Request,\n    apikey_header: Annotated[str, Security(APIKEY_AUTH_HEADER)],\n    # apikey_query: Annotated[str, Security(APIKEY_AUTH_QUERY)],\n) -&gt; tuple[list[str], dict, str]:\n    \"\"\"\n    FastAPI Security dependency for the cluster mode. Check the api key validity, passed as an HTTP header.\n\n    Args:\n        apikey_header (Security): API key passed in HTTP header\n\n    Returns:\n        Tuple of (IAM roles, config, user login) information from the keycloak account, associated to the api key.\n    \"\"\"\n\n    # Use the api key passed by either http headers or query parameter (disabled for now)\n    apikey_value = apikey_header  # or apikey_query\n    if not apikey_value:\n        raise HTTPException(\n            status_code=HTTP_403_FORBIDDEN,\n            detail=\"Not authenticated\",\n        )\n\n    # Call the cached function (fastapi Depends doesn't work with @cached)\n    auth_roles, auth_config, user_login = await __apikey_security_cached(str(apikey_value))\n    request.state.auth_roles = auth_roles\n    request.state.auth_config = auth_config\n    request.state.user_login = user_login\n    logger.debug(f\"API key information: {auth_roles, auth_config, user_login}\")\n    return auth_roles, auth_config, user_login\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.authentication.apikey_validator","title":"<code>apikey_validator(station, access_type)</code>","text":"<p>Decorator to validate API key access for a specific station and access type.</p> <p>This decorator checks if the authorization key contains the necessary role to access the specified station with the specified access type.</p> <p>Parameters:</p> Name Type Description Default <code>station</code> <code>str</code> <p>The name of the station, either \"adgs\" or \"cadip\".</p> required <code>access_type</code> <code>str</code> <p>The type of access, such as \"download\" or \"read\".</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the authorization key does not include the required role to access the specified station with the specified access type.</p> <p>Returns:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The decorator function.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/authentication.py</code> <pre><code>def apikey_validator(station, access_type):\n    \"\"\"Decorator to validate API key access for a specific station and access type.\n\n    This decorator checks if the authorization key contains the necessary role to access\n    the specified station with the specified access type.\n\n    Args:\n        station (str): The name of the station, either \"adgs\" or \"cadip\".\n        access_type (str): The type of access, such as \"download\" or \"read\".\n\n    Raises:\n        HTTPException: If the authorization key does not include the required role\n            to access the specified station with the specified access type.\n\n    Returns:\n        function (Callable): The decorator function.\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if settings.CLUSTER_MODE:\n                # Read the full cadip station passed in parameter e.g. INS, MPS, ...\n                if station == \"cadip\":\n                    cadip_station = kwargs[\"station\"]  # ins, mps, mti, nsg, sgs, or cadip\n                    try:\n                        full_station = STATIONS_AUTH_LUT[cadip_station.lower()]\n                    except KeyError as exception:\n                        raise HTTPException(\n                            status_code=status.HTTP_400_BAD_REQUEST,\n                            detail=f\"Unknown CADIP station: {cadip_station!r}\",\n                        ) from exception\n                else:  # for adgs\n                    full_station = station\n\n                requested_role = f\"rs_{full_station}_{access_type}\".upper()\n                try:\n                    auth_roles = [role.upper() for role in kwargs[\"request\"].state.auth_roles]\n                except KeyError:\n                    auth_roles = []\n\n                if requested_role not in auth_roles:\n                    raise HTTPException(\n                        status_code=status.HTTP_401_UNAUTHORIZED,\n                        detail=f\"Authorization key does not include the right role to {access_type} \"\n                        f\"from the {full_station!r} station\",\n                    )\n\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.db.database.DatabaseSessionManager","title":"<code>DatabaseSessionManager</code>","text":"<p>Database session configuration.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/db/database.py</code> <pre><code>class DatabaseSessionManager:\n    \"\"\"Database session configuration.\"\"\"\n\n    lock = Lock()\n    multiprocessing_lock = multiprocessing.Lock()\n\n    def __init__(self):\n        \"\"\"Create a Database session configuration.\"\"\"\n        self._engine: Engine | None = None\n        self._sessionmaker: sessionmaker | None = None\n\n    @classmethod\n    def url(cls):\n        \"\"\"Get database connection URL.\"\"\"\n        try:\n            # pylint: disable=consider-using-f-string\n            return os.getenv(\n                \"POSTGRES_URL\",\n                \"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}\".format(\n                    user=os.environ[\"POSTGRES_USER\"],\n                    password=os.environ[\"POSTGRES_PASSWORD\"],\n                    host=os.environ[\"POSTGRES_HOST\"],\n                    port=os.environ[\"POSTGRES_PORT\"],\n                    dbname=os.environ[\"POSTGRES_DB\"],\n                ),\n            )\n        except KeyError as key_error:\n            raise KeyError(\n                \"The PostgreSQL environment variables are missing: \"\n                \"POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB\",\n            ) from key_error\n\n    def open_session(self, url: str = \"\"):\n        \"\"\"Open database session.\"\"\"\n\n        # If the 'self' object is used by several threads in the same process,\n        # make sure to initialize the session only once.\n        with DatabaseSessionManager.lock:\n            if (self._engine is None) or (self._sessionmaker is None):\n                self._engine = create_engine(url or self.url(), poolclass=NullPool, pool_pre_ping=True)\n                self._sessionmaker = sessionmaker(autocommit=False, autoflush=False, bind=self._engine)\n\n                try:\n                    # Create all tables.\n                    # Warning: this only works if the database table modules have been imported\n                    # e.g. import rs_server_adgs.adgs_download_status\n                    self.create_all()\n\n                # It fails if the database is unreachable, but even in this case the engine and session are not None.\n                # Set them to None so we will try to create all tables again on the next try.\n                except Exception:\n                    self.close()\n                    raise\n\n    def close(self):\n        \"\"\"Close database session.\"\"\"\n        if self._engine is not None:\n            self._engine.dispose()\n            self._engine = None\n        self._sessionmaker = None\n\n    @contextlib.contextmanager\n    def connect(self) -&gt; Iterator[Connection]:\n        \"\"\"Open new database connection instance.\"\"\"\n\n        if self._engine is None:\n            raise RuntimeError(\"DatabaseSessionManager is not initialized\")\n\n        with self._engine.begin() as connection:\n            try:\n                yield connection\n\n            # In case of any exception, rollback connection and re-raise into HTTP exception\n            except Exception as exception:  # pylint: disable=broad-exception-caught\n                connection.rollback()\n                self.reraise_http_exception(exception)\n\n    @contextlib.contextmanager\n    def session(self) -&gt; Iterator[Session]:\n        \"\"\"Open new database session instance.\"\"\"\n\n        if self._sessionmaker is None:\n            raise RuntimeError(\"DatabaseSessionManager is not initialized\")\n\n        session = self._sessionmaker()\n        try:\n            yield session\n\n        # In case of any exception, rollback session and re-raise into HTTP exception\n        except Exception as exception:  # pylint: disable=broad-exception-caught\n            session.rollback()\n            self.reraise_http_exception(exception)\n\n        # Close session when deleting instance.\n        finally:\n            session.close()\n\n    @staticmethod\n    def __filelock(func):\n        \"\"\"Avoid concurrent writing to the database using a file locK.\"\"\"\n\n        @wraps(func)\n        def with_filelock(*args, **kwargs):\n            \"\"\"Wrap the the call to 'func' inside the lock.\"\"\"\n\n            # Let's do this only if the RSPY_WORKING_DIR environment variable is defined.\n            # Write a .lock file inside this directory.\n            try:\n                with FileLock(Path(os.environ[\"RSPY_WORKING_DIR\"]) / f\"{__name__}.lock\"):\n                    return func(*args, **kwargs)\n\n            # Else just call the function without a lock\n            except KeyError:\n                return func(*args, **kwargs)\n\n        return with_filelock\n\n    @__filelock\n    def create_all(self):\n        \"\"\"Create all database tables.\"\"\"\n        with DatabaseSessionManager.multiprocessing_lock:  # Handle concurrent table creation by different processes\n            Base.metadata.create_all(bind=self._engine)\n\n    @__filelock\n    def drop_all(self):\n        \"\"\"Drop all database tables.\"\"\"\n        with DatabaseSessionManager.multiprocessing_lock:  # Handle concurrent table creation by different processes\n            Base.metadata.drop_all(bind=self._engine)\n\n    @classmethod\n    def reraise_http_exception(cls, exception: Exception):\n        \"\"\"Re-raise all exceptions into HTTP exceptions.\"\"\"\n\n        # Raised exceptions are not always printed in the console, so do it manually with the stacktrace.\n        logger.error(traceback.format_exc())\n\n        if isinstance(exception, StarletteHTTPException):\n            raise exception\n        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=repr(exception))\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.db.database.DatabaseSessionManager.__filelock","title":"<code>__filelock(func)</code>  <code>staticmethod</code>","text":"<p>Avoid concurrent writing to the database using a file locK.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/db/database.py</code> <pre><code>@staticmethod\ndef __filelock(func):\n    \"\"\"Avoid concurrent writing to the database using a file locK.\"\"\"\n\n    @wraps(func)\n    def with_filelock(*args, **kwargs):\n        \"\"\"Wrap the the call to 'func' inside the lock.\"\"\"\n\n        # Let's do this only if the RSPY_WORKING_DIR environment variable is defined.\n        # Write a .lock file inside this directory.\n        try:\n            with FileLock(Path(os.environ[\"RSPY_WORKING_DIR\"]) / f\"{__name__}.lock\"):\n                return func(*args, **kwargs)\n\n        # Else just call the function without a lock\n        except KeyError:\n            return func(*args, **kwargs)\n\n    return with_filelock\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.db.database.DatabaseSessionManager.__init__","title":"<code>__init__()</code>","text":"<p>Create a Database session configuration.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/db/database.py</code> <pre><code>def __init__(self):\n    \"\"\"Create a Database session configuration.\"\"\"\n    self._engine: Engine | None = None\n    self._sessionmaker: sessionmaker | None = None\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.db.database.DatabaseSessionManager.close","title":"<code>close()</code>","text":"<p>Close database session.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/db/database.py</code> <pre><code>def close(self):\n    \"\"\"Close database session.\"\"\"\n    if self._engine is not None:\n        self._engine.dispose()\n        self._engine = None\n    self._sessionmaker = None\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.db.database.DatabaseSessionManager.connect","title":"<code>connect()</code>","text":"<p>Open new database connection instance.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/db/database.py</code> <pre><code>@contextlib.contextmanager\ndef connect(self) -&gt; Iterator[Connection]:\n    \"\"\"Open new database connection instance.\"\"\"\n\n    if self._engine is None:\n        raise RuntimeError(\"DatabaseSessionManager is not initialized\")\n\n    with self._engine.begin() as connection:\n        try:\n            yield connection\n\n        # In case of any exception, rollback connection and re-raise into HTTP exception\n        except Exception as exception:  # pylint: disable=broad-exception-caught\n            connection.rollback()\n            self.reraise_http_exception(exception)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.db.database.DatabaseSessionManager.create_all","title":"<code>create_all()</code>","text":"<p>Create all database tables.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/db/database.py</code> <pre><code>@__filelock\ndef create_all(self):\n    \"\"\"Create all database tables.\"\"\"\n    with DatabaseSessionManager.multiprocessing_lock:  # Handle concurrent table creation by different processes\n        Base.metadata.create_all(bind=self._engine)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.db.database.DatabaseSessionManager.drop_all","title":"<code>drop_all()</code>","text":"<p>Drop all database tables.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/db/database.py</code> <pre><code>@__filelock\ndef drop_all(self):\n    \"\"\"Drop all database tables.\"\"\"\n    with DatabaseSessionManager.multiprocessing_lock:  # Handle concurrent table creation by different processes\n        Base.metadata.drop_all(bind=self._engine)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.db.database.DatabaseSessionManager.open_session","title":"<code>open_session(url='')</code>","text":"<p>Open database session.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/db/database.py</code> <pre><code>def open_session(self, url: str = \"\"):\n    \"\"\"Open database session.\"\"\"\n\n    # If the 'self' object is used by several threads in the same process,\n    # make sure to initialize the session only once.\n    with DatabaseSessionManager.lock:\n        if (self._engine is None) or (self._sessionmaker is None):\n            self._engine = create_engine(url or self.url(), poolclass=NullPool, pool_pre_ping=True)\n            self._sessionmaker = sessionmaker(autocommit=False, autoflush=False, bind=self._engine)\n\n            try:\n                # Create all tables.\n                # Warning: this only works if the database table modules have been imported\n                # e.g. import rs_server_adgs.adgs_download_status\n                self.create_all()\n\n            # It fails if the database is unreachable, but even in this case the engine and session are not None.\n            # Set them to None so we will try to create all tables again on the next try.\n            except Exception:\n                self.close()\n                raise\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.db.database.DatabaseSessionManager.reraise_http_exception","title":"<code>reraise_http_exception(exception)</code>  <code>classmethod</code>","text":"<p>Re-raise all exceptions into HTTP exceptions.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/db/database.py</code> <pre><code>@classmethod\ndef reraise_http_exception(cls, exception: Exception):\n    \"\"\"Re-raise all exceptions into HTTP exceptions.\"\"\"\n\n    # Raised exceptions are not always printed in the console, so do it manually with the stacktrace.\n    logger.error(traceback.format_exc())\n\n    if isinstance(exception, StarletteHTTPException):\n        raise exception\n    raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=repr(exception))\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.db.database.DatabaseSessionManager.session","title":"<code>session()</code>","text":"<p>Open new database session instance.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/db/database.py</code> <pre><code>@contextlib.contextmanager\ndef session(self) -&gt; Iterator[Session]:\n    \"\"\"Open new database session instance.\"\"\"\n\n    if self._sessionmaker is None:\n        raise RuntimeError(\"DatabaseSessionManager is not initialized\")\n\n    session = self._sessionmaker()\n    try:\n        yield session\n\n    # In case of any exception, rollback session and re-raise into HTTP exception\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        session.rollback()\n        self.reraise_http_exception(exception)\n\n    # Close session when deleting instance.\n    finally:\n        session.close()\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.db.database.DatabaseSessionManager.url","title":"<code>url()</code>  <code>classmethod</code>","text":"<p>Get database connection URL.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/db/database.py</code> <pre><code>@classmethod\ndef url(cls):\n    \"\"\"Get database connection URL.\"\"\"\n    try:\n        # pylint: disable=consider-using-f-string\n        return os.getenv(\n            \"POSTGRES_URL\",\n            \"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}\".format(\n                user=os.environ[\"POSTGRES_USER\"],\n                password=os.environ[\"POSTGRES_PASSWORD\"],\n                host=os.environ[\"POSTGRES_HOST\"],\n                port=os.environ[\"POSTGRES_PORT\"],\n                dbname=os.environ[\"POSTGRES_DB\"],\n            ),\n        )\n    except KeyError as key_error:\n        raise KeyError(\n            \"The PostgreSQL environment variables are missing: \"\n            \"POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB\",\n        ) from key_error\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.db.database.get_db","title":"<code>get_db()</code>","text":"<p>Return a database session for FastAPI dependency injection.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/db/database.py</code> <pre><code>def get_db():\n    \"\"\"Return a database session for FastAPI dependency injection.\"\"\"\n    try:\n        with sessionmanager.session() as session:\n            yield session\n\n    # Re-raise all exceptions into HTTP exceptions\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        DatabaseSessionManager.reraise_http_exception(exception)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.logging.CustomFormatter","title":"<code>CustomFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Custom logging formatter with colored text. See: https://stackoverflow.com/a/56944256</p> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/logging.py</code> <pre><code>class CustomFormatter(logging.Formatter):\n    \"\"\"\n    Custom logging formatter with colored text.\n    See: https://stackoverflow.com/a/56944256\n    \"\"\"\n\n    _RED = \"\\x1b[31m\"\n    _BOLD_RED = \"\\x1b[31;1m\"\n    _GREEN = \"\\x1b[32m\"\n    _YELLOW = \"\\x1b[33m\"\n    _PURPLE = \"\\x1b[35m\"\n    _RESET = \"\\x1b[0m\"\n\n    _FORMAT = f\"%(asctime)s.%(msecs)03d [{{color}}%(levelname)s{_RESET}] (%(name)s) %(message)s\"\n    _DATETIME = \"%H:%M:%S\"\n\n    _FORMATS = {\n        logging.NOTSET: _FORMAT.format(color=\"\"),\n        logging.DEBUG: _FORMAT.format(color=_PURPLE),\n        logging.INFO: _FORMAT.format(color=_GREEN),\n        logging.WARNING: _FORMAT.format(color=_YELLOW),\n        logging.ERROR: _FORMAT.format(color=_BOLD_RED),\n        logging.CRITICAL: _FORMAT.format(color=_RED),\n    }\n\n    def format(self, record):\n        level_format = self._FORMATS.get(record.levelno)\n        formatter = logging.Formatter(level_format, self._DATETIME)\n        return formatter.format(record)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.logging.Logging","title":"<code>Logging</code>","text":"<p>Logging utility.</p> <p>Attributes:</p> Name Type Description <code>lock</code> <p>For code synchronization</p> <code>level</code> <p>Minimal log level to use for all new logging instances.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/logging.py</code> <pre><code>class Logging:  # pylint: disable=too-few-public-methods\n    \"\"\"\n    Logging utility.\n\n    Attributes:\n        lock: For code synchronization\n        level: Minimal log level to use for all new logging instances.\n    \"\"\"\n\n    lock = Lock()\n    level = logging.DEBUG\n\n    @classmethod\n    def default(cls, name=\"rspy\"):\n        \"\"\"\n        Return a default Logger class instance.\n\n        Args:\n            name (str): Logger name. You can pass __name__ to use your current module name.\n        \"\"\"\n        logger = logging.getLogger(name=name)\n\n        with cls.lock:\n            # Don't propagate to root logger\n            logger.propagate = False\n\n            # If we have already set the handlers for the logger with this name, do nothing more\n            if logger.hasHandlers():\n                return logger\n\n            # Set the minimal log level to use for all new logging instances.\n            logger.setLevel(cls.level)\n\n            # Create console handler\n            handler = logging.StreamHandler()\n            handler.setFormatter(CustomFormatter())\n            logger.addHandler(handler)\n\n            # Export logs to Loki, see: https://pypi.org/project/python-logging-loki/\n            # Note: on the cluster, this is not used. Promtail already forwards stdout to Loki.\n            loki_endpoint = os.getenv(\"LOKI_ENDPOINT\")\n            if loki_endpoint and settings.SERVICE_NAME:\n                handler = logging_loki.LokiQueueHandler(\n                    Queue(-1),\n                    url=loki_endpoint,\n                    tags={\"service\": settings.SERVICE_NAME},\n                    # auth=(\"username\", \"password\"),\n                    version=\"1\",\n                )\n                handler.setFormatter(CustomFormatter())\n                logger.addHandler(handler)\n\n            return logger\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.logging.Logging.default","title":"<code>default(name='rspy')</code>  <code>classmethod</code>","text":"<p>Return a default Logger class instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name. You can pass name to use your current module name.</p> <code>'rspy'</code> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/logging.py</code> <pre><code>@classmethod\ndef default(cls, name=\"rspy\"):\n    \"\"\"\n    Return a default Logger class instance.\n\n    Args:\n        name (str): Logger name. You can pass __name__ to use your current module name.\n    \"\"\"\n    logger = logging.getLogger(name=name)\n\n    with cls.lock:\n        # Don't propagate to root logger\n        logger.propagate = False\n\n        # If we have already set the handlers for the logger with this name, do nothing more\n        if logger.hasHandlers():\n            return logger\n\n        # Set the minimal log level to use for all new logging instances.\n        logger.setLevel(cls.level)\n\n        # Create console handler\n        handler = logging.StreamHandler()\n        handler.setFormatter(CustomFormatter())\n        logger.addHandler(handler)\n\n        # Export logs to Loki, see: https://pypi.org/project/python-logging-loki/\n        # Note: on the cluster, this is not used. Promtail already forwards stdout to Loki.\n        loki_endpoint = os.getenv(\"LOKI_ENDPOINT\")\n        if loki_endpoint and settings.SERVICE_NAME:\n            handler = logging_loki.LokiQueueHandler(\n                Queue(-1),\n                url=loki_endpoint,\n                tags={\"service\": settings.SERVICE_NAME},\n                # auth=(\"username\", \"password\"),\n                version=\"1\",\n            )\n            handler.setFormatter(CustomFormatter())\n            logger.addHandler(handler)\n\n        return logger\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.opentelemetry.init_traces","title":"<code>init_traces(app, service_name)</code>","text":"<p>Init instrumentation of OpenTelemetry traces.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>FastAPI</code> <p>FastAPI application</p> required <code>service_name</code> <code>str</code> <p>service name</p> required Source code in <code>docs/rs-server/services/common/rs_server_common/utils/opentelemetry.py</code> <pre><code>def init_traces(app: fastapi.FastAPI, service_name: str):\n    \"\"\"\n    Init instrumentation of OpenTelemetry traces.\n\n    Args:\n        app (fastapi.FastAPI): FastAPI application\n        service_name (str): service name\n    \"\"\"\n\n    # See: https://github.com/softwarebloat/python-tracing-demo/tree/main\n\n    # Don't call this line from pytest because it causes errors:\n    # Transient error StatusCode.UNAVAILABLE encountered while exporting metrics to localhost:4317, retrying in ..s.\n    if not FROM_PYTEST:\n        tempo_endpoint = os.getenv(\"TEMPO_ENDPOINT\")\n        if not tempo_endpoint:\n            return\n\n        # TODO: to avoid errors in local mode:\n        # Transient error StatusCode.UNAVAILABLE encountered while exporting metrics to localhost:4317, retrying in ..s.\n        #\n        # The below line does not work either but at least we have less error messages.\n        # See: https://pforge-exchange2.astrium.eads.net/jira/browse/RSPY-221?focusedId=162092&amp;\n        # page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-162092\n        #\n        # Now we have a single line error, which is less worst:\n        # Failed to export metrics to tempo:4317, error code: StatusCode.UNIMPLEMENTED\n        os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = tempo_endpoint\n\n    otel_resource = Resource(attributes={\"service.name\": service_name})\n    otel_tracer = TracerProvider(resource=otel_resource)\n    trace.set_tracer_provider(otel_tracer)\n\n    if not FROM_PYTEST:\n        otel_tracer.add_span_processor(BatchSpanProcessor(OTLPSpanExporter(endpoint=tempo_endpoint)))\n\n    FastAPIInstrumentor.instrument_app(app, tracer_provider=otel_tracer)\n    # logger.debug(f\"OpenTelemetry instrumentation of 'fastapi.FastAPIInstrumentor'\")\n\n    # Instrument all the dependencies under opentelemetry.instrumentation.*\n    # NOTE: we need 'poetry run opentelemetry-bootstrap -a install' to install these.\n\n    package = opentelemetry.instrumentation\n    prefix = package.__name__ + \".\"\n    classes = set()\n\n    # We need an empty PYTHONPATH if the env var is missing\n    os.environ[\"PYTHONPATH\"] = os.getenv(\"PYTHONPATH\", \"\")\n\n    # Recursively find all package modules\n    for _, module_str, _ in pkgutil.walk_packages(path=package.__path__, prefix=prefix, onerror=None):\n\n        # Don't instrument these modules, they have errors, maybe we should see why\n        if module_str in [\"opentelemetry.instrumentation.tortoiseorm\"]:\n            continue\n\n        # Import and find all module classes\n        __import__(module_str)\n        for _, _class in inspect.getmembers(sys.modules[module_str]):\n            if (not inspect.isclass(_class)) or (_class in classes):\n                continue\n\n            # Save the class (classes are found several times when imported by other modules)\n            classes.add(_class)\n\n            # Don't instrument these classes, they have errors, maybe we should see why\n            if _class in [AsyncioInstrumentor, AwsLambdaInstrumentor, BaseInstrumentor]:\n                continue\n\n            # If the \"instrument\" method exists, call it\n            _instrument = getattr(_class, \"instrument\", None)\n            if callable(_instrument):\n\n                _class_instance = _class()\n                if not _class_instance.is_instrumented_by_opentelemetry:\n                    _class_instance.instrument(tracer_provider=otel_tracer)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.utils.EoDAGDownloadHandler","title":"<code>EoDAGDownloadHandler</code>  <code>dataclass</code>","text":"<p>Dataclass to store arguments needed for eodag download.</p> <p>Attributes:</p> Name Type Description <code>db_handler</code> <code>DownloadStatus</code> <p>An instance used to access the database.</p> <code>thread_started</code> <code>Event</code> <p>Event to signal the start of the download thread.</p> <code>station</code> <code>str</code> <p>Station identifier (needed only for CADIP).</p> <code>product_id</code> <code>str</code> <p>Identifier of the product to be downloaded.</p> <code>name</code> <code>str</code> <p>Filename of the file to be downloaded.</p> <code>local</code> <code>str | None</code> <p>Local path where the product will be stored</p> <code>obs</code> <code>str | None</code> <p>Path to the S3 storage where the file will be uploaded</p> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/utils.py</code> <pre><code>@dataclass\nclass EoDAGDownloadHandler:\n    \"\"\"Dataclass to store arguments needed for eodag download.\n\n    Attributes:\n        db_handler (DownloadStatus): An instance used to access the database.\n        thread_started (threading.Event): Event to signal the start of the download thread.\n        station (str): Station identifier (needed only for CADIP).\n        product_id (str): Identifier of the product to be downloaded.\n        name (str): Filename of the file to be downloaded.\n        local (str | None): Local path where the product will be stored\n        obs (str | None): Path to the S3 storage where the file will be uploaded\n    \"\"\"\n\n    db_handler: DownloadStatus\n    thread_started: threading.Event\n    station: str  # needed only for CADIP\n    product_id: str\n    name: str\n    local: str | None\n    obs: str | None\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.utils.create_stac_collection","title":"<code>create_stac_collection(products, feature_template, stac_mapper)</code>","text":"<p>Creates a STAC feature collection based on a given template for a list of EOProducts.</p> <p>Parameters:</p> Name Type Description Default <code>products</code> <code>List[EOProduct]</code> <p>A list of EOProducts to create STAC features for.</p> required <code>feature_template</code> <code>dict</code> <p>The template for generating STAC features.</p> required <code>stac_mapper</code> <code>dict</code> <p>The mapping dictionary for converting EOProduct data to STAC properties.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The STAC feature collection containing features for each EOProduct.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/utils.py</code> <pre><code>def create_stac_collection(products: List[EOProduct], feature_template: dict, stac_mapper: dict) -&gt; dict:\n    \"\"\"\n    Creates a STAC feature collection based on a given template for a list of EOProducts.\n\n    Args:\n        products (List[EOProduct]): A list of EOProducts to create STAC features for.\n        feature_template (dict): The template for generating STAC features.\n        stac_mapper (dict): The mapping dictionary for converting EOProduct data to STAC properties.\n\n    Returns:\n        dict: The STAC feature collection containing features for each EOProduct.\n    \"\"\"\n    stac_template: Dict[Any, Any] = {\n        \"type\": \"FeatureCollection\",\n        \"numberMatched\": 0,\n        \"numberReturned\": 0,\n        \"features\": [],\n    }\n    for product in products:\n        product_data = extract_eo_product(product, stac_mapper)\n        feature_tmp = odata_to_stac(copy.deepcopy(feature_template), product_data, stac_mapper)\n        stac_template[\"numberMatched\"] += 1\n        stac_template[\"numberReturned\"] += 1\n        stac_template[\"features\"].append(feature_tmp)\n    return stac_template\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.utils.eodag_download","title":"<code>eodag_download(argument, db, init_provider, **kwargs)</code>","text":"<p>Initiates the eodag download process.</p> <p>Parameters:</p> Name Type Description Default <code>argument</code> <code>EoDAGDownloadHandler</code> <p>An instance of EoDAGDownloadHandler containing the arguments used in the</p> required <p>downloading process.     db: The database connection object.     init_provider (Callable[[str], Provider]): A function to initialize the provider for downloading.     **kwargs: Additional keyword arguments.</p> Note <p>The local and obs parameters are optional: - local (str | None): Local path where the product will be stored. If this     parameter is not given, the local path where the file is stored will be set to a temporary one. - obs (str | None): Path to S3 storage where the file will be uploaded, after a successful download from CADIP     server. If this parameter is not given, the file will not be uploaded to the S3 storage.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an issue connecting to the S3 storage during the download.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/utils.py</code> <pre><code>def eodag_download(argument: EoDAGDownloadHandler, db, init_provider: Callable[[str], Provider], **kwargs):\n    \"\"\"Initiates the eodag download process.\n\n    Args:\n        argument (EoDAGDownloadHandler): An instance of EoDAGDownloadHandler containing the arguments used in the\n    downloading process.\n        db: The database connection object.\n        init_provider (Callable[[str], Provider]): A function to initialize the provider for downloading.\n        **kwargs: Additional keyword arguments.\n\n    Note:\n        The local and obs parameters are optional:\n        - local (str | None): Local path where the product will be stored. If this\n            parameter is not given, the local path where the file is stored will be set to a temporary one.\n        - obs (str | None): Path to S3 storage where the file will be uploaded, after a successful download from CADIP\n            server. If this parameter is not given, the file will not be uploaded to the S3 storage.\n\n    Raises:\n        RuntimeError: If there is an issue connecting to the S3 storage during the download.\n    \"\"\"\n\n    # Open a database sessions in this thread, because the session from the root thread may have closed.\n    # Get the product download status\n\n    db_product = argument.db_handler.get(db, name=argument.name)\n    # init eodag object\n    try:\n        logger.debug(\n            \"%s : %s : %s: Thread started !\",\n            os.getpid(),\n            threading.get_ident(),\n            datetime.now(),\n        )\n\n        setup_logging(3, no_progress_bar=True)\n        # tempfile to be used here\n\n        # Update the status to IN_PROGRESS in the database\n        db_product.in_progress(db)\n        local = kwargs[\"default_path\"] if not argument.local else argument.local\n        # notify the main thread that the download will be started\n        # To be discussed: init_provider may fail, but in the same time it takes too much\n        # when properly initialized, and the timeout for download endpoint return is overpassed\n        argument.thread_started.set()\n        provider = init_provider(argument.station)\n        init = datetime.now()\n        filename = Path(local) / argument.name\n        provider.download(argument.product_id, filename)\n        logger.info(\n            \"%s : %s : File: %s downloaded in %s\",\n            os.getpid(),\n            threading.get_ident(),\n            argument.name,\n            datetime.now() - init,\n        )\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        # Pylint disabled since error is logged here.\n        logger.error(\n            \"%s : %s : %s: Exception caught: %s\",\n            os.getpid(),\n            threading.get_ident(),\n            datetime.now(),\n            exception,\n        )\n\n        # Try n times to update the status to FAILED in the database\n        update_db(db, db_product, EDownloadStatus.FAILED, repr(exception))\n        return\n\n    if argument.obs:\n        try:\n            # NOTE: The environment variables have to be set from outside\n            # otherwise the connection with the s3 endpoint fails\n            # TODO: the secrets should be set through env vars\n            # pylint: disable=pointless-string-statement\n            \"\"\"\n            secrets = {\n                \"s3endpoint\": None,\n                \"accesskey\": None,\n                \"secretkey\": None,\n            }\n            S3StorageHandler.get_secrets_from_file(secrets, \"/home/\" + os.environ[\"USER\"] + \"/.s3cfg\")\n            os.environ[\"S3_ACCESSKEY\"] = secrets[\"accesskey\"]\n            os.environ[\"S3_SECRETKEY\"] = secrets[\"secretkey\"]\n            os.environ[\"S3_ENDPOINT\"] = secrets[\"s3endpoint\"]\n            os.environ[\"S3_REGION\"] = \"sbg\"\n            \"\"\"\n            s3_handler = S3StorageHandler(\n                os.environ[\"S3_ACCESSKEY\"],\n                os.environ[\"S3_SECRETKEY\"],\n                os.environ[\"S3_ENDPOINT\"],\n                os.environ[\"S3_REGION\"],  # \"sbg\",\n            )\n            obs_array = argument.obs.split(\"/\")  # s3://bucket/path/to\n            s3_config = PutFilesToS3Config(\n                [str(filename)],\n                obs_array[2],\n                \"/\".join(obs_array[3:]),\n            )\n            s3_handler.put_files_to_s3(s3_config)\n        except (RuntimeError, KeyError) as e:\n            logger.exception(f\"Could not connect to the s3 storage: {e}\")\n            # Try n times to update the status to FAILED in the database\n            update_db(\n                db,\n                db_product,\n                EDownloadStatus.FAILED,\n                \"Could not connect to the s3 storage\",\n            )\n            return\n        except Exception as e:  # pylint: disable=broad-except\n            logger.exception(f\"General exception: {e}\")\n            return\n        finally:\n            os.remove(filename)\n\n    # Try n times to update the status to DONE in the database\n    update_db(db, db_product, EDownloadStatus.DONE)\n    logger.debug(\"Download finished succesfully for %s\", db_product.name)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.utils.extract_eo_product","title":"<code>extract_eo_product(eo_product, mapper)</code>","text":"<p>This function is creating key:value pairs from an EOProduct properties</p> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/utils.py</code> <pre><code>def extract_eo_product(eo_product: EOProduct, mapper: dict) -&gt; dict:\n    \"\"\"This function is creating key:value pairs from an EOProduct properties\"\"\"\n    return {key: value for key, value in eo_product.properties.items() if key in mapper.values()}\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.utils.is_valid_date_format","title":"<code>is_valid_date_format(date)</code>","text":"<p>Check if a string adheres to the expected date format \"YYYY-MM-DDTHH:MM:SS.sssZ\".</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>str</code> <p>The string to be validated for the specified date format.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the input string adheres to the expected date format, otherwise False.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/utils.py</code> <pre><code>def is_valid_date_format(date: str) -&gt; bool:\n    \"\"\"Check if a string adheres to the expected date format \"YYYY-MM-DDTHH:MM:SS.sssZ\".\n\n    Args:\n        date (str): The string to be validated for the specified date format.\n\n    Returns:\n        bool: True if the input string adheres to the expected date format, otherwise False.\n\n    \"\"\"\n    try:\n        datetime.strptime(date, \"%Y-%m-%dT%H:%M:%SZ\")\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.utils.odata_to_stac","title":"<code>odata_to_stac(feature_template, odata_dict, odata_stac_mapper)</code>","text":"<p>Maps OData values to a given STAC template.</p> <p>Parameters:</p> Name Type Description Default <code>feature_template</code> <code>dict</code> <p>The STAC feature template to be populated.</p> required <code>odata_dict</code> <code>dict</code> <p>The dictionary containing OData values.</p> required <code>odata_stac_mapper</code> <code>dict</code> <p>The mapping dictionary for converting OData keys to STAC properties.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The populated STAC feature template.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided STAC feature template is invalid.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/utils.py</code> <pre><code>def odata_to_stac(feature_template: dict, odata_dict: dict, odata_stac_mapper: dict) -&gt; dict:\n    \"\"\"\n    Maps OData values to a given STAC template.\n\n    Args:\n        feature_template (dict): The STAC feature template to be populated.\n        odata_dict (dict): The dictionary containing OData values.\n        odata_stac_mapper (dict): The mapping dictionary for converting OData keys to STAC properties.\n\n    Returns:\n        dict: The populated STAC feature template.\n\n    Raises:\n        ValueError: If the provided STAC feature template is invalid.\n    \"\"\"\n    if not all(item in feature_template.keys() for item in [\"properties\", \"id\", \"assets\"]):\n        raise ValueError(\"Invalid stac feature template\")\n    for stac_key, eodag_key in odata_stac_mapper.items():\n        if eodag_key in odata_dict:\n            if stac_key in feature_template[\"properties\"]:\n                feature_template[\"properties\"][stac_key] = odata_dict[eodag_key]\n            elif stac_key == \"id\":\n                feature_template[\"id\"] = odata_dict[eodag_key]\n            elif stac_key == \"file:size\":\n                feature_template[\"assets\"][\"file\"][stac_key] = odata_dict[eodag_key]\n    return feature_template\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.utils.sort_feature_collection","title":"<code>sort_feature_collection(feature_collection, sortby)</code>","text":"<p>Sorts a STAC feature collection based on a given criteria.</p> <p>Parameters:</p> Name Type Description Default <code>feature_collection</code> <code>dict</code> <p>The STAC feature collection to be sorted.</p> required <code>sortby</code> <code>str</code> <p>The sorting criteria. Use \"+fieldName\" for ascending order or \"-fieldName\" for descending order. Use \"+doNotSort\" to skip sorting.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The sorted STAC feature collection.</p> Note <p>If sortby is not in the format of \"+fieldName\" or \"-fieldName\", the function defaults to ascending order by the \"datetime\" field.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/utils.py</code> <pre><code>def sort_feature_collection(feature_collection: dict, sortby: str) -&gt; dict:\n    \"\"\"\n    Sorts a STAC feature collection based on a given criteria.\n\n    Args:\n        feature_collection (dict): The STAC feature collection to be sorted.\n        sortby (str): The sorting criteria. Use \"+fieldName\" for ascending order\n            or \"-fieldName\" for descending order. Use \"+doNotSort\" to skip sorting.\n\n    Returns:\n        dict: The sorted STAC feature collection.\n\n    Note:\n        If sortby is not in the format of \"+fieldName\" or \"-fieldName\",\n        the function defaults to ascending order by the \"datetime\" field.\n    \"\"\"\n    # Force default sorting even if the input is invalid, don't block the return collection because of sorting.\n    if sortby != \"+doNotSort\":\n        order = sortby[0]\n        if order not in [\"+\", \"-\"]:\n            order = \"+\"\n\n        if len(feature_collection[\"features\"]) and \"properties\" in feature_collection[\"features\"][0]:\n            field = sortby[1:]\n            by = \"datetime\" if field not in feature_collection[\"features\"][0][\"properties\"].keys() else field\n            feature_collection[\"features\"] = sorted(\n                feature_collection[\"features\"],\n                key=lambda feature: feature[\"properties\"][by],\n                reverse=order == \"-\",\n            )\n    return feature_collection\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.utils.update_db","title":"<code>update_db(db, db_product, estatus, status_fail_message=None)</code>","text":"<p>Update the download status of a product in the database.</p> <p>This function attempts to update the download status of a product in the database. It retries the update operation for a maximum of three times, waiting 1 second between attempts.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>Session</code> <p>The database session.</p> required <code>db_product</code> <code>DownloadStatus</code> <p>The product whose status needs to be updated.</p> required <code>estatus</code> <code>EDownloadStatus</code> <p>The new download status.</p> required <code>status_fail_message</code> <code>Optional[str]</code> <p>An optional message associated with the failure status.</p> <code>None</code> <p>Raises:</p> Type Description <code>OperationalError(exc)</code> <p>If the database update operation fails after multiple attempts.</p> Example <p>update_db(db_session, product_instance, EDownloadStatus.DONE)</p> Note <ul> <li>This function is designed to update the download status in the database.</li> <li>It retries the update operation for a maximum of three times.</li> <li>If the update fails, an exception is raised, indicating an issue with the database.</li> </ul> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/utils.py</code> <pre><code>def update_db(\n    db: sqlalchemy.orm.Session,\n    db_product: DownloadStatus,\n    estatus: EDownloadStatus,\n    status_fail_message=None,\n):\n    \"\"\"Update the download status of a product in the database.\n\n    This function attempts to update the download status of a product in the database.\n    It retries the update operation for a maximum of three times, waiting 1 second between attempts.\n\n    Args:\n        db (sqlalchemy.orm.Session): The database session.\n        db_product (DownloadStatus): The product whose status needs to be updated.\n        estatus (EDownloadStatus): The new download status.\n        status_fail_message (Optional[str]): An optional message associated with the failure status.\n\n    Raises:\n        OperationalError (sqlalchemy.exc): If the database update operation fails after multiple attempts.\n\n    Example:\n        &gt;&gt;&gt; update_db(db_session, product_instance, EDownloadStatus.DONE)\n\n    Note:\n        - This function is designed to update the download status in the database.\n        - It retries the update operation for a maximum of three times.\n        - If the update fails, an exception is raised, indicating an issue with the database.\n\n    \"\"\"\n    # Try n times to update the status.\n    # Don't do it for NOT_STARTED and IN_PROGRESS (call directly db_product.not_started\n    # or db_product.in_progress) because it will anyway be overwritten later by DONE or FAILED.\n\n    # Init last exception to empty value.\n    last_exception: Exception = Exception()\n\n    for _ in range(3):\n        try:\n            if estatus == EDownloadStatus.FAILED:\n                db_product.failed(db, status_fail_message)\n            elif estatus == EDownloadStatus.DONE:\n                db_product.done(db)\n\n            # The database update worked, exit function\n            return\n\n        # The database update failed, wait n seconds and retry\n        except sqlalchemy.exc.OperationalError as exception:\n            logger.error(f\"Error updating status in database:\\n{exception}\")\n            last_exception = exception\n            time.sleep(1)\n\n    # If all attemps failed, raise the last Exception\n    raise last_exception\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.utils.validate_inputs_format","title":"<code>validate_inputs_format(interval)</code>","text":"<p>Validate the format of the input time interval.</p> <p>This function checks whether the input interval has a valid format (start_date/stop_date) and whether the start and stop dates are in a valid ISO 8601 format.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>str</code> <p>The time interval to be validated, with the following format: \"2024-01-01T00:00:00Z/2024-01-02T23:59:59Z\"</p> required <p>Returns:</p> Type Description <code>Union[None, datetime]</code> <p>Tuple[Union[None, datetime], Union[None, datetime]]: A tuple containing: - start_date (datetime): The start date of the interval. - stop_date (datetime): The stop date of the interval.</p> <code>Union[None, datetime]</code> <p>Or [None, None] if the provided interval is empty.</p> Note <ul> <li>The input interval should be in the format \"start_date/stop_date\" (e.g., \"2022-01-01T00:00:00Z/2022-01-02T00:00:00Z\").</li> <li>This function checks for missing start/stop and validates the ISO 8601 format of start and stop dates.</li> <li>If there is an error, err_code and err_text provide information about the issue.</li> </ul> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/utils.py</code> <pre><code>def validate_inputs_format(interval: str) -&gt; Tuple[Union[None, datetime], Union[None, datetime]]:\n    \"\"\"\n    Validate the format of the input time interval.\n\n    This function checks whether the input interval has a valid format (start_date/stop_date) and\n    whether the start and stop dates are in a valid ISO 8601 format.\n\n    Args:\n        interval (str): The time interval to be validated, with the following format:\n            \"2024-01-01T00:00:00Z/2024-01-02T23:59:59Z\"\n\n    Returns:\n        Tuple[Union[None, datetime], Union[None, datetime]]:\n            A tuple containing:\n            - start_date (datetime): The start date of the interval.\n            - stop_date (datetime): The stop date of the interval.\n        Or [None, None] if the provided interval is empty.\n\n    Note:\n        - The input interval should be in the format \"start_date/stop_date\"\n        (e.g., \"2022-01-01T00:00:00Z/2022-01-02T00:00:00Z\").\n        - This function checks for missing start/stop and validates the ISO 8601 format of start and stop dates.\n        - If there is an error, err_code and err_text provide information about the issue.\n    \"\"\"\n    if not interval:\n        return None, None\n    try:\n        start_date, stop_date = interval.split(\"/\")\n    except ValueError as exc:\n        logger.error(\"Missing start or stop in endpoint call!\")\n        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail=\"Missing start/stop\") from exc\n    if (not is_valid_date_format(start_date)) or (not is_valid_date_format(stop_date)):\n        logger.error(\"Invalid start/stop in endpoint call!\")\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Missing start/stop\")\n\n    return datetime.fromisoformat(start_date), datetime.fromisoformat(stop_date)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.utils.utils.write_search_products_to_db","title":"<code>write_search_products_to_db(db_handler_class, products)</code>","text":"<p>Processes a list of products by adding them to the database if not already present.</p> <p>This function iterates over a list of products. For each product, it checks whether the product is already registered in the database. If the product is not in the database, it is added with its relevant details. The function collects a list of product IDs and names for further processing.</p> <p>Parameters:</p> Name Type Description Default <code>db_handler_class</code> <code>DownloadStatus</code> <p>The database handler class used for database operations.</p> required <code>products</code> <code>List[Product]</code> <p>A list of product objects to be processed.</p> required <p>Returns:</p> Name Type Description <code>products</code> <code>List[Tuple[str, str]]</code> <p>A list of tuples, each containing the 'id' and 'Name' properties of a product.</p> <p>Raises:</p> Type Description <code>OperationalError</code> <p>If there's an issue connecting to the database.</p> <p>Notes: The function assumes that 'products' is a list of objects with a 'properties' attribute, which is a dictionary containing keys 'id', 'Name', and 'startTimeFromAscendingNode'.</p> <p>'get_db' is a context manager that provides a database session.</p> <p>'EDownloadStatus' is an enumeration representing download status.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/utils/utils.py</code> <pre><code>def write_search_products_to_db(db_handler_class: DownloadStatus, products: EOProduct) -&gt; None:\n    \"\"\"\n    Processes a list of products by adding them to the database if not already present.\n\n    This function iterates over a list of products. For each product, it checks whether the product\n    is already registered in the database. If the product is not in the database, it is added with\n    its relevant details. The function collects a list of product IDs and names for further processing.\n\n    Args:\n        db_handler_class (DownloadStatus): The database handler class used for database operations.\n        products (List[Product]): A list of product objects to be processed.\n\n    Returns:\n        products (List[Tuple[str, str]]): A list of tuples, each containing the 'id' and 'Name' properties of a product.\n\n    Raises:\n        sqlalchemy.exc.OperationalError: If there's an issue connecting to the database.\n\n    Notes:\n    The function assumes that 'products' is a list of objects with a 'properties' attribute,\n    which is a dictionary containing keys 'id', 'Name', and 'startTimeFromAscendingNode'.\n\n    'get_db' is a context manager that provides a database session.\n\n    'EDownloadStatus' is an enumeration representing download status.\n    \"\"\"\n    with contextmanager(get_db)() as db:\n        try:\n            for product in products:\n                if db_handler_class.get_if_exists(db, product.properties[\"Name\"]) is not None:\n                    logger.info(\n                        \"Product %s is already registered in database, skipping\",\n                        product.properties[\"Name\"],\n                    )\n                    continue\n\n                db_handler_class.create(\n                    db,\n                    product_id=product.properties[\"id\"],\n                    name=product.properties[\"Name\"],\n                    available_at_station=datetime.fromisoformat(product.properties[\"startTimeFromAscendingNode\"]),\n                    status=EDownloadStatus.NOT_STARTED,\n                )\n\n        except sqlalchemy.exc.OperationalError:\n            logger.error(\"Failed to connect with DB during listing procedure\")\n            raise\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.eodag_provider.EodagProvider","title":"<code>EodagProvider</code>","text":"<p>               Bases: <code>Provider</code></p> <p>An EODAG provider.</p> <p>It uses EODAG to provide data from external sources.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/eodag_provider.py</code> <pre><code>class EodagProvider(Provider):\n    \"\"\"An EODAG provider.\n\n    It uses EODAG to provide data from external sources.\n    \"\"\"\n\n    lock = Lock()  # static Lock instance\n\n    def __init__(self, config_file: Path, provider: str):\n        \"\"\"Create a EODAG provider.\n\n        Args:\n            config_file: the path to the eodag configuration file\n            provider: the name of the eodag provider\n        \"\"\"\n        self.eodag_cfg_dir = tempfile.TemporaryDirectory()  # pylint: disable=consider-using-with\n        self.provider: str = provider\n        self.config_file = config_file\n        self.client: EODataAccessGateway = self.init_eodag_client(config_file)\n        self.client.set_preferred_provider(self.provider)\n\n    def __del__(self):\n        \"\"\"Destructor\"\"\"\n        try:\n            shutil.rmtree(self.eodag_cfg_dir.name)  # remove the unique /tmp dir\n        except FileNotFoundError:\n            pass\n\n    def init_eodag_client(self, config_file: Path) -&gt; EODataAccessGateway:\n        \"\"\"Initialize the eodag client.\n\n        The EODAG client is initialized for the given provider.\n\n        Args:\n            config_file: the path to the eodag configuration file\n\n        Returns:\n             the initialized eodag client\n        \"\"\"\n        try:\n            # Use thread-lock\n            with EodagProvider.lock:\n                os.environ[\"EODAG_CFG_DIR\"] = self.eodag_cfg_dir.name\n                return EODataAccessGateway(config_file.as_posix())\n        except Exception as e:\n            raise CreateProviderFailed(f\"Can't initialize {self.provider} provider\") from e\n\n    def _specific_search(self, between: TimeRange, **kwargs) -&gt; Union[SearchResult, List]:\n        \"\"\"\n        Conducts a search for products within a specified time range.\n\n        This private method interfaces with the client's search functionality,\n        retrieving products that fall within the given time range. The 'between'\n        parameter is expected to be a TimeRange object, encompassing start and end\n        timestamps. The method returns a dictionary of products keyed by their\n        respective identifiers.\n\n        Args:\n            between (TimeRange): An object representing the start and end timestamps\n                                for the search range.\n\n        Returns:\n            SearchResult: A dictionary where keys are product identifiers and\n                            values are EOProduct instances.\n\n        Note:\n            The time format of the 'between' parameter should be verified or formatted\n            appropriately before invoking this method. The method also assumes that the\n            client's search function is correctly set up to handle the provided time\n            range format.\n\n        Raises:\n            Exception: If the search encounters an error or fails, an exception is raised.\n        \"\"\"\n        mapped_search_args = {}\n        sessions_search = kwargs.pop(\"sessions_search\", False)\n\n        session_id = kwargs.pop(\"id\", None)\n        if session_id:\n            # If request contains session id, map it to eodag parameter accordingly (SessionID for single, Ids for list)\n            if isinstance(session_id, list):\n                mapped_search_args[\"SessionIds\"] = \", \".join(session_id)\n            elif isinstance(session_id, str):\n                mapped_search_args[\"SessionID\"] = session_id\n\n        if sessions_search:\n            # If request is for session search, handle platform - if any provided.\n            platform = kwargs.pop(\"platform\", None)\n\n            # Very annoying, for files odata is **SessionID**, for sessions is **SessionId**\n            if \"SessionID\" in mapped_search_args:\n                mapped_search_args[\"SessionId\"] = mapped_search_args.pop(\"SessionID\")\n            if platform:\n                if isinstance(platform, list):\n                    mapped_search_args[\"platforms\"] = \", \".join(platform)\n                elif isinstance(platform, str):\n                    mapped_search_args[\"platform\"] = platform\n\n        if between:\n            # Since now both for files and sessions, time interval is optional, map it if provided.\n            mapped_search_args.update(\n                {\n                    \"startTimeFromAscendingNode\": str(between.start),\n                    \"completionTimeFromAscendingNode\": str(between.end),\n                },\n            )\n\n        try:\n            # Start search -&gt; user defined search params in mapped_search_args (id), pagination in kwargs (top, limit).\n            products, _ = self.client.search(\n                **mapped_search_args,  # type: ignore\n                provider=self.provider,\n                raise_errors=True,\n                **kwargs,\n            )\n        except RequestError:\n            # Empty list if something goes wrong in eodag\n            return []\n\n        return products\n\n    def download(self, product_id: str, to_file: Path) -&gt; None:\n        \"\"\"Download the expected product at the given local location.\n\n        EODAG needs an EOProduct to download.\n        We build an EOProduct from the id and download location\n        to be able to call EODAG for download.\n\n\n        Args:\n            product_id: the id of the product to download\n            to_file: the path where the product has to be download\n\n        Returns:\n            None\n\n        \"\"\"\n        product = self.create_eodag_product(product_id, to_file.name)\n        # download_plugin = self.client._plugins_manager.get_download_plugin(product)\n        # authent_plugin = self.client._plugins_manager.get_auth_plugin(product.provider)\n        # product.register_downloader(download_plugin, authent_plugin)\n        self.client.download(product, outputs_prefix=to_file.parent)\n\n    def create_eodag_product(self, product_id: str, filename: str):\n        \"\"\"Initialize an EO product with minimal properties.\n\n        The title is used by EODAG as the name of the downloaded file.\n        The download link is used by EODAG as http request url for download.\n        The geometry is mandatory in an EO Product so we add the all earth as geometry.\n\n        Args:\n            product_id (str): the id of EO Product\n            filename (str): the name of the downloaded file\n\n        Returns:\n            product (EOProduct): the initialized EO Product\n\n        \"\"\"\n        try:\n            with open(self.config_file, \"r\", encoding=\"utf-8\") as f:\n                base_uri = yaml.safe_load(f)[self.provider.lower()][\"download\"][\"base_uri\"]\n            return EOProduct(\n                self.provider,\n                {\n                    \"id\": product_id,\n                    \"title\": filename,\n                    \"geometry\": \"POLYGON((180 -90, 180 90, -180 90, -180 -90, 180 -90))\",\n                    # TODO build from configuration (but how ?)\n                    \"downloadLink\": f\"{base_uri}({product_id})/$value\",\n                },\n            )\n        except Exception as e:\n            raise CreateProviderFailed(f\"Can't initialize {self.provider} download provider\") from e\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.eodag_provider.EodagProvider.__del__","title":"<code>__del__()</code>","text":"<p>Destructor</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/eodag_provider.py</code> <pre><code>def __del__(self):\n    \"\"\"Destructor\"\"\"\n    try:\n        shutil.rmtree(self.eodag_cfg_dir.name)  # remove the unique /tmp dir\n    except FileNotFoundError:\n        pass\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.eodag_provider.EodagProvider.__init__","title":"<code>__init__(config_file, provider)</code>","text":"<p>Create a EODAG provider.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>Path</code> <p>the path to the eodag configuration file</p> required <code>provider</code> <code>str</code> <p>the name of the eodag provider</p> required Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/eodag_provider.py</code> <pre><code>def __init__(self, config_file: Path, provider: str):\n    \"\"\"Create a EODAG provider.\n\n    Args:\n        config_file: the path to the eodag configuration file\n        provider: the name of the eodag provider\n    \"\"\"\n    self.eodag_cfg_dir = tempfile.TemporaryDirectory()  # pylint: disable=consider-using-with\n    self.provider: str = provider\n    self.config_file = config_file\n    self.client: EODataAccessGateway = self.init_eodag_client(config_file)\n    self.client.set_preferred_provider(self.provider)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.eodag_provider.EodagProvider.create_eodag_product","title":"<code>create_eodag_product(product_id, filename)</code>","text":"<p>Initialize an EO product with minimal properties.</p> <p>The title is used by EODAG as the name of the downloaded file. The download link is used by EODAG as http request url for download. The geometry is mandatory in an EO Product so we add the all earth as geometry.</p> <p>Parameters:</p> Name Type Description Default <code>product_id</code> <code>str</code> <p>the id of EO Product</p> required <code>filename</code> <code>str</code> <p>the name of the downloaded file</p> required <p>Returns:</p> Name Type Description <code>product</code> <code>EOProduct</code> <p>the initialized EO Product</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/eodag_provider.py</code> <pre><code>def create_eodag_product(self, product_id: str, filename: str):\n    \"\"\"Initialize an EO product with minimal properties.\n\n    The title is used by EODAG as the name of the downloaded file.\n    The download link is used by EODAG as http request url for download.\n    The geometry is mandatory in an EO Product so we add the all earth as geometry.\n\n    Args:\n        product_id (str): the id of EO Product\n        filename (str): the name of the downloaded file\n\n    Returns:\n        product (EOProduct): the initialized EO Product\n\n    \"\"\"\n    try:\n        with open(self.config_file, \"r\", encoding=\"utf-8\") as f:\n            base_uri = yaml.safe_load(f)[self.provider.lower()][\"download\"][\"base_uri\"]\n        return EOProduct(\n            self.provider,\n            {\n                \"id\": product_id,\n                \"title\": filename,\n                \"geometry\": \"POLYGON((180 -90, 180 90, -180 90, -180 -90, 180 -90))\",\n                # TODO build from configuration (but how ?)\n                \"downloadLink\": f\"{base_uri}({product_id})/$value\",\n            },\n        )\n    except Exception as e:\n        raise CreateProviderFailed(f\"Can't initialize {self.provider} download provider\") from e\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.eodag_provider.EodagProvider.download","title":"<code>download(product_id, to_file)</code>","text":"<p>Download the expected product at the given local location.</p> <p>EODAG needs an EOProduct to download. We build an EOProduct from the id and download location to be able to call EODAG for download.</p> <p>Parameters:</p> Name Type Description Default <code>product_id</code> <code>str</code> <p>the id of the product to download</p> required <code>to_file</code> <code>Path</code> <p>the path where the product has to be download</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/eodag_provider.py</code> <pre><code>def download(self, product_id: str, to_file: Path) -&gt; None:\n    \"\"\"Download the expected product at the given local location.\n\n    EODAG needs an EOProduct to download.\n    We build an EOProduct from the id and download location\n    to be able to call EODAG for download.\n\n\n    Args:\n        product_id: the id of the product to download\n        to_file: the path where the product has to be download\n\n    Returns:\n        None\n\n    \"\"\"\n    product = self.create_eodag_product(product_id, to_file.name)\n    # download_plugin = self.client._plugins_manager.get_download_plugin(product)\n    # authent_plugin = self.client._plugins_manager.get_auth_plugin(product.provider)\n    # product.register_downloader(download_plugin, authent_plugin)\n    self.client.download(product, outputs_prefix=to_file.parent)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.eodag_provider.EodagProvider.init_eodag_client","title":"<code>init_eodag_client(config_file)</code>","text":"<p>Initialize the eodag client.</p> <p>The EODAG client is initialized for the given provider.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>Path</code> <p>the path to the eodag configuration file</p> required <p>Returns:</p> Type Description <code>EODataAccessGateway</code> <p>the initialized eodag client</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/eodag_provider.py</code> <pre><code>def init_eodag_client(self, config_file: Path) -&gt; EODataAccessGateway:\n    \"\"\"Initialize the eodag client.\n\n    The EODAG client is initialized for the given provider.\n\n    Args:\n        config_file: the path to the eodag configuration file\n\n    Returns:\n         the initialized eodag client\n    \"\"\"\n    try:\n        # Use thread-lock\n        with EodagProvider.lock:\n            os.environ[\"EODAG_CFG_DIR\"] = self.eodag_cfg_dir.name\n            return EODataAccessGateway(config_file.as_posix())\n    except Exception as e:\n        raise CreateProviderFailed(f\"Can't initialize {self.provider} provider\") from e\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.provider.CreateProviderFailed","title":"<code>CreateProviderFailed</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an error occurred during the init of a provider.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/provider.py</code> <pre><code>class CreateProviderFailed(Exception):\n    \"\"\"Exception raised when an error occurred during the init of a provider.\"\"\"\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.provider.DownloadProductFailed","title":"<code>DownloadProductFailed</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an error occurred during the download.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/provider.py</code> <pre><code>class DownloadProductFailed(Exception):\n    \"\"\"Exception raised when an error occurred during the download.\"\"\"\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.provider.Product","title":"<code>Product</code>  <code>dataclass</code>","text":"<p>A product.</p> <p>A product has an external identifier and a dictionary of metadata.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/provider.py</code> <pre><code>@dataclass\nclass Product:\n    \"\"\"A product.\n\n    A product has an external identifier and a dictionary of metadata.\n    \"\"\"\n\n    id_: str\n    metadata: dict[str, str]\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.provider.Provider","title":"<code>Provider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A product provider.</p> <p>A provider gives a common interface to search for files from an external data source and download them locally.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/provider.py</code> <pre><code>class Provider(ABC):\n    \"\"\"A product provider.\n\n    A provider gives a common interface to search for files from an external data source\n    and download them locally.\n    \"\"\"\n\n    def search(self, between: TimeRange, **kwargs) -&gt; Any:\n        \"\"\"Search for products with the given time range.\n\n        The search result is a dictionary of products found indexed by id.\n\n        Args:\n            between: the search period\n\n        Returns:\n            The files found indexed by file id. Specific to each provider.\n\n        \"\"\"\n        if between:\n            if between.duration() == timedelta(0):\n                return []\n            if between.duration() &lt; timedelta(0):\n                raise SearchProductFailed(f\"Search timerange is inverted : ({between.start} -&gt; {between.end})\")\n        return self._specific_search(between, **kwargs)\n\n    @abstractmethod\n    def _specific_search(self, between: TimeRange) -&gt; Any:\n        \"\"\"Search for products with the given time range.\n\n        Specific search for products after common verification.\n\n        Args:\n            between: the search period\n\n        Returns:\n            the files found indexed by file id.\n\n        \"\"\"\n\n    @abstractmethod\n    def download(self, product_id: str, to_file: Path) -&gt; None:\n        \"\"\"Download the given product to the given local path.\n\n        Args:\n            product_id: id of the product to download\n            to_file: path where the file should be downloaded\n\n        Returns:\n            None\n\n        \"\"\"\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.provider.Provider.download","title":"<code>download(product_id, to_file)</code>  <code>abstractmethod</code>","text":"<p>Download the given product to the given local path.</p> <p>Parameters:</p> Name Type Description Default <code>product_id</code> <code>str</code> <p>id of the product to download</p> required <code>to_file</code> <code>Path</code> <p>path where the file should be downloaded</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/provider.py</code> <pre><code>@abstractmethod\ndef download(self, product_id: str, to_file: Path) -&gt; None:\n    \"\"\"Download the given product to the given local path.\n\n    Args:\n        product_id: id of the product to download\n        to_file: path where the file should be downloaded\n\n    Returns:\n        None\n\n    \"\"\"\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.provider.Provider.search","title":"<code>search(between, **kwargs)</code>","text":"<p>Search for products with the given time range.</p> <p>The search result is a dictionary of products found indexed by id.</p> <p>Parameters:</p> Name Type Description Default <code>between</code> <code>TimeRange</code> <p>the search period</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The files found indexed by file id. Specific to each provider.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/provider.py</code> <pre><code>def search(self, between: TimeRange, **kwargs) -&gt; Any:\n    \"\"\"Search for products with the given time range.\n\n    The search result is a dictionary of products found indexed by id.\n\n    Args:\n        between: the search period\n\n    Returns:\n        The files found indexed by file id. Specific to each provider.\n\n    \"\"\"\n    if between:\n        if between.duration() == timedelta(0):\n            return []\n        if between.duration() &lt; timedelta(0):\n            raise SearchProductFailed(f\"Search timerange is inverted : ({between.start} -&gt; {between.end})\")\n    return self._specific_search(between, **kwargs)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.provider.SearchProductFailed","title":"<code>SearchProductFailed</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an error occurred during the search.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/provider.py</code> <pre><code>class SearchProductFailed(Exception):\n    \"\"\"Exception raised when an error occurred during the search.\"\"\"\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.provider.TimeRange","title":"<code>TimeRange</code>  <code>dataclass</code>","text":"<p>A time range.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/provider.py</code> <pre><code>@dataclass\nclass TimeRange:\n    \"\"\"A time range.\"\"\"\n\n    start: datetime\n    end: datetime\n\n    def duration(self) -&gt; timedelta:\n        \"\"\"Duration of the timerange.\n\n        Returns: duration of the timerange\n        \"\"\"\n        return self.end - self.start\n\n    def __bool__(self) -&gt; bool:\n        return self.start is not None and self.end is not None\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.data_retrieval.provider.TimeRange.duration","title":"<code>duration()</code>","text":"<p>Duration of the timerange.</p> <p>Returns: duration of the timerange</p> Source code in <code>docs/rs-server/services/common/rs_server_common/data_retrieval/provider.py</code> <pre><code>def duration(self) -&gt; timedelta:\n    \"\"\"Duration of the timerange.\n\n    Returns: duration of the timerange\n    \"\"\"\n    return self.end - self.start\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.GetKeysFromS3Config","title":"<code>GetKeysFromS3Config</code>  <code>dataclass</code>","text":"<p>S3 configuration for download</p> <p>Attributes:</p> Name Type Description <code>s3_files</code> <code>list</code> <p>A list with the  S3 object keys to be downloaded.</p> <code>bucket</code> <code>str</code> <p>The S3 bucket name.</p> <code>local_prefix</code> <code>str</code> <p>The local prefix where files will be downloaded.</p> <code>overwrite</code> <code>bool</code> <p>Flag indicating whether to overwrite existing files. Default is False.</p> <code>max_retries</code> <code>int</code> <p>The maximum number of download retries. Default is DWN_S3FILE_RETRIES.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>@dataclass\nclass GetKeysFromS3Config:\n    \"\"\"S3 configuration for download\n\n    Attributes:\n        s3_files (list): A list with the  S3 object keys to be downloaded.\n        bucket (str): The S3 bucket name.\n        local_prefix (str): The local prefix where files will be downloaded.\n        overwrite (bool, optional): Flag indicating whether to overwrite existing files. Default is False.\n        max_retries (int, optional): The maximum number of download retries. Default is DWN_S3FILE_RETRIES.\n\n    \"\"\"\n\n    s3_files: list\n    bucket: str\n    local_prefix: str\n    overwrite: bool = False\n    max_retries: int = DWN_S3FILE_RETRIES\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.PutFilesToS3Config","title":"<code>PutFilesToS3Config</code>  <code>dataclass</code>","text":"<p>Configuration for uploading files to S3.</p> <p>Attributes:</p> Name Type Description <code>files</code> <code>List</code> <p>A list with the local file paths to be uploaded.</p> <code>bucket</code> <code>str</code> <p>The S3 bucket name.</p> <code>s3_path</code> <code>str</code> <p>The S3 path where files will be uploaded.</p> <code>max_retries</code> <code>int</code> <p>The maximum number of upload retries. Default is UP_S3FILE_RETRIES.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>@dataclass\nclass PutFilesToS3Config:\n    \"\"\"Configuration for uploading files to S3.\n\n    Attributes:\n        files (List): A list with the local file paths to be uploaded.\n        bucket (str): The S3 bucket name.\n        s3_path (str): The S3 path where files will be uploaded.\n        max_retries (int, optional): The maximum number of upload retries. Default is UP_S3FILE_RETRIES.\n\n    \"\"\"\n\n    files: list\n    bucket: str\n    s3_path: str\n    max_retries: int = UP_S3FILE_RETRIES\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler","title":"<code>S3StorageHandler</code>","text":"<p>Interacts with an S3 storage</p> <p>S3StorageHandler for interacting with an S3 storage service.</p> <p>Attributes:</p> Name Type Description <code>access_key_id</code> <code>str</code> <p>The access key ID for S3 authentication.</p> <code>secret_access_key</code> <code>str</code> <p>The secret access key for S3 authentication.</p> <code>endpoint_url</code> <code>str</code> <p>The endpoint URL for the S3 service.</p> <code>region_name</code> <code>str</code> <p>The region name.</p> <code>s3_client</code> <code>client</code> <p>The s3 client to interact with the s3 storage</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>class S3StorageHandler:\n    \"\"\"Interacts with an S3 storage\n\n    S3StorageHandler for interacting with an S3 storage service.\n\n    Attributes:\n        access_key_id (str): The access key ID for S3 authentication.\n        secret_access_key (str): The secret access key for S3 authentication.\n        endpoint_url (str): The endpoint URL for the S3 service.\n        region_name (str): The region name.\n        s3_client (boto3.client): The s3 client to interact with the s3 storage\n    \"\"\"\n\n    def __init__(self, access_key_id, secret_access_key, endpoint_url, region_name):\n        \"\"\"Initialize the S3StorageHandler instance.\n\n        Args:\n            access_key_id (str): The access key ID for S3 authentication.\n            secret_access_key (str): The secret access key for S3 authentication.\n            endpoint_url (str): The endpoint URL for the S3 service.\n            region_name (str): The region name.\n\n        Raises:\n            RuntimeError: If the connection to the S3 storage cannot be established.\n        \"\"\"\n        self.logger = Logging.default(__name__)\n\n        self.access_key_id = access_key_id\n        self.secret_access_key = secret_access_key\n        self.endpoint_url = endpoint_url\n        self.region_name = region_name\n        self.s3_client: boto3.client = None\n        self.connect_s3()\n        self.logger.debug(\"S3StorageHandler created !\")\n\n    def __get_s3_client(self, access_key_id, secret_access_key, endpoint_url, region_name):\n        \"\"\"Retrieve or create an S3 client instance.\n\n        Args:\n            access_key_id (str): The access key ID for S3 authentication.\n            secret_access_key (str): The secret access key for S3 authentication.\n            endpoint_url (str): The endpoint URL for the S3 service.\n            region_name (str): The region name.\n\n        Returns:\n            client (boto3): An S3 client instance.\n        \"\"\"\n\n        client_config = botocore.config.Config(\n            max_pool_connections=100,\n            # timeout for connection\n            connect_timeout=5,\n            # attempts in trying connection\n            # note:  the default behaviour of boto3 is retrying\n            # connections multiple times and exponentially backing off in between\n            retries={\"total_max_attempts\": 5},\n        )\n        try:\n            return boto3.client(\n                \"s3\",\n                aws_access_key_id=access_key_id,\n                aws_secret_access_key=secret_access_key,\n                endpoint_url=endpoint_url,\n                region_name=region_name,\n                config=client_config,\n            )\n\n        except Exception as e:\n            self.logger.exception(f\"Client error exception: {e}\")\n            raise RuntimeError(\"Client error exception \") from e\n\n    def connect_s3(self):\n        \"\"\"Establish a connection to the S3 service.\n\n        If the S3 client is not already instantiated, this method calls the private __get_s3_client\n        method to create an S3 client instance using the provided credentials and configuration (see __init__).\n        \"\"\"\n        if self.s3_client is None:\n            self.s3_client = self.__get_s3_client(\n                self.access_key_id,\n                self.secret_access_key,\n                self.endpoint_url,\n                self.region_name,\n            )\n\n    def disconnect_s3(self):\n        \"\"\"Close the connection to the S3 service.\"\"\"\n        if self.s3_client is None:\n            return\n        self.s3_client.close()\n        self.s3_client = None\n\n    def delete_file_from_s3(self, bucket, s3_obj):\n        \"\"\"Delete a file from S3.\n\n        Args:\n            bucket (str): The S3 bucket name.\n            s3_obj (str): The S3 object key.\n\n        Raises:\n            RuntimeError: If an error occurs during the bucket access check.\n        \"\"\"\n        if self.s3_client is None or bucket is None or s3_obj is None:\n            raise RuntimeError(\"Input error for deleting the file\")\n        try:\n            self.logger.info(\"Delete key s3://%s/%s\", bucket, s3_obj)\n            self.s3_client.delete_object(Bucket=bucket, Key=s3_obj)\n        except botocore.client.ClientError as e:\n            self.logger.exception(f\"Failed to delete key s3://{bucket}/{s3_obj}: {e}\")\n            raise RuntimeError(f\"Failed to delete key s3://{bucket}/{s3_obj}\") from e\n        except Exception as e:\n            self.logger.exception(f\"Failed to delete key s3://{bucket}/{s3_obj}: {e}\")\n            raise RuntimeError(f\"Failed to delete key s3://{bucket}/{s3_obj}\") from e\n\n    # helper functions\n\n    @staticmethod\n    def get_secrets_from_file(secrets, secret_file):\n        \"\"\"Read secrets from a specified file.\n\n        It reads the secrets from .s3cfg or aws credentials files\n        This function should not be used in production\n\n        Args:\n            secrets (dict): Dictionary to store retrieved secrets.\n            secret_file (str): Path to the file containing secrets.\n        \"\"\"\n        dict_filled = 0\n        with open(secret_file, \"r\", encoding=\"utf-8\") as aws_credentials_file:\n            lines = aws_credentials_file.readlines()\n            for line in lines:\n                if not secrets[\"s3endpoint\"] and \"host_bucket\" in line:\n                    dict_filled += 1\n                    secrets[\"s3endpoint\"] = line.strip().split(\"=\")[1].strip()\n                elif not secrets[\"accesskey\"] and \"access_key\" in line:\n                    dict_filled += 1\n                    secrets[\"accesskey\"] = line.strip().split(\"=\")[1].strip()\n                elif not secrets[\"secretkey\"] and \"secret_\" in line and \"_key\" in line:\n                    dict_filled += 1\n                    secrets[\"secretkey\"] = line.strip().split(\"=\")[1].strip()\n                if dict_filled == 3:\n                    break\n\n    @staticmethod\n    def get_basename(input_path):\n        \"\"\"Get the filename from a full path.\n\n        Args:\n            input_path (str): The full path.\n\n        Returns:\n            filename (str): The filename.\n        \"\"\"\n        path, filename = ntpath.split(input_path)\n        return filename or ntpath.basename(path)\n\n    @staticmethod\n    def s3_path_parser(s3_url):\n        \"\"\"\n        Parses S3 URL to extract bucket, prefix, and file.\n\n        Args:\n            s3_url (str): The S3 URL.\n\n        Returns:\n            (bucket, prefix, s3_file) (tuple): Tuple containing bucket, prefix, and file.\n        \"\"\"\n        s3_data = s3_url.replace(\"s3://\", \"\").split(\"/\")\n        bucket = \"\"\n        start_idx = 0\n        if s3_url.startswith(\"s3://\"):\n            bucket = s3_data[0]\n\n            start_idx = 1\n        prefix = \"\"\n        if start_idx &lt; len(s3_data):\n            prefix = \"/\".join(s3_data[start_idx:-1])\n        s3_file = s3_data[-1]\n        return bucket, prefix, s3_file\n\n    def files_to_be_downloaded(self, bucket, paths):\n        \"\"\"Create a list with the S3 keys to be downloaded.\n\n        The list will have the s3 keys to be downloaded from the bucket.\n        It contains pairs (local_prefix_where_the_file_will_be_downloaded, full_s3_key_path)\n        If a s3 key doesn't exist, the pair will be (None, requested_s3_key_path)\n\n        Args:\n            bucket (str): The S3 bucket name.\n            paths (list): List of S3 object keys.\n\n        Returns:\n            list_with_files (list): List of tuples (local_prefix, full_s3_key_path).\n        \"\"\"\n        # declaration of the list\n        list_with_files: List[Any] = []\n        # for each key, identify it as a file or a folder\n        # in the case of a folder, the files will be recursively gathered\n        for key in paths:\n            path = key.strip().lstrip(\"/\")\n            s3_files = self.list_s3_files_obj(bucket, path)\n            if len(s3_files) == 0:\n                self.logger.warning(\"No key %s found.\", path)\n                list_with_files.append((None, path))\n                continue\n            self.logger.debug(\"total: %s | s3_files = %s\", len(s3_files), s3_files)\n            basename_part = self.get_basename(path)\n\n            # check if it's a file or a dir\n            if len(s3_files) == 1 and path == s3_files[0]:\n                # the current key is a file, append it to the list\n                list_with_files.append((\"\", s3_files[0]))\n                self.logger.debug(\"Append files: list_with_files = %s\", list_with_files)\n            else:\n                # the current key is a folder, append all its files (reursively gathered) to the list\n                for s3_file in s3_files:\n                    split = s3_file.split(\"/\")\n                    split_idx = split.index(basename_part)\n                    list_with_files.append((os.path.join(*split[split_idx:-1]), s3_file.strip(\"/\")))\n\n        return list_with_files\n\n    def files_to_be_uploaded(self, paths):\n        \"\"\"Creates a list with the local files to be uploaded.\n\n        The list contains pairs (s3_path, absolute_local_file_path)\n        If the local file doesn't exist, the pair will be (None, requested_file_to_upload)\n\n        Args:\n            paths (list): List of local file paths.\n\n        Returns:\n            list_with_files (list): List of tuples (s3_path, absolute_local_file_path).\n        \"\"\"\n\n        list_with_files = []\n        for local in paths:\n            path = local.strip()\n            # check if it is a file\n            self.logger.debug(\"path = %s\", path)\n            if os.path.isfile(path):\n                self.logger.debug(\"Add %s\", path)\n                list_with_files.append((\"\", path))\n\n            elif os.path.isdir(path):\n                for root, dir_names, filenames in os.walk(path):\n                    for file in filenames:\n                        full_file_path = os.path.join(root, file.strip(\"/\"))\n                        self.logger.debug(\"full_file_path = %s | dir_names = %s\", full_file_path, dir_names)\n                        if not os.path.isfile(full_file_path):\n                            continue\n                        self.logger.debug(\n                            \"get_basename(path) = %s | root = %s | replace = %s\",\n                            self.get_basename(path),\n                            root,\n                            root.replace(path, \"\"),\n                        )\n\n                        keep_path = os.path.join(self.get_basename(path), root.replace(path, \"\").strip(\"/\")).strip(\"/\")\n                        self.logger.debug(\"path = %s | keep_path = %s | root = %s\", path, keep_path, root)\n\n                        self.logger.debug(\"Add: %s | %s\", keep_path, full_file_path)\n                        list_with_files.append((keep_path, full_file_path))\n            else:\n                self.logger.warning(\"The path %s is not a directory nor a file, it will not be uploaded\", path)\n\n        return list_with_files\n\n    def list_s3_files_obj(self, bucket, prefix):\n        \"\"\"Retrieve the content of an S3 directory.\n\n        Args:\n            bucket (str): The S3 bucket name.\n            prefix (str): The S3 object key prefix.\n\n        Returns:\n            s3_files (list): List containing S3 object keys.\n        \"\"\"\n\n        s3_files = []\n\n        try:\n            paginator: Any = self.s3_client.get_paginator(\"list_objects_v2\")\n            pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n            for page in pages:\n                for item in page.get(\"Contents\", ()):\n                    if item is not None:\n                        s3_files.append(item[\"Key\"])\n        except Exception as error:\n            self.logger.exception(f\"Exception when trying to list files from s3://{bucket}/{prefix}: {error}\")\n            raise RuntimeError(f\"Listing files from s3://{bucket}/{prefix}\") from error\n\n        return s3_files\n\n    def check_bucket_access(self, bucket):\n        \"\"\"Check the accessibility of an S3 bucket.\n\n        Args:\n            bucket (str): The S3 bucket name.\n\n        Raises:\n            RuntimeError: If an error occurs during the bucket access check.\n        \"\"\"\n\n        try:\n            self.connect_s3()\n            self.s3_client.head_bucket(Bucket=bucket)\n        except botocore.client.ClientError as error:\n            # check that it was a 404 vs 403 errors\n            # If it was a 404 error, then the bucket does not exist.\n            error_code = int(error.response[\"Error\"][\"Code\"])\n            if error_code == S3_ERR_FORBIDDEN_ACCESS:\n                self.logger.exception((f\"{bucket} is a private bucket. Forbidden access!\"))\n                raise RuntimeError(f\"{bucket} is a private bucket. Forbidden access!\") from error\n            if error_code == S3_ERR_NOT_FOUND:\n                self.logger.exception((f\"{bucket} bucket does not exist!\"))\n                raise RuntimeError(f\"{bucket} bucket does not exist!\") from error\n            self.logger.exception(f\"Exception when checking the access to {bucket} bucket: {error}\")\n            raise RuntimeError(f\"Exception when checking the access to {bucket} bucket\") from error\n        except botocore.exceptions.EndpointConnectionError as error:\n            self.logger.exception(f\"Could not connect to the endpoint when trying to access {bucket}: {error}\")\n            raise RuntimeError(f\"Could not connect to the endpoint when trying to access {bucket}!\") from error\n        except Exception as error:\n            self.logger.exception(f\"General exception when trying to access bucket {bucket}: {error}\")\n            raise RuntimeError(f\"General exception when trying to access bucket {bucket}\") from error\n\n    def wait_timeout(self, timeout):\n        \"\"\"\n        Wait for a specified timeout duration (minimum 200 ms).\n\n        This function implements a simple timeout mechanism, where it sleeps for 0.2 seconds\n        in each iteration until the cumulative sleep time reaches the specified timeout duration.\n\n        Args:\n            timeout (float): The total duration to wait in seconds.\n\n        \"\"\"\n        time_cnt = 0.0\n        while time_cnt &lt; timeout:\n            time.sleep(SLEEP_TIME)\n            time_cnt += SLEEP_TIME\n\n    def check_file_overwriting(self, local_file, overwrite):\n        \"\"\"Check if file exists and determine if it should be overwritten.\n\n        Args:\n            local_file (str): Path to the local file.\n            overwrite (bool): Whether to overwrite the existing file.\n\n        Returns:\n            bool (bool): True if the file should be overwritten, False otherwise.\n\n        Note:\n        - If the file already exists and the overwrite flag is set to True, the function logs a message,\n        deletes the existing file, and returns True.\n        - If the file already exists and the overwrite flag is set to False, the function logs a warning\n        message, and returns False. In this case, the existing file won't be deleted.\n        - If the file doesn't exist, the function returns True.\n\n        \"\"\"\n        if os.path.isfile(local_file):\n            if overwrite:  # The file already exists, so delete it first\n                self.logger.info(\n                    \"File %s already exists. Deleting it before downloading\",\n                    S3StorageHandler.get_basename(local_file),\n                )\n                os.remove(local_file)\n            else:\n                self.logger.warning(\n                    \"File %s already exists. Ignoring (use the overwrite flag if you want to overwrite this file)\",\n                    S3StorageHandler.get_basename(local_file),\n                )\n                return False\n\n        return True\n\n    def get_keys_from_s3(self, config: GetKeysFromS3Config) -&gt; list:\n        \"\"\"Download S3 keys specified in the configuration.\n\n        Args:\n            config (GetKeysFromS3Config): Configuration for the S3 download.\n\n        Returns:\n            List[str]: A list with the S3 keys that couldn't be downloaded.\n\n        Raises:\n            Exception: Any unexpected exception raised during the download process.\n\n        The function attempts to download files from S3 according to the provided configuration.\n        It returns a list of S3 keys that couldn't be downloaded successfully.\n\n        \"\"\"\n\n        # check the access to the bucket first, or even if it does exist\n        self.check_bucket_access(config.bucket)\n\n        # collection_files: list of files to be downloaded\n        #                   the list contains pair objects with the following\n        #                   syntax: (local_path_to_be_added_to_the_local_prefix, s3_key)\n        #                   the local_path_to_be_added_to_the_local_prefix may be none if the file doesn't exist\n        collection_files = self.files_to_be_downloaded(config.bucket, config.s3_files)\n\n        self.logger.debug(\"collection_files = %s | bucket = %s\", collection_files, config.bucket)\n        failed_files = []\n\n        for collection_file in collection_files:\n            if collection_file[0] is None:\n                failed_files.append(collection_file[1])\n                continue\n\n            local_path = os.path.join(config.local_prefix, collection_file[0].strip(\"/\"))\n            s3_file = collection_file[1]\n            # for each file to download, create the local dir (if it does not exist)\n            os.makedirs(local_path, exist_ok=True)\n            # create the path for local file\n            local_file = os.path.join(local_path, self.get_basename(s3_file).strip(\"/\"))\n\n            if not self.check_file_overwriting(local_file, config.overwrite):\n                continue\n            # download the files\n            downloaded = False\n            for keep_trying in range(config.max_retries):\n                try:\n                    self.connect_s3()\n                    dwn_start = datetime.now()\n                    self.s3_client.download_file(config.bucket, s3_file, local_file)\n                    self.logger.debug(\n                        \"s3://%s/%s downloaded to %s in %s ms\",\n                        config.bucket,\n                        s3_file,\n                        local_file,\n                        datetime.now() - dwn_start,\n                    )\n                    downloaded = True\n                    break\n                except (botocore.client.ClientError, botocore.exceptions.EndpointConnectionError) as error:\n                    self.logger.exception(\n                        \"Error when downloading the file %s. \\\nException: %s. Retrying in %s seconds for %s more times\",\n                        s3_file,\n                        error,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.disconnect_s3()\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n                except RuntimeError:\n                    self.logger.exception(\n                        \"Error when downloading the file %s. \\\nCouldn't get the s3 client. Retrying in %s seconds for %s more times\",\n                        s3_file,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n\n            if not downloaded:\n                self.logger.error(\n                    \"Could not download the file %s. The download was \\\nretried for %s times. Aborting\",\n                    s3_file,\n                    config.max_retries,\n                )\n                failed_files.append(s3_file)\n\n        return failed_files\n\n    def put_files_to_s3(self, config: PutFilesToS3Config) -&gt; list:\n        \"\"\"Upload files to S3 according to the provided configuration.\n\n        Args:\n            config (PutFilesToS3Config): Configuration for the S3 upload.\n\n        Returns:\n            List[str]: A list with the local file paths that couldn't be uploaded.\n\n        Raises:\n            Exception: Any unexpected exception raised during the upload process.\n\n        The function attempts to upload files to S3 according to the provided configuration.\n        It returns a list of local files that couldn't be uploaded successfully.\n\n        \"\"\"\n\n        # check the access to the bucket first, or even if it does exist\n        self.check_bucket_access(config.bucket)\n\n        collection_files = self.files_to_be_uploaded(config.files)\n        failed_files = []\n\n        for collection_file in collection_files:\n            if collection_file[0] is None:\n                self.logger.error(\"The file %s can't be uploaded, its s3 prefix is None\", collection_file[0])\n                failed_files.append(collection_file[1])\n                continue\n\n            file_to_be_uploaded = collection_file[1]\n            # create the s3 key\n            s3_obj = os.path.join(config.s3_path, collection_file[0], os.path.basename(file_to_be_uploaded).strip(\"/\"))\n            uploaded = False\n            for keep_trying in range(config.max_retries):\n                try:\n                    # get the s3 client\n                    self.connect_s3()\n                    self.logger.info(\n                        \"Upload file %s to s3://%s/%s\",\n                        file_to_be_uploaded,\n                        config.bucket,\n                        s3_obj.lstrip(\"/\"),\n                    )\n\n                    self.s3_client.upload_file(file_to_be_uploaded, config.bucket, s3_obj)\n                    uploaded = True\n                    break\n                except (\n                    botocore.client.ClientError,\n                    botocore.exceptions.EndpointConnectionError,\n                    boto3.exceptions.S3UploadFailedError,\n                ) as error:\n                    self.logger.exception(\n                        \"Error when uploading the file %s. \\\nException: %s. Retrying in %s seconds for %s more times\",\n                        file_to_be_uploaded,\n                        error,\n                        UP_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.disconnect_s3()\n                    self.wait_timeout(UP_S3FILE_RETRY_TIMEOUT)\n                except RuntimeError:\n                    self.logger.exception(\n                        \"Error when uploading the file %s. \\\nCouldn't get the s3 client. Retrying in %s seconds for %s more times\",\n                        file_to_be_uploaded,\n                        UP_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.wait_timeout(UP_S3FILE_RETRY_TIMEOUT)\n\n            if not uploaded:\n                self.logger.error(\n                    \"Could not upload the file %s. The upload was \\\nretried for %s times. Aborting\",\n                    file_to_be_uploaded,\n                    config.max_retries,\n                )\n                failed_files.append(file_to_be_uploaded)\n\n        return failed_files\n\n    def transfer_from_s3_to_s3(self, config: TransferFromS3ToS3Config) -&gt; list:\n        \"\"\"Copy S3 keys specified in the configuration.\n        Args:\n            config (TransferFromS3ToS3Config): Configuration object containing bucket source, bucket destination,\n                      S3 files, maximum retries.\n\n        Returns:\n            list: A list of S3 keys that failed to be copied.\n\n        Raises:\n            Exception: Any unexpected exception raised during the upload process.\n        \"\"\"\n        # check the access to both buckets first, or even if they do exist\n        self.check_bucket_access(config.bucket_src)\n        self.check_bucket_access(config.bucket_dst)\n\n        # collection_files: list of files to be downloaded\n        #                   the list contains pair objects with the following\n        #                   syntax: (local_path_to_be_added_to_the_local_prefix, s3_key)\n\n        collection_files = self.files_to_be_downloaded(config.bucket_src, config.s3_files)\n\n        self.logger.debug(\"collection_files = %s | bucket = %s\", collection_files, config.bucket_src)\n        failed_files = []\n        copy_src = {\"Bucket\": config.bucket_src, \"Key\": \"\"}\n\n        for collection_file in collection_files:\n            if collection_file[0] is None:\n                failed_files.append(collection_file[1])\n                continue\n\n            copied = False\n            for keep_trying in range(config.max_retries):\n                self.logger.debug(\n                    \"keep_trying %s | range(config.max_retries) %s \",\n                    keep_trying,\n                    range(config.max_retries),\n                )\n                try:\n                    self.connect_s3()\n                    dwn_start = datetime.now()\n                    copy_src[\"Key\"] = collection_file[1]\n                    self.logger.debug(\"copy_src = %s\", copy_src)\n                    self.s3_client.copy_object(CopySource=copy_src, Bucket=config.bucket_dst, Key=collection_file[1])\n                    self.logger.debug(\n                        \"s3://%s/%s copied to s3://%s/%s in %s ms\",\n                        config.bucket_src,\n                        collection_file[1],\n                        config.bucket_dst,\n                        collection_file[1],\n                        datetime.now() - dwn_start,\n                    )\n                    if not config.copy_only:\n                        self.delete_file_from_s3(config.bucket_src, collection_file[1])\n                        self.logger.debug(\"Key deleted s3://%s/%s\", config.bucket_src, collection_file[1])\n                    copied = True\n                    break\n                except (botocore.client.ClientError, botocore.exceptions.EndpointConnectionError) as error:\n                    self.logger.exception(\n                        \"Error when copying the file s3://%s/%s to s3://%s. \\\nException: %s. Retrying in %s seconds for %s more times\",\n                        config.bucket_src,\n                        collection_file[1],\n                        config.bucket_dst,\n                        error,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.disconnect_s3()\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n                except RuntimeError:\n                    self.logger.exception(\n                        \"Error when copying the file s3://%s/%s to s3://%s. \\\nCouldn't get the s3 client. Retrying in %s seconds for %s more times\",\n                        config.bucket_src,\n                        collection_file[1],\n                        config.bucket_dst,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n\n            if not copied:\n                self.logger.error(\n                    \"Could not copy the file s3://%s/%s to s3://%s. The copy was \\\nretried for %s times. Aborting\",\n                    config.bucket_src,\n                    collection_file[1],\n                    config.bucket_dst,\n                    config.max_retries,\n                )\n                failed_files.append(collection_file[1])\n\n        return failed_files\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.__get_s3_client","title":"<code>__get_s3_client(access_key_id, secret_access_key, endpoint_url, region_name)</code>","text":"<p>Retrieve or create an S3 client instance.</p> <p>Parameters:</p> Name Type Description Default <code>access_key_id</code> <code>str</code> <p>The access key ID for S3 authentication.</p> required <code>secret_access_key</code> <code>str</code> <p>The secret access key for S3 authentication.</p> required <code>endpoint_url</code> <code>str</code> <p>The endpoint URL for the S3 service.</p> required <code>region_name</code> <code>str</code> <p>The region name.</p> required <p>Returns:</p> Name Type Description <code>client</code> <code>boto3</code> <p>An S3 client instance.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def __get_s3_client(self, access_key_id, secret_access_key, endpoint_url, region_name):\n    \"\"\"Retrieve or create an S3 client instance.\n\n    Args:\n        access_key_id (str): The access key ID for S3 authentication.\n        secret_access_key (str): The secret access key for S3 authentication.\n        endpoint_url (str): The endpoint URL for the S3 service.\n        region_name (str): The region name.\n\n    Returns:\n        client (boto3): An S3 client instance.\n    \"\"\"\n\n    client_config = botocore.config.Config(\n        max_pool_connections=100,\n        # timeout for connection\n        connect_timeout=5,\n        # attempts in trying connection\n        # note:  the default behaviour of boto3 is retrying\n        # connections multiple times and exponentially backing off in between\n        retries={\"total_max_attempts\": 5},\n    )\n    try:\n        return boto3.client(\n            \"s3\",\n            aws_access_key_id=access_key_id,\n            aws_secret_access_key=secret_access_key,\n            endpoint_url=endpoint_url,\n            region_name=region_name,\n            config=client_config,\n        )\n\n    except Exception as e:\n        self.logger.exception(f\"Client error exception: {e}\")\n        raise RuntimeError(\"Client error exception \") from e\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.__init__","title":"<code>__init__(access_key_id, secret_access_key, endpoint_url, region_name)</code>","text":"<p>Initialize the S3StorageHandler instance.</p> <p>Parameters:</p> Name Type Description Default <code>access_key_id</code> <code>str</code> <p>The access key ID for S3 authentication.</p> required <code>secret_access_key</code> <code>str</code> <p>The secret access key for S3 authentication.</p> required <code>endpoint_url</code> <code>str</code> <p>The endpoint URL for the S3 service.</p> required <code>region_name</code> <code>str</code> <p>The region name.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the connection to the S3 storage cannot be established.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def __init__(self, access_key_id, secret_access_key, endpoint_url, region_name):\n    \"\"\"Initialize the S3StorageHandler instance.\n\n    Args:\n        access_key_id (str): The access key ID for S3 authentication.\n        secret_access_key (str): The secret access key for S3 authentication.\n        endpoint_url (str): The endpoint URL for the S3 service.\n        region_name (str): The region name.\n\n    Raises:\n        RuntimeError: If the connection to the S3 storage cannot be established.\n    \"\"\"\n    self.logger = Logging.default(__name__)\n\n    self.access_key_id = access_key_id\n    self.secret_access_key = secret_access_key\n    self.endpoint_url = endpoint_url\n    self.region_name = region_name\n    self.s3_client: boto3.client = None\n    self.connect_s3()\n    self.logger.debug(\"S3StorageHandler created !\")\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.check_bucket_access","title":"<code>check_bucket_access(bucket)</code>","text":"<p>Check the accessibility of an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The S3 bucket name.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If an error occurs during the bucket access check.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def check_bucket_access(self, bucket):\n    \"\"\"Check the accessibility of an S3 bucket.\n\n    Args:\n        bucket (str): The S3 bucket name.\n\n    Raises:\n        RuntimeError: If an error occurs during the bucket access check.\n    \"\"\"\n\n    try:\n        self.connect_s3()\n        self.s3_client.head_bucket(Bucket=bucket)\n    except botocore.client.ClientError as error:\n        # check that it was a 404 vs 403 errors\n        # If it was a 404 error, then the bucket does not exist.\n        error_code = int(error.response[\"Error\"][\"Code\"])\n        if error_code == S3_ERR_FORBIDDEN_ACCESS:\n            self.logger.exception((f\"{bucket} is a private bucket. Forbidden access!\"))\n            raise RuntimeError(f\"{bucket} is a private bucket. Forbidden access!\") from error\n        if error_code == S3_ERR_NOT_FOUND:\n            self.logger.exception((f\"{bucket} bucket does not exist!\"))\n            raise RuntimeError(f\"{bucket} bucket does not exist!\") from error\n        self.logger.exception(f\"Exception when checking the access to {bucket} bucket: {error}\")\n        raise RuntimeError(f\"Exception when checking the access to {bucket} bucket\") from error\n    except botocore.exceptions.EndpointConnectionError as error:\n        self.logger.exception(f\"Could not connect to the endpoint when trying to access {bucket}: {error}\")\n        raise RuntimeError(f\"Could not connect to the endpoint when trying to access {bucket}!\") from error\n    except Exception as error:\n        self.logger.exception(f\"General exception when trying to access bucket {bucket}: {error}\")\n        raise RuntimeError(f\"General exception when trying to access bucket {bucket}\") from error\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.check_file_overwriting","title":"<code>check_file_overwriting(local_file, overwrite)</code>","text":"<p>Check if file exists and determine if it should be overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>local_file</code> <code>str</code> <p>Path to the local file.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the existing file.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file should be overwritten, False otherwise.</p> <p>Note: - If the file already exists and the overwrite flag is set to True, the function logs a message, deletes the existing file, and returns True. - If the file already exists and the overwrite flag is set to False, the function logs a warning message, and returns False. In this case, the existing file won't be deleted. - If the file doesn't exist, the function returns True.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def check_file_overwriting(self, local_file, overwrite):\n    \"\"\"Check if file exists and determine if it should be overwritten.\n\n    Args:\n        local_file (str): Path to the local file.\n        overwrite (bool): Whether to overwrite the existing file.\n\n    Returns:\n        bool (bool): True if the file should be overwritten, False otherwise.\n\n    Note:\n    - If the file already exists and the overwrite flag is set to True, the function logs a message,\n    deletes the existing file, and returns True.\n    - If the file already exists and the overwrite flag is set to False, the function logs a warning\n    message, and returns False. In this case, the existing file won't be deleted.\n    - If the file doesn't exist, the function returns True.\n\n    \"\"\"\n    if os.path.isfile(local_file):\n        if overwrite:  # The file already exists, so delete it first\n            self.logger.info(\n                \"File %s already exists. Deleting it before downloading\",\n                S3StorageHandler.get_basename(local_file),\n            )\n            os.remove(local_file)\n        else:\n            self.logger.warning(\n                \"File %s already exists. Ignoring (use the overwrite flag if you want to overwrite this file)\",\n                S3StorageHandler.get_basename(local_file),\n            )\n            return False\n\n    return True\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.connect_s3","title":"<code>connect_s3()</code>","text":"<p>Establish a connection to the S3 service.</p> <p>If the S3 client is not already instantiated, this method calls the private get_s3_client method to create an S3 client instance using the provided credentials and configuration (see __init).</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def connect_s3(self):\n    \"\"\"Establish a connection to the S3 service.\n\n    If the S3 client is not already instantiated, this method calls the private __get_s3_client\n    method to create an S3 client instance using the provided credentials and configuration (see __init__).\n    \"\"\"\n    if self.s3_client is None:\n        self.s3_client = self.__get_s3_client(\n            self.access_key_id,\n            self.secret_access_key,\n            self.endpoint_url,\n            self.region_name,\n        )\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.delete_file_from_s3","title":"<code>delete_file_from_s3(bucket, s3_obj)</code>","text":"<p>Delete a file from S3.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The S3 bucket name.</p> required <code>s3_obj</code> <code>str</code> <p>The S3 object key.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If an error occurs during the bucket access check.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def delete_file_from_s3(self, bucket, s3_obj):\n    \"\"\"Delete a file from S3.\n\n    Args:\n        bucket (str): The S3 bucket name.\n        s3_obj (str): The S3 object key.\n\n    Raises:\n        RuntimeError: If an error occurs during the bucket access check.\n    \"\"\"\n    if self.s3_client is None or bucket is None or s3_obj is None:\n        raise RuntimeError(\"Input error for deleting the file\")\n    try:\n        self.logger.info(\"Delete key s3://%s/%s\", bucket, s3_obj)\n        self.s3_client.delete_object(Bucket=bucket, Key=s3_obj)\n    except botocore.client.ClientError as e:\n        self.logger.exception(f\"Failed to delete key s3://{bucket}/{s3_obj}: {e}\")\n        raise RuntimeError(f\"Failed to delete key s3://{bucket}/{s3_obj}\") from e\n    except Exception as e:\n        self.logger.exception(f\"Failed to delete key s3://{bucket}/{s3_obj}: {e}\")\n        raise RuntimeError(f\"Failed to delete key s3://{bucket}/{s3_obj}\") from e\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.disconnect_s3","title":"<code>disconnect_s3()</code>","text":"<p>Close the connection to the S3 service.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def disconnect_s3(self):\n    \"\"\"Close the connection to the S3 service.\"\"\"\n    if self.s3_client is None:\n        return\n    self.s3_client.close()\n    self.s3_client = None\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.files_to_be_downloaded","title":"<code>files_to_be_downloaded(bucket, paths)</code>","text":"<p>Create a list with the S3 keys to be downloaded.</p> <p>The list will have the s3 keys to be downloaded from the bucket. It contains pairs (local_prefix_where_the_file_will_be_downloaded, full_s3_key_path) If a s3 key doesn't exist, the pair will be (None, requested_s3_key_path)</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The S3 bucket name.</p> required <code>paths</code> <code>list</code> <p>List of S3 object keys.</p> required <p>Returns:</p> Name Type Description <code>list_with_files</code> <code>list</code> <p>List of tuples (local_prefix, full_s3_key_path).</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def files_to_be_downloaded(self, bucket, paths):\n    \"\"\"Create a list with the S3 keys to be downloaded.\n\n    The list will have the s3 keys to be downloaded from the bucket.\n    It contains pairs (local_prefix_where_the_file_will_be_downloaded, full_s3_key_path)\n    If a s3 key doesn't exist, the pair will be (None, requested_s3_key_path)\n\n    Args:\n        bucket (str): The S3 bucket name.\n        paths (list): List of S3 object keys.\n\n    Returns:\n        list_with_files (list): List of tuples (local_prefix, full_s3_key_path).\n    \"\"\"\n    # declaration of the list\n    list_with_files: List[Any] = []\n    # for each key, identify it as a file or a folder\n    # in the case of a folder, the files will be recursively gathered\n    for key in paths:\n        path = key.strip().lstrip(\"/\")\n        s3_files = self.list_s3_files_obj(bucket, path)\n        if len(s3_files) == 0:\n            self.logger.warning(\"No key %s found.\", path)\n            list_with_files.append((None, path))\n            continue\n        self.logger.debug(\"total: %s | s3_files = %s\", len(s3_files), s3_files)\n        basename_part = self.get_basename(path)\n\n        # check if it's a file or a dir\n        if len(s3_files) == 1 and path == s3_files[0]:\n            # the current key is a file, append it to the list\n            list_with_files.append((\"\", s3_files[0]))\n            self.logger.debug(\"Append files: list_with_files = %s\", list_with_files)\n        else:\n            # the current key is a folder, append all its files (reursively gathered) to the list\n            for s3_file in s3_files:\n                split = s3_file.split(\"/\")\n                split_idx = split.index(basename_part)\n                list_with_files.append((os.path.join(*split[split_idx:-1]), s3_file.strip(\"/\")))\n\n    return list_with_files\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.files_to_be_uploaded","title":"<code>files_to_be_uploaded(paths)</code>","text":"<p>Creates a list with the local files to be uploaded.</p> <p>The list contains pairs (s3_path, absolute_local_file_path) If the local file doesn't exist, the pair will be (None, requested_file_to_upload)</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list</code> <p>List of local file paths.</p> required <p>Returns:</p> Name Type Description <code>list_with_files</code> <code>list</code> <p>List of tuples (s3_path, absolute_local_file_path).</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def files_to_be_uploaded(self, paths):\n    \"\"\"Creates a list with the local files to be uploaded.\n\n    The list contains pairs (s3_path, absolute_local_file_path)\n    If the local file doesn't exist, the pair will be (None, requested_file_to_upload)\n\n    Args:\n        paths (list): List of local file paths.\n\n    Returns:\n        list_with_files (list): List of tuples (s3_path, absolute_local_file_path).\n    \"\"\"\n\n    list_with_files = []\n    for local in paths:\n        path = local.strip()\n        # check if it is a file\n        self.logger.debug(\"path = %s\", path)\n        if os.path.isfile(path):\n            self.logger.debug(\"Add %s\", path)\n            list_with_files.append((\"\", path))\n\n        elif os.path.isdir(path):\n            for root, dir_names, filenames in os.walk(path):\n                for file in filenames:\n                    full_file_path = os.path.join(root, file.strip(\"/\"))\n                    self.logger.debug(\"full_file_path = %s | dir_names = %s\", full_file_path, dir_names)\n                    if not os.path.isfile(full_file_path):\n                        continue\n                    self.logger.debug(\n                        \"get_basename(path) = %s | root = %s | replace = %s\",\n                        self.get_basename(path),\n                        root,\n                        root.replace(path, \"\"),\n                    )\n\n                    keep_path = os.path.join(self.get_basename(path), root.replace(path, \"\").strip(\"/\")).strip(\"/\")\n                    self.logger.debug(\"path = %s | keep_path = %s | root = %s\", path, keep_path, root)\n\n                    self.logger.debug(\"Add: %s | %s\", keep_path, full_file_path)\n                    list_with_files.append((keep_path, full_file_path))\n        else:\n            self.logger.warning(\"The path %s is not a directory nor a file, it will not be uploaded\", path)\n\n    return list_with_files\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.get_basename","title":"<code>get_basename(input_path)</code>  <code>staticmethod</code>","text":"<p>Get the filename from a full path.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>The full path.</p> required <p>Returns:</p> Name Type Description <code>filename</code> <code>str</code> <p>The filename.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>@staticmethod\ndef get_basename(input_path):\n    \"\"\"Get the filename from a full path.\n\n    Args:\n        input_path (str): The full path.\n\n    Returns:\n        filename (str): The filename.\n    \"\"\"\n    path, filename = ntpath.split(input_path)\n    return filename or ntpath.basename(path)\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.get_keys_from_s3","title":"<code>get_keys_from_s3(config)</code>","text":"<p>Download S3 keys specified in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GetKeysFromS3Config</code> <p>Configuration for the S3 download.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List[str]: A list with the S3 keys that couldn't be downloaded.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Any unexpected exception raised during the download process.</p> <p>The function attempts to download files from S3 according to the provided configuration. It returns a list of S3 keys that couldn't be downloaded successfully.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>    def get_keys_from_s3(self, config: GetKeysFromS3Config) -&gt; list:\n        \"\"\"Download S3 keys specified in the configuration.\n\n        Args:\n            config (GetKeysFromS3Config): Configuration for the S3 download.\n\n        Returns:\n            List[str]: A list with the S3 keys that couldn't be downloaded.\n\n        Raises:\n            Exception: Any unexpected exception raised during the download process.\n\n        The function attempts to download files from S3 according to the provided configuration.\n        It returns a list of S3 keys that couldn't be downloaded successfully.\n\n        \"\"\"\n\n        # check the access to the bucket first, or even if it does exist\n        self.check_bucket_access(config.bucket)\n\n        # collection_files: list of files to be downloaded\n        #                   the list contains pair objects with the following\n        #                   syntax: (local_path_to_be_added_to_the_local_prefix, s3_key)\n        #                   the local_path_to_be_added_to_the_local_prefix may be none if the file doesn't exist\n        collection_files = self.files_to_be_downloaded(config.bucket, config.s3_files)\n\n        self.logger.debug(\"collection_files = %s | bucket = %s\", collection_files, config.bucket)\n        failed_files = []\n\n        for collection_file in collection_files:\n            if collection_file[0] is None:\n                failed_files.append(collection_file[1])\n                continue\n\n            local_path = os.path.join(config.local_prefix, collection_file[0].strip(\"/\"))\n            s3_file = collection_file[1]\n            # for each file to download, create the local dir (if it does not exist)\n            os.makedirs(local_path, exist_ok=True)\n            # create the path for local file\n            local_file = os.path.join(local_path, self.get_basename(s3_file).strip(\"/\"))\n\n            if not self.check_file_overwriting(local_file, config.overwrite):\n                continue\n            # download the files\n            downloaded = False\n            for keep_trying in range(config.max_retries):\n                try:\n                    self.connect_s3()\n                    dwn_start = datetime.now()\n                    self.s3_client.download_file(config.bucket, s3_file, local_file)\n                    self.logger.debug(\n                        \"s3://%s/%s downloaded to %s in %s ms\",\n                        config.bucket,\n                        s3_file,\n                        local_file,\n                        datetime.now() - dwn_start,\n                    )\n                    downloaded = True\n                    break\n                except (botocore.client.ClientError, botocore.exceptions.EndpointConnectionError) as error:\n                    self.logger.exception(\n                        \"Error when downloading the file %s. \\\nException: %s. Retrying in %s seconds for %s more times\",\n                        s3_file,\n                        error,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.disconnect_s3()\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n                except RuntimeError:\n                    self.logger.exception(\n                        \"Error when downloading the file %s. \\\nCouldn't get the s3 client. Retrying in %s seconds for %s more times\",\n                        s3_file,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n\n            if not downloaded:\n                self.logger.error(\n                    \"Could not download the file %s. The download was \\\nretried for %s times. Aborting\",\n                    s3_file,\n                    config.max_retries,\n                )\n                failed_files.append(s3_file)\n\n        return failed_files\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.get_secrets_from_file","title":"<code>get_secrets_from_file(secrets, secret_file)</code>  <code>staticmethod</code>","text":"<p>Read secrets from a specified file.</p> <p>It reads the secrets from .s3cfg or aws credentials files This function should not be used in production</p> <p>Parameters:</p> Name Type Description Default <code>secrets</code> <code>dict</code> <p>Dictionary to store retrieved secrets.</p> required <code>secret_file</code> <code>str</code> <p>Path to the file containing secrets.</p> required Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>@staticmethod\ndef get_secrets_from_file(secrets, secret_file):\n    \"\"\"Read secrets from a specified file.\n\n    It reads the secrets from .s3cfg or aws credentials files\n    This function should not be used in production\n\n    Args:\n        secrets (dict): Dictionary to store retrieved secrets.\n        secret_file (str): Path to the file containing secrets.\n    \"\"\"\n    dict_filled = 0\n    with open(secret_file, \"r\", encoding=\"utf-8\") as aws_credentials_file:\n        lines = aws_credentials_file.readlines()\n        for line in lines:\n            if not secrets[\"s3endpoint\"] and \"host_bucket\" in line:\n                dict_filled += 1\n                secrets[\"s3endpoint\"] = line.strip().split(\"=\")[1].strip()\n            elif not secrets[\"accesskey\"] and \"access_key\" in line:\n                dict_filled += 1\n                secrets[\"accesskey\"] = line.strip().split(\"=\")[1].strip()\n            elif not secrets[\"secretkey\"] and \"secret_\" in line and \"_key\" in line:\n                dict_filled += 1\n                secrets[\"secretkey\"] = line.strip().split(\"=\")[1].strip()\n            if dict_filled == 3:\n                break\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.list_s3_files_obj","title":"<code>list_s3_files_obj(bucket, prefix)</code>","text":"<p>Retrieve the content of an S3 directory.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The S3 bucket name.</p> required <code>prefix</code> <code>str</code> <p>The S3 object key prefix.</p> required <p>Returns:</p> Name Type Description <code>s3_files</code> <code>list</code> <p>List containing S3 object keys.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def list_s3_files_obj(self, bucket, prefix):\n    \"\"\"Retrieve the content of an S3 directory.\n\n    Args:\n        bucket (str): The S3 bucket name.\n        prefix (str): The S3 object key prefix.\n\n    Returns:\n        s3_files (list): List containing S3 object keys.\n    \"\"\"\n\n    s3_files = []\n\n    try:\n        paginator: Any = self.s3_client.get_paginator(\"list_objects_v2\")\n        pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n        for page in pages:\n            for item in page.get(\"Contents\", ()):\n                if item is not None:\n                    s3_files.append(item[\"Key\"])\n    except Exception as error:\n        self.logger.exception(f\"Exception when trying to list files from s3://{bucket}/{prefix}: {error}\")\n        raise RuntimeError(f\"Listing files from s3://{bucket}/{prefix}\") from error\n\n    return s3_files\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.put_files_to_s3","title":"<code>put_files_to_s3(config)</code>","text":"<p>Upload files to S3 according to the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PutFilesToS3Config</code> <p>Configuration for the S3 upload.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List[str]: A list with the local file paths that couldn't be uploaded.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Any unexpected exception raised during the upload process.</p> <p>The function attempts to upload files to S3 according to the provided configuration. It returns a list of local files that couldn't be uploaded successfully.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>    def put_files_to_s3(self, config: PutFilesToS3Config) -&gt; list:\n        \"\"\"Upload files to S3 according to the provided configuration.\n\n        Args:\n            config (PutFilesToS3Config): Configuration for the S3 upload.\n\n        Returns:\n            List[str]: A list with the local file paths that couldn't be uploaded.\n\n        Raises:\n            Exception: Any unexpected exception raised during the upload process.\n\n        The function attempts to upload files to S3 according to the provided configuration.\n        It returns a list of local files that couldn't be uploaded successfully.\n\n        \"\"\"\n\n        # check the access to the bucket first, or even if it does exist\n        self.check_bucket_access(config.bucket)\n\n        collection_files = self.files_to_be_uploaded(config.files)\n        failed_files = []\n\n        for collection_file in collection_files:\n            if collection_file[0] is None:\n                self.logger.error(\"The file %s can't be uploaded, its s3 prefix is None\", collection_file[0])\n                failed_files.append(collection_file[1])\n                continue\n\n            file_to_be_uploaded = collection_file[1]\n            # create the s3 key\n            s3_obj = os.path.join(config.s3_path, collection_file[0], os.path.basename(file_to_be_uploaded).strip(\"/\"))\n            uploaded = False\n            for keep_trying in range(config.max_retries):\n                try:\n                    # get the s3 client\n                    self.connect_s3()\n                    self.logger.info(\n                        \"Upload file %s to s3://%s/%s\",\n                        file_to_be_uploaded,\n                        config.bucket,\n                        s3_obj.lstrip(\"/\"),\n                    )\n\n                    self.s3_client.upload_file(file_to_be_uploaded, config.bucket, s3_obj)\n                    uploaded = True\n                    break\n                except (\n                    botocore.client.ClientError,\n                    botocore.exceptions.EndpointConnectionError,\n                    boto3.exceptions.S3UploadFailedError,\n                ) as error:\n                    self.logger.exception(\n                        \"Error when uploading the file %s. \\\nException: %s. Retrying in %s seconds for %s more times\",\n                        file_to_be_uploaded,\n                        error,\n                        UP_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.disconnect_s3()\n                    self.wait_timeout(UP_S3FILE_RETRY_TIMEOUT)\n                except RuntimeError:\n                    self.logger.exception(\n                        \"Error when uploading the file %s. \\\nCouldn't get the s3 client. Retrying in %s seconds for %s more times\",\n                        file_to_be_uploaded,\n                        UP_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.wait_timeout(UP_S3FILE_RETRY_TIMEOUT)\n\n            if not uploaded:\n                self.logger.error(\n                    \"Could not upload the file %s. The upload was \\\nretried for %s times. Aborting\",\n                    file_to_be_uploaded,\n                    config.max_retries,\n                )\n                failed_files.append(file_to_be_uploaded)\n\n        return failed_files\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.s3_path_parser","title":"<code>s3_path_parser(s3_url)</code>  <code>staticmethod</code>","text":"<p>Parses S3 URL to extract bucket, prefix, and file.</p> <p>Parameters:</p> Name Type Description Default <code>s3_url</code> <code>str</code> <p>The S3 URL.</p> required <p>Returns:</p> Type Description <code>bucket, prefix, s3_file) (tuple</code> <p>Tuple containing bucket, prefix, and file.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>@staticmethod\ndef s3_path_parser(s3_url):\n    \"\"\"\n    Parses S3 URL to extract bucket, prefix, and file.\n\n    Args:\n        s3_url (str): The S3 URL.\n\n    Returns:\n        (bucket, prefix, s3_file) (tuple): Tuple containing bucket, prefix, and file.\n    \"\"\"\n    s3_data = s3_url.replace(\"s3://\", \"\").split(\"/\")\n    bucket = \"\"\n    start_idx = 0\n    if s3_url.startswith(\"s3://\"):\n        bucket = s3_data[0]\n\n        start_idx = 1\n    prefix = \"\"\n    if start_idx &lt; len(s3_data):\n        prefix = \"/\".join(s3_data[start_idx:-1])\n    s3_file = s3_data[-1]\n    return bucket, prefix, s3_file\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.transfer_from_s3_to_s3","title":"<code>transfer_from_s3_to_s3(config)</code>","text":"<p>Copy S3 keys specified in the configuration. Args:     config (TransferFromS3ToS3Config): Configuration object containing bucket source, bucket destination,               S3 files, maximum retries.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of S3 keys that failed to be copied.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Any unexpected exception raised during the upload process.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>    def transfer_from_s3_to_s3(self, config: TransferFromS3ToS3Config) -&gt; list:\n        \"\"\"Copy S3 keys specified in the configuration.\n        Args:\n            config (TransferFromS3ToS3Config): Configuration object containing bucket source, bucket destination,\n                      S3 files, maximum retries.\n\n        Returns:\n            list: A list of S3 keys that failed to be copied.\n\n        Raises:\n            Exception: Any unexpected exception raised during the upload process.\n        \"\"\"\n        # check the access to both buckets first, or even if they do exist\n        self.check_bucket_access(config.bucket_src)\n        self.check_bucket_access(config.bucket_dst)\n\n        # collection_files: list of files to be downloaded\n        #                   the list contains pair objects with the following\n        #                   syntax: (local_path_to_be_added_to_the_local_prefix, s3_key)\n\n        collection_files = self.files_to_be_downloaded(config.bucket_src, config.s3_files)\n\n        self.logger.debug(\"collection_files = %s | bucket = %s\", collection_files, config.bucket_src)\n        failed_files = []\n        copy_src = {\"Bucket\": config.bucket_src, \"Key\": \"\"}\n\n        for collection_file in collection_files:\n            if collection_file[0] is None:\n                failed_files.append(collection_file[1])\n                continue\n\n            copied = False\n            for keep_trying in range(config.max_retries):\n                self.logger.debug(\n                    \"keep_trying %s | range(config.max_retries) %s \",\n                    keep_trying,\n                    range(config.max_retries),\n                )\n                try:\n                    self.connect_s3()\n                    dwn_start = datetime.now()\n                    copy_src[\"Key\"] = collection_file[1]\n                    self.logger.debug(\"copy_src = %s\", copy_src)\n                    self.s3_client.copy_object(CopySource=copy_src, Bucket=config.bucket_dst, Key=collection_file[1])\n                    self.logger.debug(\n                        \"s3://%s/%s copied to s3://%s/%s in %s ms\",\n                        config.bucket_src,\n                        collection_file[1],\n                        config.bucket_dst,\n                        collection_file[1],\n                        datetime.now() - dwn_start,\n                    )\n                    if not config.copy_only:\n                        self.delete_file_from_s3(config.bucket_src, collection_file[1])\n                        self.logger.debug(\"Key deleted s3://%s/%s\", config.bucket_src, collection_file[1])\n                    copied = True\n                    break\n                except (botocore.client.ClientError, botocore.exceptions.EndpointConnectionError) as error:\n                    self.logger.exception(\n                        \"Error when copying the file s3://%s/%s to s3://%s. \\\nException: %s. Retrying in %s seconds for %s more times\",\n                        config.bucket_src,\n                        collection_file[1],\n                        config.bucket_dst,\n                        error,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.disconnect_s3()\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n                except RuntimeError:\n                    self.logger.exception(\n                        \"Error when copying the file s3://%s/%s to s3://%s. \\\nCouldn't get the s3 client. Retrying in %s seconds for %s more times\",\n                        config.bucket_src,\n                        collection_file[1],\n                        config.bucket_dst,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n\n            if not copied:\n                self.logger.error(\n                    \"Could not copy the file s3://%s/%s to s3://%s. The copy was \\\nretried for %s times. Aborting\",\n                    config.bucket_src,\n                    collection_file[1],\n                    config.bucket_dst,\n                    config.max_retries,\n                )\n                failed_files.append(collection_file[1])\n\n        return failed_files\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.wait_timeout","title":"<code>wait_timeout(timeout)</code>","text":"<p>Wait for a specified timeout duration (minimum 200 ms).</p> <p>This function implements a simple timeout mechanism, where it sleeps for 0.2 seconds in each iteration until the cumulative sleep time reaches the specified timeout duration.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>The total duration to wait in seconds.</p> required Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def wait_timeout(self, timeout):\n    \"\"\"\n    Wait for a specified timeout duration (minimum 200 ms).\n\n    This function implements a simple timeout mechanism, where it sleeps for 0.2 seconds\n    in each iteration until the cumulative sleep time reaches the specified timeout duration.\n\n    Args:\n        timeout (float): The total duration to wait in seconds.\n\n    \"\"\"\n    time_cnt = 0.0\n    while time_cnt &lt; timeout:\n        time.sleep(SLEEP_TIME)\n        time_cnt += SLEEP_TIME\n</code></pre>"},{"location":"generate_src_doc/rs-server/common/#rs_server_common.s3_storage_handler.s3_storage_handler.TransferFromS3ToS3Config","title":"<code>TransferFromS3ToS3Config</code>  <code>dataclass</code>","text":"<p>S3 configuration for copying a list with keys between buckets</p> <p>Attributes:</p> Name Type Description <code>s3_files</code> <code>list</code> <p>A list with the S3 object keys to be copied.</p> <code>bucket_src</code> <code>str</code> <p>The source S3 bucket name.</p> <code>bucket_dst</code> <code>str</code> <p>The destination S3 bucket name.</p> <code>max_retries</code> <code>int</code> <p>The maximum number of download retries. Default is DWN_S3FILE_RETRIES.</p> Source code in <code>docs/rs-server/services/common/rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>@dataclass\nclass TransferFromS3ToS3Config:\n    \"\"\"S3 configuration for copying a list with keys between buckets\n\n    Attributes:\n        s3_files (list): A list with the S3 object keys to be copied.\n        bucket_src (str): The source S3 bucket name.\n        bucket_dst (str): The destination S3 bucket name.\n        max_retries (int, optional): The maximum number of download retries. Default is DWN_S3FILE_RETRIES.\n\n    \"\"\"\n\n    s3_files: list\n    bucket_src: str\n    bucket_dst: str\n    copy_only: bool = False\n    max_retries: int = DWN_S3FILE_RETRIES\n</code></pre>"},{"location":"generate_src_doc/rs-server/frontend/","title":"Frontend","text":"<p>The frontend application.</p>"},{"location":"generate_src_doc/rs-server/frontend/#rs_server_frontend.main.Frontend","title":"<code>Frontend</code>","text":"<p>The frontend application.</p> Source code in <code>docs/rs-server/services/frontend/rs_server_frontend/main.py</code> <pre><code>class Frontend:\n    \"\"\"The frontend application.\"\"\"\n\n    def __init__(self):\n        \"\"\"Create a frontend application.\n\n        The frontend serves the rs-server REST API documentation.\n        This documentation is an openapi specification loaded from a json file.\n        This file location is given by the RSPY_OPENAPI_FILE environment variable.\n\n        This file is loaded during the frontend application initialization\n        and is kept in memory cache for the entire life of the application.\n\n        A specific FrontendFailed exception is raised if the openapi loading failed.\n        \"\"\"\n\n        # For cluster deployment: override the swagger /docs URL from an environment variable.\n        # Also set the openapi.json URL under the same path.\n        try:\n            docs_url = env[\"RSPY_DOCS_URL\"].strip(\"/\")\n            docs_params = {\"docs_url\": f\"/{docs_url}\", \"openapi_url\": f\"/{docs_url}/openapi.json\"}\n        except KeyError:\n            docs_params = {}\n\n        try:\n            self.openapi_spec: dict = self.load_openapi_spec()\n            self.app: FastAPI = FastAPI(\n                version=__version__,\n                **docs_params,  # type: ignore\n                # Same hardcoded values than in the apikey manager\n                # (they don't appear in the openapi.json)\n                swagger_ui_init_oauth={\n                    \"clientId\": \"(this value is not used)\",\n                    \"appName\": \"API-Key Manager\",\n                    \"usePkceWithAuthorizationCodeGrant\": True,\n                },\n            )\n            self.app.openapi = self.get_openapi  # type: ignore\n        except BaseException as e:\n            raise FrontendFailed(\"Unable to serve openapi specification.\") from e\n\n        # include_in_schema=False: hide this endpoint from the swagger\n        @self.app.get(\"/health\", response_model=HealthSchema, name=\"Check service health\", include_in_schema=False)\n        async def health() -&gt; HealthSchema:\n            \"\"\"\n            Always return a flag set to 'true' when the service is up and running.\n            \\f\n            Otherwise this code won't be run anyway and the caller will have other sorts of errors.\n            \"\"\"\n            return HealthSchema(healthy=True)\n\n    @staticmethod\n    def load_openapi_spec() -&gt; dict:\n        \"\"\"Load the openapi specification.\n\n        The openapi is loaded from a json file.\n        This json file location is given by the environment variable RSPY_OPENAPI_FILE.\n\n        An IOError is raised in case of errors during the file reading.\n        A ValueError is raised in case of errors during the json parsing.\n\n        Returns:\n            the loaded openapi specification\n\n        \"\"\"\n        openapi_location = os.getenv(\"RSPY_OPENAPI_FILE\", \"\")\n        try:\n            with open(openapi_location, \"r\", encoding=\"utf-8\") as file:\n                return json.load(file)\n        except (FileNotFoundError, IOError) as e:\n            raise type(e)(\n                f\"openapi spec was not found at {openapi_location!r}. \"\n                \"Is the 'RSPY_OPENAPI_FILE' environment variable correctly set ?\",\n            ) from e\n        except ValueError as e:\n            raise ValueError(\n                f\"openapi spec was found at {openapi_location!r} but the file is not valid.\",\n            ) from e\n\n    def get_openapi(self) -&gt; dict:\n        \"\"\"Returns the openapi specification.\n\n        Returns:\n            the openapi specification as a dict.\n        \"\"\"\n        return self.openapi_spec\n</code></pre>"},{"location":"generate_src_doc/rs-server/frontend/#rs_server_frontend.main.Frontend.__init__","title":"<code>__init__()</code>","text":"<p>Create a frontend application.</p> <p>The frontend serves the rs-server REST API documentation. This documentation is an openapi specification loaded from a json file. This file location is given by the RSPY_OPENAPI_FILE environment variable.</p> <p>This file is loaded during the frontend application initialization and is kept in memory cache for the entire life of the application.</p> <p>A specific FrontendFailed exception is raised if the openapi loading failed.</p> Source code in <code>docs/rs-server/services/frontend/rs_server_frontend/main.py</code> <pre><code>def __init__(self):\n    \"\"\"Create a frontend application.\n\n    The frontend serves the rs-server REST API documentation.\n    This documentation is an openapi specification loaded from a json file.\n    This file location is given by the RSPY_OPENAPI_FILE environment variable.\n\n    This file is loaded during the frontend application initialization\n    and is kept in memory cache for the entire life of the application.\n\n    A specific FrontendFailed exception is raised if the openapi loading failed.\n    \"\"\"\n\n    # For cluster deployment: override the swagger /docs URL from an environment variable.\n    # Also set the openapi.json URL under the same path.\n    try:\n        docs_url = env[\"RSPY_DOCS_URL\"].strip(\"/\")\n        docs_params = {\"docs_url\": f\"/{docs_url}\", \"openapi_url\": f\"/{docs_url}/openapi.json\"}\n    except KeyError:\n        docs_params = {}\n\n    try:\n        self.openapi_spec: dict = self.load_openapi_spec()\n        self.app: FastAPI = FastAPI(\n            version=__version__,\n            **docs_params,  # type: ignore\n            # Same hardcoded values than in the apikey manager\n            # (they don't appear in the openapi.json)\n            swagger_ui_init_oauth={\n                \"clientId\": \"(this value is not used)\",\n                \"appName\": \"API-Key Manager\",\n                \"usePkceWithAuthorizationCodeGrant\": True,\n            },\n        )\n        self.app.openapi = self.get_openapi  # type: ignore\n    except BaseException as e:\n        raise FrontendFailed(\"Unable to serve openapi specification.\") from e\n\n    # include_in_schema=False: hide this endpoint from the swagger\n    @self.app.get(\"/health\", response_model=HealthSchema, name=\"Check service health\", include_in_schema=False)\n    async def health() -&gt; HealthSchema:\n        \"\"\"\n        Always return a flag set to 'true' when the service is up and running.\n        \\f\n        Otherwise this code won't be run anyway and the caller will have other sorts of errors.\n        \"\"\"\n        return HealthSchema(healthy=True)\n</code></pre>"},{"location":"generate_src_doc/rs-server/frontend/#rs_server_frontend.main.Frontend.get_openapi","title":"<code>get_openapi()</code>","text":"<p>Returns the openapi specification.</p> <p>Returns:</p> Type Description <code>dict</code> <p>the openapi specification as a dict.</p> Source code in <code>docs/rs-server/services/frontend/rs_server_frontend/main.py</code> <pre><code>def get_openapi(self) -&gt; dict:\n    \"\"\"Returns the openapi specification.\n\n    Returns:\n        the openapi specification as a dict.\n    \"\"\"\n    return self.openapi_spec\n</code></pre>"},{"location":"generate_src_doc/rs-server/frontend/#rs_server_frontend.main.Frontend.load_openapi_spec","title":"<code>load_openapi_spec()</code>  <code>staticmethod</code>","text":"<p>Load the openapi specification.</p> <p>The openapi is loaded from a json file. This json file location is given by the environment variable RSPY_OPENAPI_FILE.</p> <p>An IOError is raised in case of errors during the file reading. A ValueError is raised in case of errors during the json parsing.</p> <p>Returns:</p> Type Description <code>dict</code> <p>the loaded openapi specification</p> Source code in <code>docs/rs-server/services/frontend/rs_server_frontend/main.py</code> <pre><code>@staticmethod\ndef load_openapi_spec() -&gt; dict:\n    \"\"\"Load the openapi specification.\n\n    The openapi is loaded from a json file.\n    This json file location is given by the environment variable RSPY_OPENAPI_FILE.\n\n    An IOError is raised in case of errors during the file reading.\n    A ValueError is raised in case of errors during the json parsing.\n\n    Returns:\n        the loaded openapi specification\n\n    \"\"\"\n    openapi_location = os.getenv(\"RSPY_OPENAPI_FILE\", \"\")\n    try:\n        with open(openapi_location, \"r\", encoding=\"utf-8\") as file:\n            return json.load(file)\n    except (FileNotFoundError, IOError) as e:\n        raise type(e)(\n            f\"openapi spec was not found at {openapi_location!r}. \"\n            \"Is the 'RSPY_OPENAPI_FILE' environment variable correctly set ?\",\n        ) from e\n    except ValueError as e:\n        raise ValueError(\n            f\"openapi spec was found at {openapi_location!r} but the file is not valid.\",\n        ) from e\n</code></pre>"},{"location":"generate_src_doc/rs-server/frontend/#rs_server_frontend.main.FrontendFailed","title":"<code>FrontendFailed</code>","text":"<p>               Bases: <code>BaseException</code></p> <p>Exception raised if the frontend initialization failed.</p> Source code in <code>docs/rs-server/services/frontend/rs_server_frontend/main.py</code> <pre><code>class FrontendFailed(BaseException):\n    \"\"\"Exception raised if the frontend initialization failed.\"\"\"\n</code></pre>"},{"location":"generate_src_doc/rs-server/frontend/#rs_server_frontend.main.HealthSchema","title":"<code>HealthSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Health status flag.</p> Source code in <code>docs/rs-server/services/frontend/rs_server_frontend/main.py</code> <pre><code>class HealthSchema(BaseModel):\n    \"\"\"Health status flag.\"\"\"\n\n    healthy: bool\n</code></pre>"},{"location":"generate_src_doc/rs-server/frontend/#rs_server_frontend.main.start_app","title":"<code>start_app()</code>","text":"<p>Start the starlette app.</p> <p>Factory function that starts the application.</p> <p>Returns:</p> Type Description <code>FastAPI</code> <p>the initialized application</p> Source code in <code>docs/rs-server/services/frontend/rs_server_frontend/main.py</code> <pre><code>def start_app() -&gt; FastAPI:\n    \"\"\"Start the starlette app.\n\n    Factory function that starts the application.\n\n    Returns:\n        the initialized application\n\n    \"\"\"\n    return Frontend().app\n</code></pre>"},{"location":"generate_src_doc/rs-server/swagger_index/","title":"SwaggerUI","text":"<p>To access the Swagger API, click this link: swagger</p> <p>Or use it dirrectly from here</p>"},{"location":"rs-client-libraries/docs/doc/","title":"RS Client Libraries overview","text":"<p>The RS-Client libraries allows the user to call all the RS-Server services. It is composed mainly from the base class RsClient and the classes which extend this base class: AuxipClient, CadipClient and StacClient.</p> <p></p> <p>The RsClient class is a base class for interacting with the RS-Server. It provides methods for managing and retrieving files about various resources and stations available on the server. The class includes methods for checking file download statuses, staging files for a future processing, and searching for files within specified time ranges. It also provides methods for creating instances of its child classes: AuxipClient, CadipClient, and StacClient.</p> <p>The AuxipClient class extends the RsClient class to provide specific functionality for interfacing with the ADGS (Auxiliary Data Generation System) endpoints runnin on the RS-Server. This class overrides the necessary properties to define the ADGS-specific endpoints for searching, staging, and checking the status of file downloads.</p> <p>The CadipClient class extends the RsClient class to provide specific functionality for interfacing with the CADIP (CADU Ingestion Platform) endpoints running on the RS-Server. This class overrides the necessary properties to define the CADIP-specific endpoints for searching sessions, staging files, and checking the status of file downloads.</p> <p>The StacClient class extends the RsClient class and the PySTAC to provide specific functionality for interfacing with the STAC (SpatioTemporal Asset Catalog) endpoints on the RS-Server. This class overrides the necessary properties to define the STAC-specific endpoints. Please check also the PyStac client usage documentation for how to use the read catalog methods.</p> <p></p>"},{"location":"rs-demo/","title":"Running Modes","text":"<p>In this page, we will see how to run the Jupyter notebooks on cluster, local and hybrid mode.</p>"},{"location":"rs-demo/#quick-links","title":"Quick links","text":"<ul> <li> <p>On cluster mode:</p> <ul> <li>JupyterHub: https://processing.ops.rs-python.eu/jupyter</li> <li>RS-Server website (Swagger/OpenAPI): https://rspy.ops.rs-python.eu/docs</li> <li>Create an API key: https://apikeymanager.rspy.ops.rs-python.eu/docs#/Manage%20API%20keys/create_api_key_auth_api_key_new_get</li> <li>Prefect dashboard (orchestrator): https://processing.ops.rs-python.eu/</li> <li>Grafana (logs, traces, metrics): https://monitoring.ops.rs-python.eu/</li> </ul> </li> <li> <p>On hybrid mode:</p> <ul> <li>RS-Server website (Swagger/OpenAPI): https://rspy.ops.rs-python.eu/docs</li> <li>Create an API key: https://apikeymanager.rspy.ops.rs-python.eu/docs#/Manage%20API%20keys/create_api_key_auth_api_key_new_get</li> <li>Prefect dashboard (orchestrator): http://localhost:4200</li> <li>Grafana (logs, traces, metrics): http://localhost:3000/explore</li> </ul> </li> <li> <p>On local mode: </p> <ul> <li> <p>RS-Server website (Swagger/OpenAPI):</p> <ul> <li>http://localhost:8000/docs (frontend, only for visualization, not functional)</li> <li>http://localhost:8001/docs (auxip)</li> <li>http://localhost:8002/docs (cadip)</li> <li>http://localhost:8003/api.html (catalog)</li> </ul> </li> <li> <p>Prefect dashboard (orchestrator): http://localhost:4200</p> </li> <li>Grafana (logs, traces, metrics): http://localhost:3000/explore</li> <li> <p>Minio s3 bucket: http://localhost:9001/browser with:</p> <ul> <li>Username: <code>_minio_</code></li> <li>Password: <code>_Strong#Pass#1234_</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"rs-demo/#run-on-cluster-mode","title":"Run on cluster mode","text":"<p>On cluster mode, we run the Jupyter notebooks from our JupyterHub session deployed on the cluster. They connect to the services deployed on the RS-Server website (=cluster). Authentication is required for this mode.</p>"},{"location":"rs-demo/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have access to JupyterHub: https://processing.ops.rs-python.eu/jupyter</li> <li>You have access to the RS-Server website: https://rspy.ops.rs-python.eu/docs</li> <li>You have generated an API key from the RS-Server website.</li> </ul>"},{"location":"rs-demo/#run-the-demos-on-cluster-mode","title":"Run the demos on cluster mode","text":"<ul> <li>Open a JupyterHub session.</li> <li> <p>Open a terminal, check that <code>rs-client-libraries</code> is installed by running:</p> <pre><code>pip show rs-client-libraries # should show the name, version, ...\n</code></pre> </li> <li> <p>Optional: save your API key into your <code>~/.env</code> file so it is loaded automatically by your notebooks: </p> <ol> <li>Go to the menu View -&gt; Show Hidden Files</li> <li>In the file browser in the left panel, check that you are in your <code>/</code> home folder.</li> <li>Double-click the <code>.env</code> file to open it using the integrated text editor.</li> <li>Add this line to the file, save it with Ctrl-S and close the editor:</li> </ol> <pre><code># Replace by your value\nexport RSPY_APIKEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n</code></pre> </li> <li> <p>On the left, in the file explorer, go to the demos or tutorial folder and double-click a notebook to open it:</p> </li> </ul> <p></p>"},{"location":"rs-demo/#install-a-new-rs-client-libraries-version","title":"Install a new <code>rs-client-libraries</code> version","text":""},{"location":"rs-demo/#option-1-update-the-jupyterhub-image-affects-everyone","title":"Option 1: update the JupyterHub image (affects everyone)","text":"<ol> <li> <p>V\u00e9rify in the CI/CD that the last <code>rs-client-libraries</code> modifications were merged into the <code>develop</code> branch: https://github.com/RS-PYTHON/rs-client-libraries/actions/workflows/publish-binaries.yml</p> </li> <li> <p>Ask the <code>rs-infrastructure</code> administrator to run a new CI/CD workflow to publish this <code>rs-client-libraries</code> version into a new JupyterHub image.</p> </li> </ol>"},{"location":"rs-demo/#option-2-from-a-wheel-package-affects-only-you","title":"Option 2: from a wheel package (affects only you)","text":"<ol> <li> <p>In the CI/CD, click on the last <code>rs-client-libraries</code> branch workflow that you want to use: https://github.com/RS-PYTHON/rs-client-libraries/actions/workflows/publish-binaries.yml, go to the <code>Artifacts</code> section and download the <code>.whl</code> package file.</p> </li> <li> <p>Or alternatively, build the wheel yourself from your local <code>rs-client-libraries</code> project by running: <code>poetry build --format wheel</code></p> </li> <li> <p>Upload the <code>.whl</code> package file to your JupyterHub session, open a Terminal and run:</p> <pre><code># Uninstall the old version. Note: this fails if we do it for the first time because \n# we try to uninstall the root installation of the library, but this this OK.\npip uninstall -y rs-client-libraries 2&gt;/dev/null\n\n# You may have conflicts between dependencies installed for the root user\n# and the current user. You can uninstall all current user dependencies with:\n# for dep in $(pip freeze | cut -d \"@\" -f1); do pip uninstall -y $dep 2&gt;/dev/null; done\n\n# The old rs-client-libraries version is still installed for the root user.\n# This is the version you (=current user) use by default.\npip show rs-client-libraries | grep Location # should display: /opt/conda/lib/python3.11/site-packages\n\n# Install the new version for the current user.\npip install rs_client_libraries-&lt;version&gt;-py3-none-any.whl\n</code></pre> </li> </ol>"},{"location":"rs-demo/#run-on-local-mode","title":"Run on local mode","text":"<p>On local mode, docker-compose and Docker images are used to run services and libraries locally (not on a cluster). There is no authentication for this mode.</p>"},{"location":"rs-demo/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>You have Docker installed on your system, see: https://docs.docker.com/engine/install/</li> <li>You have access to the RSPY project on GitHub: https://github.com/RS-PYTHON</li> <li> <p>You have created a personal access token (PAT) on GitHub: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens</p> <ul> <li>This access token is used to retrieve the rs-server product on the github repository.</li> <li>You may want to create a classic PAT with the <code>read:packages</code> permissions.</li> </ul> </li> <li> <p>You have checked out this git project:</p> <pre><code>git clone git@github.com:RS-PYTHON/rs-demo.git # either with SSH\n# git clone https://github.com/RS-PYTHON/rs-demo.git # or with HTTPS\n\n# Get last version\ncd rs-demo\ngit checkout develop\n</code></pre> </li> </ul>"},{"location":"rs-demo/#run-the-demos-on-local-mode","title":"Run the demos on local mode","text":"<p>To pull the latest Docker images, run:</p> <pre><code># Login into the project ghcr.io (GitHub Container Registry)\n# Username: your GitHub login\n# Password: your personal access token (PAT) created above\ndocker login https://ghcr.io/v2/rs-python\n\n# From the local-mode directory, pull the images\ncd ./local-mode\ndocker compose pull\n</code></pre> <p>Then to run the demos:</p> <pre><code># Still from the local-mode directory, if you're not there yet\ncd ./local-mode\n\n# Run all services.\n# Note: in case of port conflicts, you can kill all your running docker containers with:\n# docker rm -f $(docker ps -aq)\ndocker compose down -v; docker compose up # -d for detached\n\n# Note: we always need to call 'down' before 'up' or we'll have errors\n# when the stac database will initialize a second time.\n</code></pre> <p>Near the end of the logs you will see some Jupyter information e.g: <pre><code>jupyter | To access the server, open this file in a browser:\njupyter |     ...\njupyter | Or copy and paste one of these URLs:\njupyter |     ...\njupyter |     http://127.0.0.1:8888/lab?token=612cb124335d9ab80a5a6414631a7df186b2401234050001\n</code></pre></p> <p>Open (ctrl-click) the <code>http://127.0.0.1:8888/lab?token=...</code> link to open the Jupyter web client (=Jupyter Notebook) in your browser.</p> <p>Note: the token is auto-generated by Jupyter and changes everytime you relaunch the containers. So after relaunching, your old Jupyter web session won't be available anymore.</p> <p>To show the Jupyter logs from another terminal, run:</p> <pre><code>docker compose logs jupyter\n</code></pre> <p>On the left, in the file explorer, go to the demos or tutorial folder and double-click a notebook to open it:</p> <p></p> <pre><code># When you're done, shutdown all services and volumes (-v)\n# with Ctrl-C (if not in detached mode i.e. -d) then:\ndocker compose down -v\n\n# You can use this to remove all docker volumes \n# (use with care if you have other docker containers)\ndocker volume prune\n</code></pre>"},{"location":"rs-demo/#how-does-it-work","title":"How does it work","text":"<p>The docker-compose.yml file uses Docker images to run all the necessary container services for the demos :</p> <ul> <li>The latest rs-server images available:<ul> <li>Built from the CI/CD: https://github.com/RS-PYTHON/rs-server/actions/workflows/publish-binaries.yml</li> <li>Available in the ghcr.io: https://github.com/orgs/RS-PYTHON/packages</li> </ul> </li> <li>The AUXIP, CADIP ... station mockups:<ul> <li>Built from the CI/CD: https://github.com/RS-PYTHON/rs-testmeans/actions/workflows/publish-docker.yml</li> <li>Also available in the ghcr.io</li> </ul> </li> <li>STAC PostgreSQL database</li> <li>MinIO S3 bucket server</li> <li>Jupyter server</li> </ul> <p>These containers are run locally (not on a cluster). The Jupyter notebooks accessed from http://127.0.0.1:8888 are run from the containerized Jupyter server, not from your local environment. This Jupyter environment contains all the Python modules required to call the rs-server HTTP endpoints.</p>"},{"location":"rs-demo/#how-to-run-your-local-rs-server-code-in-this-environment","title":"How to run your local rs-server code in this environment","text":"<p>It can be helpful to use your last rs-server code version to debug it or to test modifications without pushing them and rebuilding the Docker image. Follow these steps:</p> <ol> <li> <p>Go to the <code>local-mode</code> directory and run:  </p> <pre><code>cp 'docker-compose.yml' 'docker-compose-debug.yml'\n</code></pre> </li> <li> <p>If your local <code>rs-server</code> github repository is under <code>/my/local/rs-server</code>, modify the <code>docker-compose-debug.yml</code> file to mount your local <code>rs-server</code> services:  </p> <pre><code># e.g.\nrs-server-adgs:\n  # ...\n  volumes:\n    - /my/local/rs-server/services/common/rs_server_common:/usr/local/lib/python3.11/site-packages/rs_server_common\n    - /my/local/rs-server/services/adgs/rs_server_adgs:/usr/local/lib/python3.11/site-packages/rs_server_adgs\n    - /my/local/rs-server/services/adgs/config:/usr/local/lib/python3.11/site-packages/config\n    # - and any other useful files ...\n</code></pre> </li> <li> <p>Run the demo with:  </p> <pre><code># Still from the local-mode directory, if you're not there yet\ncd ./local-mode\n\n# Run all services\ndocker compose down -v; docker compose -f docker-compose-debug.yml up # -d for detached\n</code></pre> </li> </ol>"},{"location":"rs-demo/#run-on-hybrid-mode","title":"Run on hybrid mode","text":"<p>On hybrid mode, we run the Jupyter notebooks locally, but they connect to the services deployed on the RS-Server website (=cluster). Authentication is required for this mode.</p>"},{"location":"rs-demo/#prerequisites_2","title":"Prerequisites","text":"<ul> <li>You have access to the RS-Server website: https://rspy.ops.rs-python.eu/docs</li> <li>You have generated an API key from the RS-Server website.</li> <li>You have saved the S3 bucket configuration in you local file: <code>~/.s3cfg</code></li> <li>Python is installed on your system.</li> </ul> <p>You also need the rs-client-libraries project:</p> <ul> <li> <p>If you have its source code, install it with:</p> <pre><code>cd /path/to/rs-client-libraries\npip install poetry\npoetry install --with dev,demo\npoetry run opentelemetry-bootstrap -a install\n</code></pre> </li> <li> <p>Or if you only have its <code>.whl</code> package, install it with: </p> <pre><code>pip install rs_client_libraries-*.whl\n# then install jupyter lab\npip install jupyterlab\n</code></pre> </li> </ul>"},{"location":"rs-demo/#run-the-demos-on-hybrid-mode","title":"Run the demos on hybrid mode","text":"<p>From your terminal in the rs-demo, run:</p> <pre><code>export RSPY_APIKEY=your_api_key # see the prerequisites\n\n# NOTE: at CS France premises, use this to deactivate the proxy which causes random errors\nunset no_proxy ftp_proxy https_proxy http_proxy\n\n# To use your local rs-client-libraries source code\ncd /path/to/rs-client-libraries\n# git checkout develop &amp;&amp; git pull # maybe take the latest default branch\npoetry run /path/to/rs-demo/hybrid-mode/start-jupyterlab.sh\n\n# Or if you have installed it from rs_client_libraries-*.whl, \n# just run\n/path/to/rs-demo/hybrid-mode/start-jupyterlab.sh\n</code></pre> <p>The Jupyter web client (=Jupyter Notebook) opens in a new tab of your browser. </p> <p>WARNING: the cluster is shut down from 18h30 to 8h00 each night and on the weekends.</p>"},{"location":"rs-demo/#how-to-check-your-python-interpreter-used-in-notebooks","title":"How to check your Python interpreter used in notebooks","text":"<p>In a notebook cell, run:  <pre><code>import sys\nprint(sys.executable)\n</code></pre></p> <p>If you use the rs-client-libraries poetry environment, it should show something like: <pre><code>${HOME}/.cache/pypoetry/virtualenvs/rs-client-libraries-xxxxxxxx-py3.11/bin/python\n</code></pre></p>"},{"location":"rs-demo/doc/","title":"Running a notebook","text":"<p>You can run the Jupyter notebooks either on: </p> <ul> <li>Cluster mode</li> <li>Local mode</li> <li>Hybrid mode</li> </ul>"},{"location":"rs-helm/","title":"RS-Server deployment","text":""},{"location":"rs-helm/#rs-server-helm-charts","title":"RS-Server Helm Charts","text":"<p>This repository contains the official RS-Server Helm charts for installing and configuring <code>rs-server</code> on Kubernetes.</p>"},{"location":"rs-helm/#rs-server-adgs","title":"rs-server-adgs","text":"<p>The Auxiliary Data Gathering Service (ADGS) is a pick-up point for Sentinel auxiliary files. This service allows clients to discover and retrieve available auxiliary data files through a standard OData RESTful API. The following endpoints have been implemented in RS-Server to interact with ADGS RESTful API.</p>"},{"location":"rs-helm/#rs-server-cadip","title":"rs-server-cadip","text":"<p>The CADU Interface delivery Point (CADIP) is a pick-up point for Sentinel CADU data. The CADIP allows clients to straightforwardly discover and retrieve available data files through a standard OData RESTful API. The following endpoints have been implemented in RS-Server to interact with CADIP RESTful API.</p>"},{"location":"rs-helm/#rs-server-catalog","title":"rs-server-catalog","text":"<p>The catalog is a STAC catalog that contains the medatadata of the products. It is organized in collections and items. It is based on STAC FastAPI.</p>"},{"location":"rs-helm/#rs-server-catalog-db","title":"rs-server-catalog-db","text":"<p>The catalog's database is a PostgreSQL database with the PostGIS extension. It is based on PgSTAC.</p>"},{"location":"rs-helm/#rs-server-frontend","title":"rs-server-frontend","text":"<p>The frontend is a simple FastAPI interface based on Swagger exposing the endpoints. A user can use it to interact with the rs-server backends (CADIP, ADGS, catalog, etc).</p>"},{"location":"rs-helm/#mockup-station-adgs","title":"mockup-station-adgs","text":"<p>A simple mock for ADGS station.</p>"},{"location":"rs-helm/#mockup-station-cadip","title":"mockup-station-cadip","text":"<p>A simple mock for CADIP station.</p>"},{"location":"rs-helm/#mockup-processor-dpr","title":"mockup-processor-dpr","text":"<p>A simple mock for DPR processor.</p> <p>Find more documentation on the rs-server repository.</p>"},{"location":"rs-helm/#usage","title":"Usage","text":"<p>Helm must be installed to use the charts. Please refer to Helm's documentation to get started.</p>"},{"location":"rs-helm/#tldr","title":"TL;DR","text":"<pre><code>helm repo add rs-python https://home.rs-python.eu/rs-helm\nhelm repo update rs-python\nhelm search repo rs-python --versions --devel\nhelm install my-release rs-python/&lt;chart&gt;\n</code></pre>"},{"location":"rs-helm/#installing","title":"Installing","text":"<p>Find below an example on how to install the <code>rs-server-frontend</code> component and set some values:</p> <pre><code>helm install rs-server-frontend rs-python/rs-server-frontend --namespace=processing --set ingress.host=dev-rspy.esa-copernicus.eu --set image.version=latest --set image.PullPolicy=Always  \n</code></pre>"},{"location":"rs-helm/#upgrading","title":"Upgrading","text":"<p>Find below an example on how to upgrade the <code>rs-server-cadip</code> component, re-use previously set values, and set some values:</p> <pre><code>helm upgrade rs-server-cadip rs-python/rs-server-cadip --namespace=processing --reuse-values --set obs.endpoint=https://oss.eu-west-0.prod-cloud-ocb.orange-business.com --set obs.region=eu-west-0 --set obs.secret.ak=XXXXXXXXXXXXXX --set obs.secret.sk=YYYYYYYYYYYYYYYYYYYYY --force --version 0.0.0-f9c864f\n</code></pre> <p> </p> <p>This project is funded by the EU and ESA.</p> <p> </p>"},{"location":"rs-helm/CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p> <p>Content of release :</p> <ul> <li>Added for new features.</li> <li>Changed for changes in existing functionality.</li> <li>Deprecated for soon-to-be removed features.</li> <li>Removed for now removed features.</li> <li>Fixed for any bug fixes.</li> <li>Security in case of vulnerabilities.</li> </ul>"},{"location":"rs-helm/CHANGELOG/#02a1-2024-06-26","title":"[0.2a1] - 2024-06-26","text":""},{"location":"rs-helm/CHANGELOG/#added","title":"Added","text":"<ul> <li>Liveness and Readiness probes for mdockup-processor-dpr, mockup-station-cadip, mockup-station-adgs and rs-server-catalog</li> <li><code>presignedUrlExpirationTime</code> for rs-server-catalog</li> <li><code>cadipSessionExpand</code> for mockup-station-cadip</li> <li><code>uacHomeUrl</code> for rs-server-frontend</li> </ul>"},{"location":"rs-helm/CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Renamed <code>uacURL</code> to <code>uacCheckUrl</code> for rs-server-frontend</li> <li>CI/CD to take into account non SemVer 2 versions from rs-server</li> <li>ConfigMap for mockup-station-cadip</li> <li>Bumped Image version of docker images</li> </ul>"},{"location":"rs-helm/CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>Liveness and Readiness probes for rs-server-catalog-db</li> </ul>"},{"location":"rs-helm/CHANGELOG/#01a10-2024-06-12","title":"[0.1a10] - 2024-06-12","text":""},{"location":"rs-helm/CHANGELOG/#added_1","title":"Added","text":"<ul> <li>Grafana Tempo for rs-server-cadip, rs-server-auxip, rs-server-catalog</li> <li>PVC for mockup-station-cadip and mockup-station-adgs</li> <li>ConfigMap for mockup-station-cadip and mockup-station-adgs</li> </ul>"},{"location":"rs-helm/CHANGELOG/#changed_1","title":"Changed","text":"<ul> <li>Ingress : added a parameter to enabled or not in the value file. Enabled is set to true by default.</li> <li>Image version for mockup</li> </ul>"},{"location":"rs-helm/CHANGELOG/#01a8-2024-04-30","title":"[0.1a8] - 2024-04-30","text":""},{"location":"rs-helm/CHANGELOG/#added_2","title":"Added","text":"<ul> <li>DPR mockup</li> </ul>"},{"location":"rs-helm/CHANGELOG/#changed_2","title":"Changed","text":"<ul> <li>Set tag from <code>latest</code> to <code>0.1a8</code> to match rs-server latest version.</li> </ul>"},{"location":"rs-helm/CHANGELOG/#removed","title":"Removed","text":"<ul> <li><code>image.tag</code> in favor of <code>image.version</code></li> </ul>"},{"location":"rs-helm/CHANGELOG/#01a7-2024-04-16","title":"[0.1a7] - 2024-04-16","text":""},{"location":"rs-helm/CHANGELOG/#added_3","title":"Added","text":"<p>First release of RS-Server Helm Charts.</p>"},{"location":"rs-helm/charts/mockup-processor-dpr/","title":"mockup-processor-dpr","text":"<p>MOCKUP PROCESSOR DPR</p>"},{"location":"rs-helm/charts/mockup-processor-dpr/#maintainers","title":"Maintainers","text":"Name Email Url CS GROUP https://github.com/RS-PYTHON/rs-helm"},{"location":"rs-helm/charts/mockup-processor-dpr/#values","title":"Values","text":"Key Type Default Description app.port int <code>8000</code> Port for the application image.PullPolicy string <code>\"IfNotPresent\"</code> Image pull policy image.name string <code>\"rs-testmeans_dpr-processor-mock\"</code> Image name image.registry string <code>\"ghcr.io\"</code> Image registry image.repository string <code>\"rs-python\"</code> Image repository image.version string <code>\"sha256:17567678389023285a48360472d87ad5960f2224d30dfe65726ad17c771f8a56\"</code> Image version, can be a tag or a digest namespace string <code>\"processing\"</code> Namespace for the deployment obs.endpoint string <code>\"http://minio.minio.svc.cluster.local:9000\"</code> URL of the object storage service endpoint obs.region string <code>\"sbg\"</code> Region of the object storage service obs.secret.ak string <code>\"TDr8foJqSygBQ9YFmWDy\"</code> Access Key to authenticate with the object storage service obs.secret.sk string <code>\"z2RaqjFttnVZRTsLLqmy4PE6PzJOKzPsE47alDBs\"</code> Secret Key to authenticate with the object storage service probe.liveness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the liveness probe probe.liveness.path string <code>\"/health\"</code> Path for the liveness probe probe.liveness.periodSeconds int <code>30</code> periodSeconds for the liveness probe probe.liveness.port int <code>8000</code> Port for the liveness probe probe.liveness.timeoutSeconds int <code>5</code> timeoutSeconds for the liveness probe probe.readiness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the readiness probe probe.readiness.path string <code>\"/health\"</code> Path for the readiness probe probe.readiness.periodSeconds int <code>30</code> periodSeconds for the readiness probe probe.readiness.port int <code>8000</code> Port for the readiness probe probe.readiness.timeoutSeconds int <code>5</code> timeoutSeconds for the readiness probe replicaCount int <code>1</code> Number of replicas for the deployment resources.limit.cpu string <code>\"500m\"</code> Pod CPU limit resources.limit.ram string <code>\"1000Mi\"</code> Pod memory limit resources.request.cpu string <code>\"100m\"</code> Pod CPU request resources.request.ram string <code>\"256Mi\"</code> Pod memory request service.port int <code>8080</code> Port for the service <p>Autogenerated from chart metadata using helm-docs v1.12.0</p>"},{"location":"rs-helm/charts/mockup-station-adgs/","title":"mockup-station-adgs","text":"<p>MOCKUP STATION ADGS</p>"},{"location":"rs-helm/charts/mockup-station-adgs/#maintainers","title":"Maintainers","text":"Name Email Url CS GROUP https://github.com/RS-PYTHON/rs-helm"},{"location":"rs-helm/charts/mockup-station-adgs/#values","title":"Values","text":"Key Type Default Description app.authConfigFile string <code>\"auth.json\"</code> Auth configuration file for the application app.catalogConfigFile string <code>\"GETFileResponse.json\"</code> catalog configuration file for the application app.confDir string <code>\"/opt/adgs/config\"</code> Config directory for the application app.port int <code>5000</code> Port for the application app.workDir string <code>\"/app\"</code> Working directory for the application helmResourcePolicy string <code>\"keep\"</code> Keep the ressources for PVC and ConfigMap. Default is to keep. image.PullPolicy string <code>\"Always\"</code> Image pull policy image.name string <code>\"rs-testmeans_adgs-station-mock\"</code> Image name image.registry string <code>\"ghcr.io\"</code> Image registry image.repository string <code>\"rs-python\"</code> Image repository image.version string <code>\"sha256:91e57f397e886d615c2ca2f66e8d397520e47a5fc45fb94166b880b4882a9185\"</code> Image version, can be a tag or a digest namespace string <code>\"processing\"</code> Namespace for the deployment probe.liveness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the liveness probe probe.liveness.path string <code>\"/health\"</code> Path for the liveness probe probe.liveness.periodSeconds int <code>30</code> periodSeconds for the liveness probe probe.liveness.port int <code>5000</code> Port for the liveness probe probe.liveness.timeoutSeconds int <code>5</code> timeoutSeconds for the liveness probe probe.readiness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the readiness probe probe.readiness.path string <code>\"/health\"</code> Path for the readiness probe probe.readiness.periodSeconds int <code>30</code> periodSeconds for the readiness probe probe.readiness.port int <code>5000</code> Port for the readiness probe probe.readiness.timeoutSeconds int <code>5</code> timeoutSeconds for the readiness probe replicaCount int <code>1</code> Number of replicas for the deployment resources.limit.cpu string <code>\"500m\"</code> Pod CPU limit resources.limit.ram string <code>\"1000Mi\"</code> Pod memory limit resources.request.cpu string <code>\"100m\"</code> Pod CPU request resources.request.ram string <code>\"256Mi\"</code> Pod memory request service.port int <code>8080</code> Port for the service volume.accessModes string <code>\"ReadWriteOnce\"</code> AccessMode of the database volume volume.size string <code>\"10Gi\"</code> Size of the database volume volume.storageClassName string <code>\"csi-cinder-sc-retain\"</code> StorageClass of the database volume <p>Autogenerated from chart metadata using helm-docs v1.12.0</p>"},{"location":"rs-helm/charts/mockup-station-cadip/","title":"mockup-station-cadip","text":"<p>MOCKUP STATION CADIP</p>"},{"location":"rs-helm/charts/mockup-station-cadip/#maintainers","title":"Maintainers","text":"Name Email Url CS GROUP https://github.com/RS-PYTHON/rs-helm"},{"location":"rs-helm/charts/mockup-station-cadip/#values","title":"Values","text":"Key Type Default Description app.authConfigFile string <code>\"auth.json\"</code> Auth configuration file for the application app.cadipSessionExpand bool <code>true</code> Support for expand option, default true app.catalogConfigFile string <code>\"FileResponse.json\"</code> catalog configuration file for the application app.confDir string <code>\"/opt/cadip/config\"</code> Config directory for the application app.port int <code>5000</code> Port for the application app.qualityConfigFile string <code>\"QualityInfoResponse.json\"</code> quality configuration file for the application app.sessionIDConfigFile string <code>\"SPJ.json\"</code> sessionID configuration file for the application app.stationName string <code>\"cadip\"</code> Name of the station for the application app.workDir string <code>\"/app\"</code> Working directory for the application helmResourcePolicy string <code>\"keep\"</code> Keep the ressources for PVC and ConfigMap. Default is to keep. image.PullPolicy string <code>\"Always\"</code> Image pull policy image.name string <code>\"rs-testmeans_cadip-station-mock\"</code> Image name image.registry string <code>\"ghcr.io\"</code> Image registry image.repository string <code>\"rs-python\"</code> Image repository image.version string <code>\"sha256:4803a989ef66f6371645690ac9aa7e2fbdb1c146d19ffd937b01d0663d331811\"</code> Image version, can be a tag or a digest namespace string <code>\"processing\"</code> Namespace for the deployment probe.liveness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the liveness probe probe.liveness.path string <code>\"/health\"</code> Path for the liveness probe probe.liveness.periodSeconds int <code>30</code> periodSeconds for the liveness probe probe.liveness.port int <code>5000</code> Port for the liveness probe probe.liveness.timeoutSeconds int <code>5</code> timeoutSeconds for the liveness probe probe.readiness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the readiness probe probe.readiness.path string <code>\"/health\"</code> Path for the readiness probe probe.readiness.periodSeconds int <code>30</code> periodSeconds for the readiness probe probe.readiness.port int <code>5000</code> Port for the readiness probe probe.readiness.timeoutSeconds int <code>5</code> timeoutSeconds for the readiness probe replicaCount int <code>1</code> Number of replicas for the deployment resources.limit.cpu string <code>\"500m\"</code> Pod CPU limit resources.limit.ram string <code>\"1000Mi\"</code> Pod memory limit resources.request.cpu string <code>\"100m\"</code> Pod CPU request resources.request.ram string <code>\"256Mi\"</code> Pod memory request service.port int <code>8080</code> Port for the service volume.accessModes string <code>\"ReadWriteOnce\"</code> AccessMode of the database volume volume.size string <code>\"50Gi\"</code> Size of the database volume volume.storageClassName string <code>\"csi-cinder-sc-retain\"</code> StorageClass of the database volume <p>Autogenerated from chart metadata using helm-docs v1.12.0</p>"},{"location":"rs-helm/charts/rs-server-adgs/","title":"rs-server-adgs","text":"<p>RS SERVER ADGS</p>"},{"location":"rs-helm/charts/rs-server-adgs/#maintainers","title":"Maintainers","text":"Name Email Url CS GROUP https://github.com/RS-PYTHON/rs-helm"},{"location":"rs-helm/charts/rs-server-adgs/#values","title":"Values","text":"Key Type Default Description app.confDir string <code>\"/app/conf\"</code> Config directory for the application app.eodagConfigFile string <code>\"adgs_ws_config.yaml\"</code> EODAG configuration file for the application app.port int <code>8000</code> Port for the application app.station.endpoint.secret.password string <code>\"test\"</code> Password to authenticate with the ADGS endpoint app.station.endpoint.secret.username string <code>\"test\"</code> Username to authenticate with the ADGS endpoint app.station.endpoint.url string <code>\"http://mockup-station-adgs-svc.processing.svc.cluster.local:8080/Products\"</code> ADGS URL app.stationConfigFile string <code>\"stations_cfg.json\"</code> Station configuration file for the application app.uacURL string <code>\"http://apikeymanager.processing.svc.cluster.local:8000/check/api_key\"</code> URL of the API Key Manager service app.workDir string <code>\"/app\"</code> Working directory for the application image.PullPolicy string <code>\"IfNotPresent\"</code> Image pull policy image.name string <code>\"rs-server-adgs\"</code> Image name image.registry string <code>\"ghcr.io\"</code> Image registry image.repository string <code>\"rs-python\"</code> Image repository image.version string <code>\"0.2a1\"</code> Image version, can be a tag or a digest ingress.enabled bool <code>true</code> Enabled/Disable ingress ingress.host string <code>\"subdomain.example.com\"</code> Ingress host name ingress.issuer.name string <code>\"letsencrypt-prod\"</code> Ingress Issuer name ingress.issuer.type string <code>\"cluster-issuer\"</code> Ingress Issuer type ingress.path string <code>\"/adgs\"</code> Ingress path namespace string <code>\"processing\"</code> Namespace for the deployment obs.endpoint string <code>\"http://minio.minio.svc.cluster.local:9000\"</code> URL of the object storage service endpoint obs.region string <code>\"sbg\"</code> Region of the object storage service obs.secret.ak string <code>\"TDr8foJqSygBQ9YFmWDy\"</code> Access Key to authenticate with the object storage service obs.secret.sk string <code>\"z2RaqjFttnVZRTsLLqmy4PE6PzJOKzPsE47alDBs\"</code> Secret Key to authenticate with the object storage service postgres.db string <code>\"rspydemo\"</code> PostgreSQL database name postgres.host string <code>\"postgresql-cluster-rw.database.svc.cluster.local\"</code> PostgreSQL service URL postgres.port string <code>\"5432\"</code> PostgreSQL port postgres.secret.pass string <code>\"test\"</code> Password to authenticate with the PostgreSQL service postgres.secret.user string <code>\"test\"</code> Username to authenticate with the PostgreSQL service probe.liveness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the liveness probe probe.liveness.path string <code>\"/health\"</code> Path for the liveness probe probe.liveness.periodSeconds int <code>30</code> periodSeconds for the liveness probe probe.liveness.port int <code>8000</code> Port for the liveness probe probe.liveness.timeoutSeconds int <code>5</code> timeoutSeconds for the liveness probe probe.readiness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the readiness probe probe.readiness.path string <code>\"/health\"</code> Path for the readiness probe probe.readiness.periodSeconds int <code>30</code> periodSeconds for the readiness probe probe.readiness.port int <code>8000</code> Port for the readiness probe probe.readiness.timeoutSeconds int <code>5</code> timeoutSeconds for the readiness probe replicaCount int <code>1</code> Number of replicas for the deployment resources.limit.cpu string <code>\"500m\"</code> Pod CPU limit resources.limit.ram string <code>\"1000Mi\"</code> Pod memory limit resources.request.cpu string <code>\"100m\"</code> Pod CPU request resources.request.ram string <code>\"256Mi\"</code> Pod memory request service.port int <code>8080</code> Port for the service tempo.endpoint string <code>\"http://grafana-tempo-distributor.logging.svc.cluster.local:4317\"</code> Grafana tempo endpoint. <p>Autogenerated from chart metadata using helm-docs v1.12.0</p>"},{"location":"rs-helm/charts/rs-server-cadip/","title":"rs-server-cadip","text":"<p>RS SERVER CADIP</p>"},{"location":"rs-helm/charts/rs-server-cadip/#maintainers","title":"Maintainers","text":"Name Email Url CS GROUP https://github.com/RS-PYTHON/rs-helm"},{"location":"rs-helm/charts/rs-server-cadip/#values","title":"Values","text":"Key Type Default Description app.confDir string <code>\"/app/conf\"</code> Config directory for the application app.eodagConfigFile string <code>\"cadip_ws_config.yaml\"</code> EODAG configuration file for the application app.port int <code>8000</code> Port for the application app.station.cadip object <code>{\"endpoint\":{\"secret\":{\"password\":\"test\",\"username\":\"test\"},\"url\":{\"file\":\"http://mockup-station-cadip-cadip-svc.processing.svc.cluster.local:8080/Files\",\"session\":\"http://mockup-station-cadip-cadip-svc.processing.svc.cluster.local:8080/Sessions\"}}}</code> CADIP station name app.station.cadip.endpoint.secret.password string <code>\"test\"</code> Password to authenticate with the CADIP station app.station.cadip.endpoint.secret.username string <code>\"test\"</code> Username to authenticate with the CADIP station app.station.cadip.endpoint.url object <code>{\"file\":\"http://mockup-station-cadip-cadip-svc.processing.svc.cluster.local:8080/Files\",\"session\":\"http://mockup-station-cadip-cadip-svc.processing.svc.cluster.local:8080/Sessions\"}</code> CADIP station URL app.stationConfigFile string <code>\"stations_cfg.json\"</code> Station configuration file for the application app.uacURL string <code>\"http://apikeymanager.processing.svc.cluster.local:8000/check/api_key\"</code> URL of the API Key Manager service app.workDir string <code>\"/app\"</code> Working directory for the application image.PullPolicy string <code>\"IfNotPresent\"</code> Image pull policy image.name string <code>\"rs-server-cadip\"</code> Image name image.registry string <code>\"ghcr.io\"</code> Image registry image.repository string <code>\"rs-python\"</code> Image repository image.version string <code>\"0.2a1\"</code> Image version, can be a tag or a digest ingress.enabled bool <code>true</code> Enabled/Disable ingress ingress.host string <code>\"subdomain.example.com\"</code> Ingress host name ingress.issuer.name string <code>\"letsencrypt-prod\"</code> Ingress Issuer name ingress.issuer.type string <code>\"cluster-issuer\"</code> Ingress Issuer type ingress.path string <code>\"/cadip\"</code> Ingress path namespace string <code>\"processing\"</code> Namespace for the deployment obs.endpoint string <code>\"http://minio.minio.svc.cluster.local:9000\"</code> URL of the object storage service endpoint obs.region string <code>\"sbg\"</code> Region of the object storage service obs.secret.ak string <code>\"TDr8foJqSygBQ9YFmWDy\"</code> Access Key to authenticate with the object storage service obs.secret.sk string <code>\"z2RaqjFttnVZRTsLLqmy4PE6PzJOKzPsE47alDBs\"</code> Secret Key to authenticate with the object storage service postgres.db string <code>\"rspydemo\"</code> PostgreSQL database name postgres.host string <code>\"postgresql-cluster-rw.database.svc.cluster.local\"</code> PostgreSQL service URL postgres.port string <code>\"5432\"</code> PostgreSQL port postgres.secret.pass string <code>\"test\"</code> Password to authenticate with the PostgreSQL service postgres.secret.user string <code>\"test\"</code> Username to authenticate with the PostgreSQL service probe.liveness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the liveness probe probe.liveness.path string <code>\"/health\"</code> Path for the liveness probe probe.liveness.periodSeconds int <code>30</code> periodSeconds for the liveness probe probe.liveness.port int <code>8000</code> Port for the liveness probe probe.liveness.timeoutSeconds int <code>5</code> timeoutSeconds for the liveness probe probe.readiness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the readiness probe probe.readiness.path string <code>\"/health\"</code> Path for the readiness probe probe.readiness.periodSeconds int <code>30</code> periodSeconds for the readiness probe probe.readiness.port int <code>8000</code> Port for the readiness probe probe.readiness.timeoutSeconds int <code>5</code> timeoutSeconds for the readiness probe replicaCount int <code>1</code> Number of replicas for the deployment resources.limit.cpu string <code>\"500m\"</code> Pod CPU limit resources.limit.ram string <code>\"1000Mi\"</code> Pod memory limit resources.request.cpu string <code>\"100m\"</code> Pod CPU request resources.request.ram string <code>\"256Mi\"</code> Pod memory request service.port int <code>8080</code> Port for the service tempo.endpoint string <code>\"http://grafana-tempo-distributor.logging.svc.cluster.local:4317\"</code> Grafana tempo endpoint. <p>Autogenerated from chart metadata using helm-docs v1.12.0</p>"},{"location":"rs-helm/charts/rs-server-catalog/","title":"rs-server-catalog","text":"<p>RS SERVER CATALOG</p>"},{"location":"rs-helm/charts/rs-server-catalog/#maintainers","title":"Maintainers","text":"Name Email Url CS GROUP https://github.com/RS-PYTHON/rs-helm"},{"location":"rs-helm/charts/rs-server-catalog/#values","title":"Values","text":"Key Type Default Description app.catalogBucket string <code>\"rs-cluster-catalog\"</code> Object Storage bucket for the catalog app.port int <code>8000</code> Port for the application app.presignedUrlExpirationTime int <code>1800</code> Presigned URL expiration time in seconds. 30 min by default app.uacURL string <code>\"http://apikeymanager.processing.svc.cluster.local:8000/check/api_key\"</code> URL of the API Key Manager service image.PullPolicy string <code>\"IfNotPresent\"</code> Image pull policy image.name string <code>\"rs-server-catalog\"</code> Image name image.registry string <code>\"ghcr.io\"</code> Image registry image.repository string <code>\"rs-python\"</code> Image repository image.version string <code>\"0.2a1\"</code> Image version, can be a tag or a digest ingress.enabled bool <code>true</code> Enabled/Disable ingress ingress.host string <code>\"subdomain.example.com\"</code> Ingress host name ingress.issuer.name string <code>\"letsencrypt-prod\"</code> Ingress Issuer name ingress.issuer.type string <code>\"cluster-issuer\"</code> Ingress Issuer type ingress.path string <code>\"/catalog\"</code> Ingress path namespace string <code>\"processing\"</code> Namespace for the deployment obs.endpoint string <code>\"http://minio.minio.svc.cluster.local:9000\"</code> URL of the object storage service endpoint obs.region string <code>\"sbg\"</code> Region of the object storage service obs.secret.ak string <code>\"TDr8foJqSygBQ9YFmWDy\"</code> Access Key to authenticate with the object storage service obs.secret.sk string <code>\"z2RaqjFttnVZRTsLLqmy4PE6PzJOKzPsE47alDBs\"</code> Secret Key to authenticate with the object storage service postgres.db string <code>\"catalog\"</code> PostgreSQL database name postgres.host.ro string <code>\"rs-server-catalog-db-svc.database.svc.cluster.local\"</code> PostgreSQL service URL for Read Only postgres.host.rw string <code>\"rs-server-catalog-db-svc.database.svc.cluster.local\"</code> PostgreSQL service URL for Read Write postgres.port string <code>\"5432\"</code> PostgreSQL port postgres.secret.pass string <code>\"password\"</code> Password to authenticate with the PostgreSQL service postgres.secret.user string <code>\"postgres\"</code> Username to authenticate with the PostgreSQL service probe.liveness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the liveness probe probe.liveness.path string <code>\"/health\"</code> Path for the liveness probe probe.liveness.periodSeconds int <code>30</code> periodSeconds for the liveness probe probe.liveness.port int <code>8000</code> Port for the liveness probe probe.liveness.timeoutSeconds int <code>5</code> timeoutSeconds for the liveness probe probe.readiness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the readiness probe probe.readiness.path string <code>\"/health\"</code> Path for the readiness probe probe.readiness.periodSeconds int <code>30</code> periodSeconds for the readiness probe probe.readiness.port int <code>8000</code> Port for the readiness probe probe.readiness.timeoutSeconds int <code>5</code> timeoutSeconds for the readiness probe replicaCount int <code>1</code> Number of replicas for the deployment resources.limit.cpu string <code>\"500m\"</code> Pod CPU limit resources.limit.ram string <code>\"1000Mi\"</code> Pod memory limit resources.request.cpu string <code>\"100m\"</code> Pod CPU request resources.request.ram string <code>\"256Mi\"</code> Pod memory request service.port int <code>8080</code> Port for the service tempo.endpoint string <code>\"http://grafana-tempo-distributor.logging.svc.cluster.local:4317\"</code> Grafana tempo endpoint. <p>Autogenerated from chart metadata using helm-docs v1.12.0</p>"},{"location":"rs-helm/charts/rs-server-catalog-db/","title":"rs-server-catalog-db","text":"<p>RS SERVER CATALOG DB</p>"},{"location":"rs-helm/charts/rs-server-catalog-db/#maintainers","title":"Maintainers","text":"Name Email Url CS GROUP https://github.com/RS-PYTHON/rs-helm"},{"location":"rs-helm/charts/rs-server-catalog-db/#values","title":"Values","text":"Key Type Default Description helmResourcePolicy string <code>\"keep\"</code> Keep the ressources for PVC and ConfigMap. Default is to keep. image.PullPolicy string <code>\"IfNotPresent\"</code> Image pull policy image.name string <code>\"pgstac\"</code> Image name image.registry string <code>\"ghcr.io\"</code> Image registry image.repository string <code>\"stac-utils\"</code> Image repository image.version string <code>\"v0.7.10\"</code> Image version, can be a tag or a digest namespace string <code>\"database\"</code> Namespace for the deployment postgres.db string <code>\"catalog\"</code> PostgreSQL database name postgres.port string <code>\"5432\"</code> PostgreSQL port postgres.secret.pass string <code>\"password\"</code> Password to authenticate with the PostgreSQL service postgres.secret.user string <code>\"postgres\"</code> Username to authenticate with the PostgreSQL service probe.liveness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the liveness probe probe.liveness.periodSeconds int <code>30</code> periodSeconds for the liveness probe probe.liveness.timeoutSeconds int <code>5</code> timeoutSeconds for the liveness probe probe.readiness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the readiness probe probe.readiness.periodSeconds int <code>30</code> periodSeconds for the readiness probe probe.readiness.timeoutSeconds int <code>5</code> timeoutSeconds for the readiness probe resources.limit.cpu string <code>\"500m\"</code> Pod CPU limit resources.limit.ram string <code>\"1000Mi\"</code> Pod memory limit resources.request.cpu string <code>\"100m\"</code> Pod CPU request resources.request.ram string <code>\"256Mi\"</code> Pod memory request service.port int <code>5432</code> Port for the service volume.accessModes string <code>\"ReadWriteOnce\"</code> AccessMode of the database volume volume.size string <code>\"20Gi\"</code> Size of the database volume volume.storageClassName string <code>\"csi-cinder-sc-retain\"</code> StorageClass of the database volume <p>Autogenerated from chart metadata using helm-docs v1.12.0</p>"},{"location":"rs-helm/charts/rs-server-frontend/","title":"rs-server-frontend","text":"<p>RS SERVER FRONTEND</p>"},{"location":"rs-helm/charts/rs-server-frontend/#maintainers","title":"Maintainers","text":"Name Email Url CS GROUP https://github.com/RS-PYTHON/rs-helm"},{"location":"rs-helm/charts/rs-server-frontend/#values","title":"Values","text":"Key Type Default Description app.docsUrl string <code>\"/docs\"</code> URL suffix for the application. Must be the same value as ingress.path app.port int <code>8000</code> Port for the application app.uacCheckUrl string <code>\"http://apikeymanager.processing.svc.cluster.local:8000/check/api_key\"</code> URL of the API Key Manager service (internal) app.uacHomeUrl string <code>\"https://apikeymanager.subdomain.example.com/docs\"</code> URL of the API Key Manager home page (public) image.PullPolicy string <code>\"IfNotPresent\"</code> Image pull policy image.name string <code>\"rs-server-frontend\"</code> Image name image.registry string <code>\"ghcr.io\"</code> Image registry image.repository string <code>\"rs-python\"</code> Image repository image.version string <code>\"0.2a1\"</code> Image version, can be a tag or a digest ingress.enabled bool <code>true</code> Enabled/Disable ingress ingress.host string <code>\"subdomain.example.com\"</code> Ingress host name ingress.issuer.name string <code>\"letsencrypt-prod\"</code> Ingress Issuer name ingress.issuer.type string <code>\"cluster-issuer\"</code> Ingress Issuer type ingress.path string <code>\"/docs\"</code> Ingress path for the application. Must be the same value as app.docsUrl. namespace string <code>\"processing\"</code> Namespace for the deployment probe.liveness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the liveness probe probe.liveness.path string <code>\"/health\"</code> Path for the liveness probe probe.liveness.periodSeconds int <code>30</code> periodSeconds for the liveness probe probe.liveness.port int <code>8000</code> Port for the liveness probe probe.liveness.timeoutSeconds int <code>5</code> timeoutSeconds for the liveness probe probe.readiness.initialDelaySeconds int <code>30</code> InitialDelaySeconds for the readiness probe probe.readiness.path string <code>\"/health\"</code> Path for the readiness probe probe.readiness.periodSeconds int <code>30</code> periodSeconds for the readiness probe probe.readiness.port int <code>8000</code> Port for the readiness probe probe.readiness.timeoutSeconds int <code>5</code> timeoutSeconds for the readiness probe replicaCount int <code>1</code> Number of replicas for the deployment resources.limit.cpu string <code>\"500m\"</code> Pod CPU limit resources.limit.ram string <code>\"1000Mi\"</code> Pod memory limit resources.request.cpu string <code>\"100m\"</code> Pod CPU request resources.request.ram string <code>\"256Mi\"</code> Pod memory request service.port int <code>8080</code> Port for the service <p>Autogenerated from chart metadata using helm-docs v1.12.0</p>"},{"location":"rs-infrastructure/","title":"rs-infrastructure","text":"<p>This repository provides kubernetes infrastructure and security layouts to support rs-server processing chains.</p>"},{"location":"rs-infrastructure/#getting-started","title":"Getting started","text":"<p>Documentation is available in the docs' folder.</p> <p> </p> <p>This project is funded by the EU and ESA.</p> <p> </p>"},{"location":"rs-infrastructure/NOTICE/","title":"NOTICE.md","text":"<p>This software is distributed under the Apache Software License (ASL) v2.0, see LICENSE file or http://www.apache.org/licenses/LICENSE-2.0 for details.</p> <ul> <li>Please be advised that:</li> <li>The Grafana and Loki software are used with the AGPL License.</li> <li>The Ansible and Wazuh software are used with the GPL License.</li> </ul> <p>Below are all the FOSS (Free and open-source software) used and their respective licenses:</p> <ul> <li>Cert manager :</li> <li>Helm chart<ul> <li>Version: v1.13.3</li> <li>License: Apache License 2.0</li> <li>Source: https://github.com/cert-manager/cert-manager/tree/v1.13.3/deploy/charts/cert-manager</li> <li>Copyright: Copyright The cert-manager Authors. Authors and Contributors</li> </ul> </li> <li> <p>Container image(s)</p> <ul> <li>quay.io/jetstack/cert-manager-cainjector:v1.13.3</li> <li>License: Apache License 2.0</li> </ul> </li> <li> <p>NGINX Ingress Controller :</p> </li> <li>Helm chart<ul> <li>Version: v4.9.1</li> <li>License: Apache License 2.0</li> <li>Source: https://github.com/kubernetes/ingress-nginx/tree/helm-chart-4.9.1/charts/ingress-nginx</li> <li>Copyright: Copyright The ingress-nginx Authors. Authors and Contributors here and here</li> </ul> </li> <li> <p>Container image(s)</p> <ul> <li>registry.k8s.io/ingress-nginx/controller:v1.9.6</li> <li>License: Apache License 2.0</li> <li>docker.io/library/nginx:1.25.2-alpine</li> <li>License: BSD 2-Clause \"Simplified\" License</li> </ul> </li> <li> <p>Grafana Operator</p> </li> <li>Helm chart: None<ul> <li>Version: 1.1.0</li> <li>License: Apache License 2.0</li> <li>Copyright: Copyright the Grafana Authors. Authors and Contributors</li> </ul> </li> <li> <p>Container image(s)</p> <ul> <li>ghcr.io/grafana/grafana-operator:v5.9.0</li> <li>License: Apache License 2.0</li> <li>grafana/grafana:10.0.3</li> <li>License: GNU Affero General Public License v3.0</li> </ul> </li> <li> <p>PostgreSQL</p> </li> <li>Helm chart:<ul> <li>Version: 0.20.1</li> <li>Licence: Apache License 2.0</li> <li>Source: https://github.com/cloudnative-pg/charts/tree/cloudnative-pg-v0.20.1</li> <li>Copyright: Copyright The CloudNativePG authors. Authors and Contributors</li> </ul> </li> <li> <p>Container image(s)</p> <ul> <li>ghcr.io/cloudnative-pg/cloudnative-pg:1.22.1</li> <li>License: Apache License 2.0</li> <li>ghcr.io/cloudnative-pg/postgresql:16.1</li> <li>License: Apache License 2.0</li> </ul> </li> <li> <p>Loki</p> </li> <li>Helm chart:<ul> <li>Version: 0.79.0</li> <li>Licence: Apache License 2.0</li> <li>Source: https://github.com/grafana/helm-charts/tree/loki-distributed-0.79.0/charts/loki-distributed</li> <li>Copyright: Copyright The Loki Authors. Authors and Contributors</li> </ul> </li> <li> <p>Container image(s)</p> <ul> <li>docker.io/grafana/loki:2.9.6</li> <li>License: GNU Affero General Public License v3.0</li> </ul> </li> <li> <p>Keycloak</p> </li> <li>Helm chart: None</li> <li> <p>Container image(s)</p> <ul> <li>quay.io/keycloak/keycloak:23.0.6</li> <li>License: Apache License 2.0</li> <li>quay.io/keycloak/keycloak-operator:23.0.6</li> <li>License: Apache License 2.0</li> </ul> </li> <li> <p>CoreDNS</p> </li> <li>Helm chart: None</li> <li> <p>Container image(s)</p> <ul> <li>registry.k8s.io/coredns/coredns:v1.10.1</li> <li>License: Apache License 2.0</li> </ul> </li> <li> <p>Kubernetes</p> </li> <li>Helm chart: None</li> <li>Container image(s): None</li> <li>Version: v1.27.7</li> <li> <p>License: Apache License 2.0</p> </li> <li> <p>Wazuh</p> </li> <li>Helm chart: None</li> <li>Container image(s): None</li> <li>Version: 4.7.2</li> <li> <p>License: GNU General Public License v2.0</p> </li> <li> <p>Containerd</p> </li> <li>Helm chart: None</li> <li>Container image(s): None</li> <li>Version: 1.4.9-1</li> <li> <p>License: Apache License 2.0</p> </li> <li> <p>Jupyter</p> </li> <li>Helm chart:<ul> <li>Version: 3.3.6</li> <li>Licence: Apache License 2.0</li> <li>Source: https://github.com/jupyterhub/zero-to-jupyterhub-k8s/tree/3.3.6</li> <li>Copyright: Copyright (c) Jupyter Development Team. Authors and Contributors</li> </ul> </li> <li> <p>Container image(s)</p> <ul> <li>ghcr.io/rs-python/rs-infrastructure-jupyter:latest</li> <li>License: Apache License 2.0</li> <li>quay.io/jupyterhub/k8s-image-awaiter:3.3.6</li> <li>Licence: Apache License 2.0</li> <li>quay.io/jupyterhub/k8s-hub:3.3.6</li> <li>Licence: Apache License 2.0</li> </ul> </li> <li> <p>Neuvector</p> </li> <li>Helm chart:<ul> <li>Version: 2.7.3</li> <li>Licence: Apache License 2.0</li> <li>Source: https://github.com/neuvector/neuvector-helm/tree/2.7.3</li> <li>Copyright: Copyright The Neuvector Development Team. Authors and Contributors</li> </ul> </li> <li> <p>Container image(s)</p> <ul> <li>docker.io/neuvector/scanner:latest</li> <li>License: Apache License 2.0</li> <li>docker.io/neuvector/enforcer:5.3.2</li> <li>Licence: Apache License 2.0</li> <li>docker.io/neuvector/manager:5.3.2</li> <li>Licence: Apache License 2.0</li> <li>docker.io/neuvector/prometheus-exporter:latest</li> <li>Licence: Apache License 2.0</li> </ul> </li> <li> <p>oauth2-proxy</p> </li> <li>Helm chart:<ul> <li>Version: 7.1.0</li> <li>Licence: Apache License 2.0</li> <li>Source: https://github.com/oauth2-proxy/manifests/tree/oauth2-proxy-7.1.0</li> <li>Copyright: Copyright The oauth2-proxy Development Team. Authors and Contributors</li> </ul> </li> <li> <p>Container image(s)</p> <ul> <li>quay.io/oauth2-proxy/oauth2-proxy:v7.6.0</li> <li>License: MIT License</li> </ul> </li> <li> <p>Prefect</p> </li> <li>Helm chart:<ul> <li>Version: 7.1.0</li> <li>Licence: Apache License 2.0</li> <li>Source: https://github.com/PrefectHQ/prefect-helm/tree/2024.5.2224951</li> <li>Copyright: Copyright The Prefect Development Team. Authors and Contributors</li> </ul> </li> <li> <p>Container image(s)</p> <ul> <li>prefecthq/prefect:2.18.1-python3.11-kubernetes</li> <li>License: Apache License 2.0</li> <li>prefecthq/prefect:2.18.3-python3.11-kubernetes</li> <li>License: Apache License 2.0</li> </ul> </li> <li> <p>kube-prometheus-stack</p> </li> <li>Helm chart:<ul> <li>Version: 58.0.0</li> <li>Licence: Apache License 2.0</li> <li>Source: https://github.com/prometheus-community/helm-charts/tree/kube-prometheus-stack-58.0.0</li> <li>Copyright: Copyright The Prometheus community Development Team. Authors and Contributors</li> </ul> </li> <li> <p>Container image(s)</p> <ul> <li>quay.io/prometheus-operator/prometheus-config-reloader:v0.73.0</li> <li>License: Apache License 2.0</li> <li>quay.io/prometheus/alertmanager:v0.27.0</li> <li>License: Apache License 2.0</li> <li>quay.io/prometheus-operator/prometheus-operator:v0.73.0</li> <li>License: Apache License 2.0</li> <li>registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0</li> <li>License: Apache License 2.0</li> <li>quay.io/prometheus/prometheus:v2.51.1</li> <li>License: Apache License 2.0</li> <li>quay.io/prometheus/node-exporter:v1.7.0</li> <li>License: Apache License 2.0</li> </ul> </li> <li> <p>Promtail</p> </li> <li>Helm chart:<ul> <li>Version: 6.15.5</li> <li>Licence: Apache License 2.0</li> <li>Source: https://github.com/grafana/helm-charts/tree/promtail-6.15.5/charts/promtail</li> <li>Copyright: Copyright The Grafana Development Team. Authors and Contributors</li> </ul> </li> <li>Container image(s)<ul> <li>docker.io/grafana/promtail:2.9.3</li> <li>License: GNU Affero General Public License v3.0</li> </ul> </li> </ul>"},{"location":"rs-infrastructure/NOTICE/#licenses","title":"Licenses","text":""},{"location":"rs-infrastructure/NOTICE/#apache-license-20","title":"Apache License 2.0","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"rs-infrastructure/NOTICE/#gnu-affero-general-public-license-v30","title":"GNU Affero General Public License v3.0","text":"<pre><code>                GNU AFFERO GENERAL PUBLIC LICENSE\n                   Version 3, 19 November 2007\n</code></pre> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate.  Many developers of free software are heartened and encouraged by the resulting cooperation.  However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community.  It requires the operator of a network server to provide the source code of the modified version running there to the users of that server.  Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals.  This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                   TERMS AND CONDITIONS\n</code></pre> <ol> <li>Definitions.</li> </ol> <p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License.  Each licensee is addressed as \"you\".  \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p> <ol> <li>Source Code.</li> </ol> <p>The \"source code\" for a work means the preferred form of the work for making modifications to it.  \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p> <ol> <li>Basic Permissions.</li> </ol> <p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.</p> <ol> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.</li> </ol> <p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p> <ol> <li>Conveying Verbatim Copies.</li> </ol> <p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p> <ol> <li>Conveying Modified Source Versions.</li> </ol> <p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <pre><code>a) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\n</code></pre> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p> <ol> <li>Conveying Non-Source Forms.</li> </ol> <p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <pre><code>a) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\n</code></pre> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p> <ol> <li>Additional Terms.</li> </ol> <p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <pre><code>a) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\n</code></pre> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p> <ol> <li>Termination.</li> </ol> <p>You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p> <ol> <li>Acceptance Not Required for Having Copies.</li> </ol> <p>You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p> <ol> <li>Automatic Licensing of Downstream Recipients.</li> </ol> <p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p> <ol> <li>Patents.</li> </ol> <p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients.  \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p> <ol> <li>No Surrender of Others' Freedom.</li> </ol> <p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p> <ol> <li>Remote Network Interaction; Use with the GNU General Public License.</li> </ol> <p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software.  This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p> <ol> <li>Revised Versions of this License.</li> </ol> <p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p> <ol> <li>Disclaimer of Warranty.</li> </ol> <p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <ol> <li>Limitation of Liability.</li> </ol> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <ol> <li>Interpretation of Sections 15 and 16.</li> </ol> <p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <pre><code>                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\n</code></pre> <p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source.  For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code.  There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"rs-infrastructure/NOTICE/#gnu-general-public-license-v20","title":"GNU General Public License v2.0","text":"<pre><code>                GNU GENERAL PUBLIC LICENSE\n                   Version 2, June 1991\n</code></pre> <p>Copyright (C) 1989, 1991 Free Software Foundation, Inc.,  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The licenses for most software are designed to take away your freedom to share and change it.  By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users.  This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it.  (Some other Free Software Foundation software is covered by the GNU Lesser General Public License instead.)  You can apply it to your programs, too.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things.</p> <p>To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it.</p> <p>For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have.  You must make sure that they, too, receive or can get the source code.  And you must show them these terms so they know their rights.</p> <p>We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software.</p> <p>Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software.  If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations.</p> <p>Finally, any free program is threatened constantly by software patents.  We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary.  To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                GNU GENERAL PUBLIC LICENSE\n</code></pre> <p>TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION</p> <ol> <li>This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License.  The \"Program\", below, refers to any such program or work, and a \"work based on the Program\" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language.  (Hereinafter, translation is included without limitation in the term \"modification\".)  Each licensee is addressed as \"you\".</li> </ol> <p>Activities other than copying, distribution and modification are not covered by this License; they are outside its scope.  The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does.</p> <ol> <li>You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program.</li> </ol> <p>You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee.</p> <ol> <li> <p>You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions:</p> <p>a) You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change.</p> <p>b) You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License.</p> <p>c) If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License.  (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.)</p> </li> </ol> <p>These requirements apply to the modified work as a whole.  If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works.  But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it.</p> <p>Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program.</p> <p>In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License.</p> <ol> <li> <p>You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following:</p> <p>a) Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or,</p> <p>b) Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or,</p> <p>c) Accompany it with the information you received as to the offer to distribute corresponding source code.  (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.)</p> </li> </ol> <p>The source code for a work means the preferred form of the work for making modifications to it.  For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable.  However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable.</p> <p>If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code.</p> <ol> <li> <p>You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License.  Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance.</p> </li> <li> <p>You are not required to accept this License, since you have not signed it.  However, nothing else grants you permission to modify or distribute the Program or its derivative works.  These actions are prohibited by law if you do not accept this License.  Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it.</p> </li> <li> <p>Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions.  You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License.</p> </li> <li> <p>If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all.  For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program.</p> </li> </ol> <p>If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances.</p> <p>It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices.  Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice.</p> <p>This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License.</p> <ol> <li> <p>If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded.  In such case, this License incorporates the limitation as if written in the body of this License.</p> </li> <li> <p>The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> </li> </ol> <p>Each version is given a distinguishing version number.  If the Program specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation.</p> <ol> <li> <p>If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission.  For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this.  Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally.</p> <pre><code>                    NO WARRANTY\n</code></pre> </li> <li> <p>BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> </li> <li> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <pre><code>             END OF TERMS AND CONDITIONS\n\n    How to Apply These Terms to Your New Programs\n</code></pre> </li> </ol> <p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively convey the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 2 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License along\nwith this program; if not, write to the Free Software Foundation, Inc.,\n51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If the program is interactive, make it output a short notice like this when it starts in an interactive mode:</p> <pre><code>Gnomovision version 69, Copyright (C) year name of author\nGnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; type `show c' for details.\n</code></pre> <p>The hypothetical commands <code>show w' and</code>show c' should show the appropriate parts of the General Public License.  Of course, the commands you use may be called something other than <code>show w' and</code>show c'; they could even be mouse-clicks or menu items--whatever suits your program.</p> <p>You should also get your employer (if you work as a programmer) or your school, if any, to sign a \"copyright disclaimer\" for the program, if necessary.  Here is a sample; alter the names:</p> <p>Yoyodyne, Inc., hereby disclaims all copyright interest in the program   `Gnomovision' (which makes passes at compilers) written by James Hacker.</p> <p>, 1 April 1989   Ty Coon, President of Vice <p>This General Public License does not permit incorporating your program into proprietary programs.  If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library.  If this is what you want to do, use the GNU Lesser General Public License instead of this License.</p>"},{"location":"rs-infrastructure/NOTICE/#mit-license","title":"MIT License","text":"<p>MIT License</p> <p>Copyright (c) year</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"rs-infrastructure/NOTICE/#bsd-2-clause-simplified-license","title":"BSD 2-Clause \"Simplified\" License","text":"<p>BSD 2-Clause License</p> <p>Copyright (c) [year], [fullname]</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright notice, this    list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,    this list of conditions and the following disclaimer in the documentation    and/or other materials provided with the distribution.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"rs-infrastructure/NOTICE/#bsd-3-clause-new-or-revised-license","title":"BSD 3-Clause \"New\" or \"Revised\" License","text":"<p>BSD 3-Clause License</p> <p>Copyright (c) [year], [fullname]</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright notice, this    list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,    this list of conditions and the following disclaimer in the documentation    and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its    contributors may be used to endorse or promote products derived from    this software without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"rs-infrastructure/docs/installation/","title":"Infrastructure - Installation","text":""},{"location":"rs-infrastructure/docs/installation/#overview","title":"Overview","text":"<p>Admin's machine is called BASTION in the rest of the installation manual</p>"},{"location":"rs-infrastructure/docs/installation/#bastion-requirements","title":"Bastion requirements","text":"<ul> <li>miniforge</li> <li>git</li> <li>jq</li> </ul>"},{"location":"rs-infrastructure/docs/installation/#dependencies","title":"Dependencies","text":""},{"location":"rs-infrastructure/docs/installation/#terraform","title":"Terraform","text":"<p>This project exploits Terraform to deploy the infrastructure on the Cloud Provider. The fully detailed documentation and configuration options are available on its page: https://www.terraform.io/</p>"},{"location":"rs-infrastructure/docs/installation/#kubespray","title":"Kubespray","text":"<p>This project exploits Kubespray to deploy Kubernetes. The fully detailed documentation and configuration options are available on its page: https://kubespray.io/</p>"},{"location":"rs-infrastructure/docs/installation/#hashicorp-vault-optional","title":"HashiCorp Vault (optional)","text":"<p>This project can integrate credentials from a custom <code>HashiCorp Vault</code> instance, see the specific documentation: how to/Credentials</p>"},{"location":"rs-infrastructure/docs/installation/#openstack-cli","title":"Openstack CLI","text":"<p>This project exploits Openstack CLI to manage the state of the infrastructure on the Cloud Provider. The fully detailled documentation and configuration options are available on its page: https://docs.openstack.org/newton/user-guide/cli.html</p>"},{"location":"rs-infrastructure/docs/installation/#quickstart","title":"Quickstart","text":""},{"location":"rs-infrastructure/docs/installation/#1-get-the-rs-infrastructure-repository","title":"1. Get the rs-infrastructure repository","text":"<pre><code>git clone https://github.com/RS-PYTHON/rs-infrastructure.git\ncd rs-infrastructure\n</code></pre>"},{"location":"rs-infrastructure/docs/installation/#2-install-requirements","title":"2. Install requirements","text":"<pre><code># Install miniforge\nmkdir -p ~/miniforge3\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\" -O ~/miniforge3/miniforge.sh\nbash ~/miniforge3/miniforge.sh -b -u -p ~/miniforge3\nrm -f ~/miniforge3/miniforge.sh\n\n# Init conda depending on your shell\n~/miniforge3/bin/conda init bash\n~/miniforge3/bin/conda init zsh\n\n# Create conda env with python=3.11 and activate it\nconda create -y -n rspy python=3.11\nconda activate rspy\n\n# Install Ansible, Terraform, Openstackclient\nconda install conda-forge::ansible\nconda install conda-forge::terraform\nconda install conda-forge::python-openstackclient\nconda install conda-forge::passlib\n\n# Init Kubespray collection with remote\ngit submodule update --init --remote\n\npip install -U pyOpenSSL ecdsa -r collections/kubespray/requirements.txt\n\nansible-galaxy collection install \\\n    kubernetes.core \\\n    openstack.cloud\n</code></pre>"},{"location":"rs-infrastructure/docs/installation/#3-copy-the-sample-inventory","title":"3. Copy the sample inventory","text":"<pre><code>cp -rfp inventory/sample inventory/mycluster\n</code></pre>"},{"location":"rs-infrastructure/docs/installation/#4-review-and-change-the-default-configuration-to-match-your-needs","title":"4. Review and change the default configuration to match your needs","text":"<pre><code>cp -rfp roles/terraform/create-cluster/tasks/.env.template roles/terraform/create-cluster/tasks/.env\n</code></pre> <p>Copy the openrc.sh.template into openrc.sh and change the values inside to match your configuration :</p> <pre><code>cp -rfp inventory/mycluster/openrc.sh.template inventory/mycluster/openrc.sh\n</code></pre> <ul> <li>Credentials, domain name, the stash license, S3 endpoints in <code>inventory/mycluster/group_vars/main.yaml</code></li> <li>Credentials in <code>roles/terraform/create-cluster/tasks/.env</code></li> <li>Credentials, domain name in <code>inventory/mycluster/openrc.sh</code></li> <li>Node groups, Network sizing, S3 buckets in <code>inventory/mycluster/cluster.tfvars</code></li> <li>S3 backend for terraform in <code>inventory/mycluster/backend.tfvars</code></li> <li>Optimization for well-known zones and/or internal-only domains, i.e. VPN/Object Storage for internal networks in <code>inventory/mycluster/host_vars/setup/kubespray.yaml</code></li> <li>Values for custom parameters in <code>inventory/mycluster/group_vars/apps.yml</code></li> </ul> <pre><code>ansible-playbook generate_inventory.yaml \\\n    -i inventory/mycluster/hosts.yaml\n</code></pre>"},{"location":"rs-infrastructure/docs/installation/#5-create-and-configure-machines","title":"5. Create and configure machines","text":"<pre><code>ansible-playbook cluster.yaml \\\n    -i inventory/mycluster/hosts.yaml\n</code></pre> <p>Note: DNS configuration</p> <p>At this point, you should configure your domain name to point to the master's IP from the <code>inventory/mycluster/hosts.yaml</code> file.</p>"},{"location":"rs-infrastructure/docs/installation/#6-deploy-kubernetes-with-kubespray","title":"6. Deploy Kubernetes with <code>kubespray</code>","text":"<pre><code>ansible-playbook kubernetes.yaml \\\n    -i inventory/mycluster/hosts.yaml\n</code></pre>"},{"location":"rs-infrastructure/docs/installation/#7-deploy-the-apps","title":"7. Deploy the apps","text":"<p>Disclaimer: For Wazuh Server installation</p> <p>See \"1. Enable Bcrypt encryption\" in the Wazuh-Server_Install and update the <code>encrypt.py</code> library before deploy the apps.</p> <pre><code>ansible-playbook apps.yaml \\\n    -i inventory/mycluster/hosts.yaml\n</code></pre> <p>Disclaimer: For Prefect-Worker post-configuration</p> <p>See \"2. set <code>Concurrency Limit</code> on workpool _on-demand-k8s-pool\"_ in the Prefect-Worker after deploy the app.</p>"},{"location":"rs-infrastructure/docs/installation/#8-deploy-the-rs-server","title":"8. Deploy the rs-server","text":"<pre><code>ansible-playbook rs-server.yaml \\\n    -i inventory/mycluster/hosts.yaml\n</code></pre>"},{"location":"rs-infrastructure/docs/installation/#copyright-and-license","title":"Copyright and license","text":"<p>The Reference System Software as a whole is distributed under the Apache License, version 2.0. A copy of this license is available in the LICENSE file. Reference System Software depends on third-party components and code snippets released under their own license (obviously, all compatible with the one of the Reference System Software). These dependencies are listed in the NOTICE file.</p> <p> </p> <p>This project is funded by the EU and ESA.</p> <p> </p>"},{"location":"rs-infrastructure/docs/how-to/Cluster%20Management/","title":"Infrastructure - Cluster Management","text":""},{"location":"rs-infrastructure/docs/how-to/Cluster%20Management/#overview","title":"Overview","text":"<p>Every command listed in this file needs to be executed in the terraform folder : <code>roles/terraform/create-cluster/tasks</code></p>"},{"location":"rs-infrastructure/docs/how-to/Cluster%20Management/#show-what-terraform-objects-are-created","title":"Show what terraform objects are created","text":"<pre><code>terraform state list\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Cluster%20Management/#remove-a-terraform-object-from-the-state","title":"Remove a terraform object from the state","text":"<p>NOTE: This wont delete the object from the cloud provider, it just won't be managed by terraform anymore. <pre><code>terraform state rm &lt;terraform_object&gt;\n</code></pre></p>"},{"location":"rs-infrastructure/docs/how-to/Cluster%20Management/#show-the-output-variables","title":"Show the output variables","text":"<pre><code>terraform output\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Cluster%20start%20and%20stop/","title":"Start and stop the cluster","text":""},{"location":"rs-infrastructure/docs/how-to/Cluster%20start%20and%20stop/#prerequisites","title":"Prerequisites","text":"<p>Copy the <code>openrc.sh.template</code> into <code>openrc.sh</code> and change the values inside to match your configuration</p> <pre><code>cp -rfp inventory/sample/openrc.sh.template inventory/mycluster/openrc.sh\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Cluster%20start%20and%20stop/#stop-the-cluster","title":"Stop the cluster","text":"<p>After configuring the <code>openrc.sh</code> file, use this command to stop the cluster :</p> <pre><code>ansible-playbook start-stop.yaml     -i inventory/mycluster/hosts.yaml -t stop\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Cluster%20start%20and%20stop/#start-the-cluster","title":"Start the cluster","text":"<p>After configuring the <code>openrc.sh</code> file, use this command to stop the cluster :</p> <pre><code>ansible-playbook start-stop.yaml     -i inventory/mycluster/hosts.yaml -t start\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Credentials/","title":"Credentials management in RS","text":""},{"location":"rs-infrastructure/docs/how-to/Credentials/#the-generate_inventoryyaml-playbook-and-inventory-files","title":"The generate_inventory.yaml playbook and inventory files","text":"<p>All the credentials necessary to the deployment of the different applications can be set in the different inventory files located under <code>{{ inventory_dir }}/host_vars/setup</code>.</p> <p>Setup your variables by:</p> <ul> <li>using separate files for variables corresponding to different applications, like in the sample inventory and its <code>{{ inventory_dir }}/host_vars/setup/apps</code> subfolder</li> <li>creating new files (for example <code>{{ inventory_dir }}/host_vars/setup/production_env1.yaml</code>) with your variables</li> <li>editing the variables given as sample in the sample inventory</li> </ul> <p>On the run of the <code>generate_inventory.yaml</code> playbook, the files under <code>{{ inventory_dir }}/host_vars/setup</code> will be templated and a new <code>generated_inventory_vars.yaml</code> file will be written to the <code>{{ inventory_dir }}/group_vars/all</code> folder.</p> <p>The values actually used by the app-installer come from the <code>generated_inventory_vars.yaml</code>. You will find all the credentials there.</p> <p>This workflow prevents the app-installer from changing the credentials of the apps between the deployments on the same platform, which would cause many issues.</p>"},{"location":"rs-infrastructure/docs/how-to/Credentials/#generate-random-credentials","title":"Generate random credentials","text":"<p>Like in the example values, you can choose to generate some credentials using this ansible function:</p> <pre><code># {{ inventory_dir }}/host_vars/setup/apps/openldap.yaml\nopenldap:\n  admin_user_password: \"{{ lookup('password', '/dev/null length=60 chars=ascii_letters') }}\"\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Credentials/#manually-set-credentials","title":"Manually set credentials","text":"<p>Otherwise, you can freely set passwords by hand:</p> <pre><code># {{ inventory_dir }}/host_vars/setup/apps/graylog.yaml\ngraylog:\n  oidc_client_secret: \"m4nuAl_s3cret_example\"\n</code></pre> <p>Note: Of course, credentials that give access to external services such as S3 storage or private registries cannot be generated and have to be set by hand, they are written in UPPER_CASE in the sample inventory.</p>"},{"location":"rs-infrastructure/docs/how-to/Credentials/#reuse-credentials","title":"Reuse credentials","text":"<p>Like in the example values, you can reuse crendentials already set up in the inventory files. This functionnality is used in the sample inventory for the S3 keys and endpoints that are often the same accross applications:</p> <pre><code># {{ inventory_dir }}/host_vars/setup/main.yaml\ns3:\n  endpoint: S3_ENDPOINT\n  region: S3_REGION\n  secret_key: S3_SECRET_KEY\n  access_key: S3_ACCESS_KEY\n</code></pre> <pre><code># {{ inventory_dir }}/host_vars/setup/apps/thanos.yaml\nthanos:\n  s3:\n    bucket: THANOS_BUCKET\n    endpoint: \"{{ common.s3.endpoint }}\"\n    region: \"{{ common.s3.region }}\"\n    access_key: \"{{ common.s3.access_key }}\"\n    secret_key: \"{{ common.s3.secret_key }}\"\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Credentials/#retrieve-indivindual-credentials-from-a-hashicorp-vault","title":"Retrieve indivindual credentials from a HashiCorp Vault","text":"<p>You can retrieve credentials from a HashiCorp Vault instance using the hvac ansible plugin:</p> <pre><code># {{ inventory_dir }}/host_vars/setup/main.yaml\nvault:\n  url: VAULT_ENDPOINT\n  token: VAULT_TOKEN\n  path: VAULT_PATH # add '/data/' after the secret engine name to use kv version 2\n  download_inventory_vars: false\n  upload_backup: true\n  upload_existing: false\n\n[...]\n\n\ns3:\n  endpoint: S3_ENDPOINT\n  region: S3_REGION\n  secret_key: \"{{ lookup('community.hashi_vault.hashi_vault', vault.path + 'SECRET_NAME', token=vault.token, url=vault.url)['KEY_IN_SECRET'] }}\"\n  access_key: \"{{ lookup('community.hashi_vault.hashi_vault', vault.path + 'SECRET_NAME', token=vault.token, url=vault.url)['KEY_IN_SECRET'] }}\"\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Credentials/#backup-and-restore-all-inventory-variables-with-a-hashicorp-vault","title":"Backup and restore all inventory variables with a HashiCorp Vault","text":""},{"location":"rs-infrastructure/docs/how-to/Credentials/#options","title":"Options","text":"<ul> <li> <p>vault.upload_backup: send a backup of the <code>generated_inventory_vars.yaml</code> file in JSON format to the remote secret engine.</p> </li> <li> <p>vault.upload_existing: upload the existing <code>generated_inventory_vars.yaml</code> file without generating it before (useful if it has been edited by hand)</p> </li> <li> <p>vault.download_inventory_vars: download the previously backed-up <code>generated_inventory_vars.yaml</code> file and write it to the <code>{{ inventory_dir }}/group_vars/all</code> folder.</p> </li> </ul> <p>Note: All the other variables will not be taken in account, this allows an operator to deploy apps to any cluster with only the few vault variables set up, and it allows any operator with vault access to read the application specific credentials directly on the vault web interface.</p>"},{"location":"rs-infrastructure/docs/how-to/GitHub%20Container%20Registry/","title":"Create a docker registry secret in Kubernetes","text":"<p>You might need to create a docker registry secret to retrieve private container image from the GitHub Container Registry : ghcr.io</p>"},{"location":"rs-infrastructure/docs/how-to/GitHub%20Container%20Registry/#create-a-personal-access-tokens-classic","title":"Create a Personal Access Tokens (classic)","text":"<p>Check the official GitHub documentation : https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-with-a-personal-access-token-classic</p> <p>And create a PAT with at least <code>read:packages</code> scope.</p>"},{"location":"rs-infrastructure/docs/how-to/GitHub%20Container%20Registry/#create-a-docker-registry-secret-in-kubernetes_1","title":"Create a docker registry secret in Kubernetes","text":"<p>Check the official Kubernetes documentation : https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_secret_docker-registry/</p> <p>And create a Kubernetes secret with the GitHub token obtained in the previous step :</p> <pre><code>kubectl -n processing create secret docker-registry ghcr-k8s --docker-server=ghcr.io --docker-username=&lt;USERNAME&gt; --docker-password=&lt;TOKEN_FROM_PREVIOUS_STEP&gt;\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Ingress/","title":"Ingress","text":"<p>an Ingress is an API object that manages external access to the services in a cluster, typically HTTP.</p> <p>Ingress may provide load balancing, SSL termination and name-based virtual hosting.</p> <p>Learn more about Ingress on the main Kubernetes documentation site.</p>"},{"location":"rs-infrastructure/docs/how-to/Ingress/#nginx-ingress-controller","title":"NGINX Ingress Controller","text":"<p>ingress-nginx is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer.</p>"},{"location":"rs-infrastructure/docs/how-to/Ingress/#cert-manager","title":"cert-manager","text":"<p>cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates.</p> <p>cert-manager also ensures certificates remain valid and up to date, attempting to renew certificates at an appropriate time before expiry to reduce the risk of outages and remove toil.</p> <p>Documentation for cert-manager can be found at cert-manager.io.</p> <p>For the common use-case of automatically issuing TLS certificates for Ingress resources, see the cert-manager nginx-ingress quick start guide.</p> <p>For a more comprehensive guide to issuing your first certificate, see our getting started guide.</p>"},{"location":"rs-infrastructure/docs/how-to/Ingress/#ingress-ssl","title":"Ingress + SSL","text":"<p>Thanks to the NGINX ingress controller and the cert-manager components, when installed and properly configured, it's easy to deploy in ingress with SSL with just one Kubernetes resource file.</p> <p>The example below shows how to deploy an ingress that will automatically provides the SSL certificate and updates it before it expires:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n  name: demo\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: demo.example.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: demo\n            port:\n              number: 80\n        path: /\n        pathType: Prefix\n  tls:\n  - hosts:\n    - demo.example.com\n    secretName: demo-example-com-secret\n</code></pre> <p>(Replace the value of \"demo.example.com\" and \"demo-example-com-secret\")</p>"},{"location":"rs-infrastructure/docs/how-to/Prefect-Worker/","title":"Prefect Worker Workpoool concurrency-limits","text":""},{"location":"rs-infrastructure/docs/how-to/Prefect-Worker/#1-open-shell-to-the-running-container-prefect-server","title":"1. Open shell to the running container <code>prefect-server</code>","text":"<pre><code>kubectl -n processing exec -it prefect-server-abc456  -- /bin/bash\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Prefect-Worker/#2-check-concurrency-limit-on-workpool-on-demand-k8s-pool","title":"2. Check <code>Concurrency Limit</code> on workpool on-demand-k8s-pool","text":"<pre><code> prefect work-pool ls\n</code></pre> Name Type ID Concurrency Limit on-demand-k8s-pool kubernetes xyz123-xyz123-... <code>None</code>"},{"location":"rs-infrastructure/docs/how-to/Prefect-Worker/#3-set-concurrency-limit-on-workpool-on-demand-k8s-pool","title":"3. Set <code>Concurrency Limit</code> on workpool on-demand-k8s-pool","text":"<pre><code>prefect work-pool set-concurrency-limit on-demand-k8s-pool 10\n</code></pre> <p>Return command on prompt should be : Set concurrency limit for work pool 'on-demand-k8s-pool' to 10</p> <pre><code> prefect work-pool ls\n</code></pre> Name Type ID Concurrency Limit on-demand-k8s-pool kubernetes xyz123-xyz123-... <code>10</code> <p>Setting should be check into WebUI too, on parameters page of workpool on-demand-k8s-pool</p>"},{"location":"rs-infrastructure/docs/how-to/Remote%20kubectl/","title":"Remote cluster administration using kubectl","text":""},{"location":"rs-infrastructure/docs/how-to/Remote%20kubectl/#install-kubectl-and-the-oidc-plugin","title":"Install kubectl and the oidc plugin","text":"<ul> <li>Install kubectl using the official documentation: https://kubernetes.io/fr/docs/tasks/tools/install-kubectl/</li> <li>Install the kubelogin using official documentation: https://github.com/int128/kubelogin or by running:</li> </ul> <pre><code>curl -LO https://github.com/int128/kubelogin/releases/download/v1.28.0/kubelogin_linux_amd64.zip\nunzip kubelogin_linux_amd64.zip\nsudo mv kubelogin /usr/local/bin/kubectl-oidc_login\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Remote%20kubectl/#configure-your-kubeconfig-file-to-use-the-platforms-authentification","title":"Configure your kubeconfig file to use the platform's authentification","text":"<p>Setup your <code>kubeconfig</code> file like the following example, and set manually the the variables that come from the inventory:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: https://kube.{{ platform_domain_name }}\n  name: {{ cluster.name }}\ncontexts:\n- context:\n    cluster: {{ cluster.name }}\n    user: oidc\n  name: default\ncurrent-context: default\nkind: Config\npreferences: {}\nusers:\n- name: oidc\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1beta1\n      args:\n        - oidc-login\n        - get-token\n        - --oidc-issuer-url=https://iam.{{ platform_domain_name }}/realms/{{ keycloak.realm.name }}\n        - --oidc-client-id=kubectl\n        - --oidc-client-secret={{ kubectl_oidc_client_secret }}\n      command: kubectl\n      env: null\n      provideClusterInfo: false\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Remote%20kubectl/#run-your-kubectl-commands-as-usual","title":"Run your kubectl commands as usual","text":"<p>On first login or on token expiration, your browser will open a login page where you can use your platform credentials.</p> <p>Note: Add <code>--grant-type=authcode-keyboard</code> to the args if you want to copy-paste a link in your browser manually, it is useful if you are in a ssh session</p>"},{"location":"rs-infrastructure/docs/how-to/Wazuh-Server_Install/","title":"Wazuh Server Installation","text":""},{"location":"rs-infrastructure/docs/how-to/Wazuh-Server_Install/#1-enable-bcrypt-encryption-for-installation","title":"1. Enable Bcrypt encryption for installation","text":"<p>Backup original file <code>encrypt.py</code> to <code>encrypt.py.ori</code></p> <pre><code>mv ~/miniforge3/envs/rspy/lib/python3.11/site-packages/ansible/utils/encrypt.py ~/miniforge3/envs/rspy/lib/python3.11/site-packages/ansible/utils/encrypt.py.ori\n</code></pre> <p>Download library Passlib library</p> <pre><code>wget https://raw.githubusercontent.com/ansible/ansible/3f74bc08cefccec791c9dc5315185d2396e5c5ac/lib/ansible/utils/encrypt.py -O ~/miniforge3/envs/rspy/lib/python3.11/site-packages/ansible/utils/encrypt.py\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Wazuh-Server_Install/#2-generate-self-signed-certificates","title":"2. Generate self-signed certificates","text":"<p>Generate SSL certificates Kubernetes deployments</p>"},{"location":"rs-infrastructure/docs/how-to/Wazuh-Server_Install/#21-generate-certificates-for-dashboard","title":"2.1 Generate certificates for dashboard","text":"<pre><code>./apps/04-wazuh-server/wazuh/certs/dashboard_http/generate_certs.sh\n</code></pre> <p>Two files should be generated:</p> <p><code>cert.pem</code> <code>key.pem</code></p> <p>In the following folder:</p> <p><code>./apps/04-wazuh-server/wazuh/certs/dashboard_http</code></p>"},{"location":"rs-infrastructure/docs/how-to/Wazuh-Server_Install/#22-generate-certificates-for-all-other-nodes","title":"2.2 Generate certificates for all other nodes","text":"<pre><code>./apps/04-wazuh-server/wazuh/certs/indexer_cluster/generate_certs.sh\n</code></pre> <p>Several files should be generated:</p> <p><code>admin-key-temp.pem</code> <code>admin-key.pem</code> <code>admin.csr</code> <code>admin.pem</code> <code>dashboard-key-temp.pem</code> <code>dashboard-key.pem</code> <code>dashboard.csr</code> <code>dashboard.pem</code> <code>filebeat-key-temp.pem</code> <code>filebeat-key.pem</code> <code>filebeat.csr</code> <code>filebeat.pem</code> <code>node-key-temp.pem</code> <code>node-key.pem</code> <code>node.csr</code> <code>node.pem</code> <code>root-ca-key.pem</code> <code>root-ca.pem</code> <code>root-ca.srl</code></p> <p>into folder :</p> <p><code>./apps/04-wazuh-server/wazuh/certs/indexer_cluster</code></p>"},{"location":"rs-infrastructure/docs/how-to/Wazuh-Server_Install/#3-apply-credentials","title":"3. Apply credentials","text":"<p>[!IMPORTANT] After Wazuh Server application is installed</p> <p>In order to apply credentials set during installation process</p> <p>Regarding Wazuh editor documentation : Kubernetes deployments Update accounts credentials</p>"},{"location":"rs-infrastructure/docs/how-to/Wazuh-Server_Install/#31-open-interactive-session-to-indexer-pod-0","title":"3.1 Open interactive session to indexer pod 0","text":"<pre><code>kubectl exec -it wazuh-indexer-0 -n security -- /bin/bash\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Wazuh-Server_Install/#32-set-variables","title":"3.2 Set variables","text":"<pre><code>export INSTALLATION_DIR=/usr/share/wazuh-indexer\nCACERT=$INSTALLATION_DIR/certs/root-ca.pem\nKEY=$INSTALLATION_DIR/certs/admin-key.pem\nCERT=$INSTALLATION_DIR/certs/admin.pem\nexport JAVA_HOME=/usr/share/wazuh-indexer/jdk\n</code></pre>"},{"location":"rs-infrastructure/docs/how-to/Wazuh-Server_Install/#33-run-command","title":"3.3 Run command","text":"<pre><code>bash /usr/share/wazuh-indexer/plugins/opensearch-security/tools/securityadmin.sh -cd /usr/share/wazuh-indexer/opensearch-security/ -nhnv -cacert  $CACERT -cert $CERT -key $KEY -p 9200 -icl -h $NODE_NAME\n</code></pre> <p>[!NOTE] Note: Wait a little bit that cluster should be ready to execute command. Anyway If status is not ready command is relaunch automatically until that cluster be ready.</p> <p><code>Clusterstate: GREEN</code></p> <p>Test to login to Web UI with new credentials to validate operation.</p>"},{"location":"rs-server/","title":"Index","text":""},{"location":"rs-server/#quick-links","title":"Quick links","text":"<ul> <li> <p>Deployed services: https://dev-rspy.esa-copernicus.eu/docs</p> </li> <li> <p>Online documentation: https://home.rs-python.eu/rs-documentation/rs-server/docs/doc/</p> </li> <li> <p>SonarQube reports:     https://sonarqube.ops-csc.com/dashboard?id=RS-PYTHON_rs-server_AYw0m7ixvQv-JMsowILQ&amp;branch=develop</p> </li> </ul>"},{"location":"rs-server/#overview","title":"Overview","text":"<p>RS server is a toolbox that allows users to retrieve external data used by Copernicus processing chains, store them in internal S3 buckets and catalog them.</p> <p>Its goal is to be used by the Copernicus processing chains to perform their works.</p> <p>The toolbox exposes REST endpoints enabling users to :</p> <ul> <li> <p>search for external data</p> </li> <li> <p>download external data into a S3 bucket</p> </li> <li> <p>catalog data</p> </li> <li> <p>search for data in the catalog</p> </li> </ul> <p>All these functionalities are reserve to authorized users only. The permissions are technical and/or functional.</p>"},{"location":"rs-server/#installing-the-rs-server","title":"Installing the rs-server","text":"<p>TODO explain how to install the rs-server locally or with a cluster.</p>"},{"location":"rs-server/#using-the-rs-server","title":"Using the rs-server","text":"<p>TODO explain how to use the rs-server</p>"},{"location":"rs-server/#developing-the-rs-server","title":"Developing the rs-server","text":"<p>Look at the technical documentation. It contains all the technical details to develop on the rs-server.</p>"},{"location":"rs-server/#links","title":"Links","text":"<ul> <li> <p>Project homepage: https://github.com/RS-PYTHON/rs-server</p> </li> <li> <p>Repository: https://github.com/RS-PYTHON/rs-server</p> </li> <li> <p>Issue tracker: https://github.com/RS-PYTHON/rs-server/issues. In case of sensitive bugs like security vulnerabilities, please contact my@email.com directly instead of using issue tracker. We value your effort to improve the security and privacy of this project!</p> </li> </ul>"},{"location":"rs-server/#licensing","title":"Licensing","text":"<p>The code in this project is licensed under Apache License 2.0.</p> <p></p> <p>This project is funded by the EU and ESA.</p>"},{"location":"rs-server/CHANGELOG/","title":"CHANGELOG","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog.</p> <p>The RS-SERVER is delivered with a version using 2 digits (\"major.minor\"), following the Version specifiers for Python.</p>"},{"location":"rs-server/CHANGELOG/#unreleased","title":"[Unreleased]","text":""},{"location":"rs-server/CHANGELOG/#added","title":"Added","text":"<ul> <li>initialize technical documentation</li> <li>initialize python project</li> </ul>"},{"location":"rs-server/CHANGELOG/#fixed","title":"Fixed","text":"<p>None</p>"},{"location":"rs-server/CHANGELOG/#changed","title":"Changed","text":"<p>None</p>"},{"location":"rs-server/CHANGELOG/#02a1-sprint-11-2024-06-26","title":"[0.2a1] - Sprint 11 - 2024-06-26","text":""},{"location":"rs-server/CHANGELOG/#added_1","title":"Added","text":"<ul> <li>RSPY-141: Implement the STAC timestamps extension in Catalog</li> <li>RSPY-153: Implicit collection owner when calling catalog endpoints</li> <li>RSPY-161: Map CADIP session files as STAC assets for stations that support Expand=Files</li> <li>RSPY-186: Catalog application does crash properly during initialization</li> <li>RSPY-192: Update CADU search endpoint to search by session_id:</li> <li>RSPY-210: Deploy Grafana Tempo on K8S cluster:</li> <li>RSPY-254: [Safety] Errors displayed on Wazuh UI:</li> <li>RSPY-256: [rs-testmeans] document how to add mock data:</li> <li>RSPY-277: Missing probes in some RS-Server components:</li> <li>RSPY-286: STAC OpenAPI links not working:</li> <li>RSPY-288: Update CADIP/AUXIP mocks to support PVC:</li> <li>RSPY-292: API Key manager wipes IAM roles of existing keys:</li> <li>RSPY-294: Umbrella to collect all ISSUES points (sprint 11):</li> <li>RSPY-296: Simplify authentication to RS frontend:</li> <li>RSPY-300: Internal error when calling /cadip/{station}/session without params:</li> <li>RSPY-303: Catalog asset download links are invalid:</li> <li>RSPY-304: Response of catalog asset download is invalid</li> </ul>"},{"location":"rs-server/CHANGELOG/#01a10-2024-06-05","title":"[0.1a10] - 2024-06-05","text":""},{"location":"rs-server/CHANGELOG/#added_2","title":"Added","text":"<ul> <li>RSPY-112: Take into account feedback on generated documentation</li> <li>RSPY-116: Prepare and perform 0.1 delivery for ESA checkpoint</li> <li>RSPY-159: StacClient Class Python development</li> <li>RSPY-241: [Deployment] JupiterHub UI not reachable after deployment</li> <li>RSPY-245: [Deployment] Missing namespaces in kustomization.yaml</li> <li>RSPY-252: [Deployment] Namespace issue during installation of Neuvector crds</li> <li>RSPY-253: Deploy RS-Client libraries into JupiterLab instances</li> <li>RSPY-255: Infra bugfixing for CP 0.1</li> <li>RSPY-258: [Deployment] Grafana in CrashLoop when no plugin</li> <li>RSPY-259: [Deployment] Missing secret for Loki</li> <li>RSPY-260: [monitoring] Monitoring certificate secret name not match with deployment for grafana and prometheus</li> <li>RSPY-261: [Monitoring] No prometheus value retrieved for neuvector</li> <li>RSPY-263: [Monitoring] Prometheus GrafanaDatasource not created during prometheus deployment</li> <li>RSPY-267: [Security] No severity score on huge amount of CVE</li> </ul>"},{"location":"rs-server/CHANGELOG/#01a9-2024-05-15","title":"[0.1a9] - 2024-05-15","text":""},{"location":"rs-server/CHANGELOG/#added_3","title":"Added","text":"<ul> <li>RSPY-130: Deploy Grafana on K8S cluster</li> <li>RSPY-133: Deploy Prefect Workers on K8S cluster</li> <li>RSPY-148: CadipClient Class Python development</li> <li>RSPY-149: AuxipClient Class Python development</li> <li>RSPY-158: RsClient Class Python development</li> <li>RSPY-174: [EODAG] download in parallel on the same machine or container</li> <li>RSPY-181: Deployment: label not well set by deployment script</li> <li>RSPY-196: Platform deployment: error keycloak realm import</li> <li>RSPY-213: Improve error handling in catalog</li> <li>RSPY-218: Sprint 9 corrections of issues</li> <li>RSPY-220: Kubectl commands with kubectl OIDC not working</li> <li>RSPY-224: RS-Server: missing resources tag in yaml</li> <li>RSPY-225: RS-Server: No image version reference</li> <li>RSPY-227: Add missing unit tests for * RSPY-120</li> <li>RSPY-228: Add missing configuration for promtail</li> <li>RSPY-229: Helm charts - Dynamic list based on values</li> <li>RSPY-239: [Deployment] No JupiterHub image reachable from Validation platform</li> <li>RSPY-240: [Deployment] No Secret Create during Wazuh Agent deployment</li> </ul>"},{"location":"rs-server/CHANGELOG/#01a8-2024-04-24","title":"[0.1a8] - 2024-04-24","text":""},{"location":"rs-server/CHANGELOG/#added_4","title":"Added","text":"<ul> <li>RSPY-69: Implement access control to the catalog (with UAC)</li> <li>RSPY-99: Deploy JupyterHub on K8S cluster</li> <li>RSPY-120: Implement a first S1L0 processing Prefect @flow</li> <li>RSPY-123: Create Jupyter notebook to launch Prefect chains</li> <li>RSPY-128: Deploy promtail and Grafana Loki on K8S cluster</li> <li>RSPY-162: Python modules for log &amp; trace</li> <li>RSPY-167: New endpoint to get CADIP session information</li> <li>RSPY-170: Platform deployment and start-stop playbook failed due to missing credential</li> <li>RSPY-176: Platform deployment: first application deployment execution failed for the step cluster-issuer</li> <li>RSPY-177: Platform Deployment: Failed to deployed application due to missing parameter in group_vars</li> <li>RSPY-179: Platform deployment: no cinder controller for PVC</li> <li>RSPY-180: Platform deployment: kubelet errors with cpu manager</li> <li>RSPY-182: Wazuh agent is being reinstalled when cluster is restarted</li> <li>RSPY-183: Prometheus is not accessible from the ingress</li> <li>RSPY-185: Sprint 8 corrections of infrastructure issues</li> <li>RSPY-186: Catalog application does crash properly during initialization</li> <li>RSPY-219: Improve AUXIP &amp; CADIP mockups representativeness</li> </ul>"},{"location":"rs-server/CHANGELOG/#01a7-2024-04-03","title":"[0.1a7] - 2024-04-03","text":""},{"location":"rs-server/CHANGELOG/#added_5","title":"Added","text":"<ul> <li>RSPY-86: Deploy security stack on K8S cluster</li> <li>RSPY-122: Deploy RS-Server on K8S cluster</li> <li>RSPY-129: Deploy prometheus and node_exporter on K8S cluster</li> <li>RSPY-135: Setup Swagger/OpenAPI documentation aggregation frontend</li> <li>RSPY-137: Link RS-Server frontend with Backend Catalog endpoints (Without UAC) - Part 3</li> <li>RSPY-152: Simplify Catalog endpoints with \"ownerId:collectionId\"</li> <li>RSPY-157: Update the datetimes STAC mapping for ADGS &amp; CADIP</li> <li>RSPY-163: Implement access control to the CADIP stations (with UAC)</li> <li>RSPY-164: Implement access control to the ADGS center (with UAC)</li> <li>RSPY-169: Project versioning and naming</li> <li>RSPY-171: [URGENT] Replace Miniconda by Miniforge</li> </ul>"},{"location":"rs-server/CHANGELOG/#01a6-2024-03-14","title":"[0.1a6] - 2024-03-14","text":""},{"location":"rs-server/CHANGELOG/#added_6","title":"Added","text":"<ul> <li>RSPY-15: Setup UAC Manager</li> <li>RSPY-25: Override endpoint \"publication of STAC item\" to the Catalog backend server</li> <li>RSPY-49: Deploy Prefect Server on K8S cluster</li> <li>RSPY-85: Implement CADU ingestion Prefect @flow</li> <li>RSPY-100: Link RS-Server frontend with Backend Catalog endpoints (Without UAC) - Part 2</li> <li>RSPY-115: Implement ADGS ingestion Prefect @flow</li> <li>RSPY-125: Cluster configuration folder is hard-coded</li> <li>RSPY-139: Add endpoint to download product (without UAC)</li> </ul>"},{"location":"rs-server/CHANGELOG/#01a5-2024-02-21","title":"[0.1a5] - 2024-02-21","text":""},{"location":"rs-server/CHANGELOG/#added_7","title":"Added","text":"<ul> <li>RSPY-68: Configure OpenID Connect on K8S cluster</li> <li>RSPY-73: Link RS-Server frontend with CADIP backend endpoints (without UAC)</li> <li>RSPY-78: Link RS-Server frontend with Backend Catalog endpoints (Without UAC) - Part 1</li> <li>RSPY-81: Deploy keycloak on K8S cluster</li> <li>RSPY-91: Link RS-Server frontend with ADGS backend endpoints (without UAC)</li> <li>RSPY-94: Implement a DPR mockup</li> <li>RSPY-121: Setup Ingress Controller</li> <li>RSPY-126: Initialize RS-SERVER-Libraries repository</li> <li>RSPY-134: Setup Helm Chart Releaser to use Github Pages as Helm chart repository</li> <li>RSPY-138: Add public architecture documentation on GitHub</li> </ul>"},{"location":"rs-server/CHANGELOG/#01a4-2024-01-31","title":"[0.1a4] - 2024-01-31","text":""},{"location":"rs-server/CHANGELOG/#added_8","title":"Added","text":"<ul> <li>RSPY-29: Deploy Kubernetes</li> <li>RSPY-33: Generate CI/CD documentation from Github</li> <li>RSPY-87: Develop ADGS backend server with first endpoint \"GET /adgs/aux/search\"</li> <li>RSPY-88: Add endpoint GET \"/adgs/aux\" to ADGS backend server</li> <li>RSPY-90: Add endpoint GET \"/adgs/aux/status\" to ADGS backend server</li> <li>RSPY-117: Create a Jupyter demo for local target</li> </ul>"},{"location":"rs-server/CHANGELOG/#01a3-2024-01-16","title":"[0.1a3] - 2024-01-16","text":""},{"location":"rs-server/CHANGELOG/#added_9","title":"Added","text":"<ul> <li>RSPY-14: Add endpoint \"download cadu\" to CADIP backend server</li> <li>RSPY-16: Develop CADIP backend server with first endpoint \"get cadu\"</li> <li>RSPY-31: Initiate Developer Guide</li> <li>RSPY-39: Implement a CADIP station mockup</li> <li>RSPY-41: Implement an ADGS station mockup</li> <li>RSPY-53: Develop Catalog backend server</li> <li>RSPY-72: Add endpoint \"CADU status\" to CADIP backend server</li> </ul>"},{"location":"rs-server/docs/doc/","title":"RS-Server","text":"<p>The Reference System server provides a set of services necessary to build Copernicus processing workflows. All services are subject to access control.</p> <p>STAC is everywhere with RS-Server. RS-Server offers a catalog of Sentinel products compatible with the STAC (SpatioTemporal Asset Catalog) standard, but also on the next release the staging function to retrieve AUXIP and CADIP data will be provided with a STAC interface.</p> <p></p>"},{"location":"rs-server/docs/doc/#features","title":"Features","text":"<p>To achieve this, RS-Server exposes REST endpoints that allow users to: -   Search and stage CADU chunks from CADIP stations -   Search and stage auxiliary data from AUXIP station -   Connect to all the endpoints from SpatioTemporal Asset Catalog API</p> <p>Please note that the STAC catalog will also embed STAC extension to support Sentinel product format from EOPF-CPM.</p> <p>All these functionalities are available exclusively to authorized users. Permissions can be both technical and functional.</p>"},{"location":"rs-server/docs/doc/#user-manual","title":"User Manual","text":"<p>Access the User Manual for detailed instructions and guidance.</p>"},{"location":"rs-server/docs/doc/#developer-manual","title":"Developer Manual","text":"<p>Access the Developer Manual for technical documentation and developer guidelines.</p>"},{"location":"rs-server/docs/doc/developer_manual/","title":"Service Structure","text":"<p>The RS-Server architecture is composed of several modular services, each designed to offer distinct functionalities accessible via HTTPS endpoints. Here is a detailed overview of the key aspects of the RS-Server services:</p> <ol> <li> <p>Independence and Modularity:</p> <ul> <li>Each service operates independently, encapsulated within its own Docker image. This isolation ensures that services can be developed, deployed, and scaled without interdependencies.</li> <li>Services are designed to be stateless, which facilitates easy scaling and load distribution across multiple instances.</li> </ul> </li> <li> <p>Deployment Modes:</p> <ul> <li>Laptop Mode: Services can be initiated on a local machine, making it convenient for development and testing purposes.</li> <li>Cluster Mode: For production environments, services can be deployed in a cluster setup , enhancing their ability to handle multiple concurrent requests. This mode supports parallel processing, thereby improving performance and reliability.</li> </ul> </li> <li> <p>Cluster Management:</p> <ul> <li>The cluster is governed by Kubernetes, which orchestrates the deployment, scaling, and management of the containerized services. Kubernetes ensures high availability, load balancing, and efficient resource utilization across the cluster.</li> </ul> </li> <li> <p>Scalability:</p> <ul> <li>Due to their stateless nature, services can be scaled horizontally. This means additional instances can be spawned to manage increased loads without any complex configuration.</li> </ul> </li> </ol> <p>The RS-Server's microservices architecture ensures robustness, scalability, and ease of management. By leveraging Docker for containerization and Kubernetes for orchestration, along with the rs-server-common package for shared functionalities, RS-Server offers a flexible and efficient solution for various deployment needs. This architecture supports both small-scale local deployments and large-scale production environments, providing the necessary tools and structures to handle a wide range of requests efficiently.</p>"},{"location":"rs-server/docs/doc/developer_manual/#main-services-common-structures-and-mechanisms","title":"Main Services, Common Structures and Mechanisms","text":"<p>There are currently 3 main services implemented:</p> <ol> <li> <p>CADIP service It facilitates the search and download of files from a CADIP server to a S3 bucket. Each instance of this service is started for a given station and provides 3 endpoints:</p> <ul> <li>A search endpoint that enables to search files for a time period</li> <li>A download endpoint that enables to download a file using own its name</li> <li>A status endpoint that enables the check of the current status for a downloading file.</li> </ul> </li> <li> <p>ADGS service - Provides the same functionality as CADIP service, facilitates the search and download of files from an ADGS server to a S3 bucket</p> </li> <li>Catalog service - facilitates the use of the main RS-Server catalog in PySTAC format</li> </ol> <p>To maintain consistency and streamline development, all services use shared components and mechanisms provided by the <code>rs-server-common package</code>. This package includes:</p> <ul> <li>Standardized Data Structures: Ensuring uniformity in data handling across all services.</li> <li>Common Utilities: Functions and tools that are frequently used across different services, promoting code reuse and reducing redundancy.</li> <li>Configuration Management: Centralized configuration settings that can be applied uniformly to all services, simplifying deployment and maintenance.</li> </ul>"},{"location":"rs-server/docs/doc/developer_manual/#start-coding","title":"Start coding","text":"<p>To start coding on RS-Server, you have to install it first:</p> <ul> <li>Installation</li> </ul>"},{"location":"rs-server/docs/doc/developer_manual/#code-style","title":"Code Style","text":"<p>The following is the code style used in developing for RS-Server:</p> <ul> <li>Code style</li> </ul>"},{"location":"rs-server/docs/doc/developer_manual/#rs-server-rest-api-documentation","title":"RS-Server REST API Documentation","text":"<p>The frontend of the RS-Server ecosystem is a specialized service that primarily provides a REST API documentation. Here\u2019s a detailed overview of its features and functionality: Service Overview</p> <ol> <li> <p>Purpose:</p> <ul> <li>The frontend service is dedicated to displaying and managing the API documentation for all RS-Server services.</li> <li>It does not implement any functional endpoints of the RS-Server itself but focuses on documentation.</li> </ul> </li> <li> <p>Endpoints:</p> <ul> <li>/doc Endpoint: This endpoint displays the aggregated OpenAPI specification of all RS-Server services using SwaggerUI, providing an interactive interface for users to explore the API.</li> <li>/openapi.json Endpoint: This endpoint serves the same OpenAPI specification in JSON format, allowing programmatic access to the documentation.</li> </ul> </li> </ol> <p>OpenAPI Specification</p> <ol> <li>Specification Source:<ul> <li>The OpenAPI specification is provided by a JSON file. The location of this file is configurable via an environment variable read at the service startup.</li> </ul> </li> <li>Configuration:<ul> <li>The environment variable determines the path to the JSON file containing the OpenAPI specification, allowing flexibility in deployment and updates.</li> </ul> </li> </ol> <p>Static Documentation</p> <ol> <li> <p>Build Procedure:</p> <ul> <li>The content of the documentation is static. It is computed during the frontend build procedure and then integrated statically into the frontend service.</li> <li>Whenever a RS-Server service endpoint is updated, the frontend service must be rebuilt to provide up-to-date documentation.</li> </ul> </li> <li> <p>CI Workflow:</p> <ul> <li>The OpenAPI specification is constructed as part of the continuous integration (CI) workflow that builds the frontend service executable.</li> <li>This workflow starts all RS-Server services, retrieves their individual OpenAPI specifications, aggregates them, and stores the combined specification in a file.</li> <li>This file is then integrated into the Docker image of the frontend service.</li> </ul> </li> </ol> <p>Configuration Management</p> <ol> <li>Configuration File:<ul> <li>A configuration file defines all RS-Server services that should be included in the aggregated OpenAPI specification.</li> <li>Each time a new service is added, its details must be included in this configuration file to ensure it is part of the aggregated documentation.</li> </ul> </li> </ol> <p>Functionality</p> <ol> <li>Interactive Documentation:<ul> <li>Although the frontend does not implement the functional endpoints, the <code>Try it out</code> feature of SwaggerUI is functional. This is enabled by the deployment of RS-Server with an ingress controller that manages request redirection to the appropriate RS-Server services.</li> </ul> </li> </ol> <p>By maintaining a clear separation between documentation and functional implementation, the frontend service ensures that users have access to comprehensive and interactive API documentation, while also facilitating easy updates and integration of new services through a structured CI workflow.</p>"},{"location":"rs-server/docs/doc/developer_manual/#python-api-library","title":"Python API Library","text":"<p>Please check the <code>Python API Library</code> link in the sidebar navigation to access the generated documentation directly from the source code.</p>"},{"location":"rs-server/docs/doc/developer_manual/#additional-information","title":"Additional information","text":"<p>Here are some additional information that may help you in finding the answer when coding on RS-Server:</p> <ul> <li> <p>Tree structure</p> </li> <li> <p>Workflow</p> </li> <li> <p>CI</p> </li> </ul>"},{"location":"rs-server/docs/doc/user_manual/","title":"Overall Architecture","text":"<p>The overall architecture is described here: Architecture</p>"},{"location":"rs-server/docs/doc/user_manual/#main-functionalities","title":"Main Functionalities","text":"<p>The main functionalities are explained here: Functionalities</p>"},{"location":"rs-server/docs/doc/dev/code-style/","title":"Code style","text":"<p>These are the coding style followed on the RS-Server project.</p>"},{"location":"rs-server/docs/doc/dev/code-style/#pre-commit-checks","title":"Pre-commit checks","text":"<p>pre-commit rules are configured to perform basic checks before each commit. It can be installed on local computer using installing your environment. It is recommended to verify if code follows the project coding rules.</p>"},{"location":"rs-server/docs/doc/dev/code-style/#python-style","title":"Python style","text":"<p>By default, the pep8 is followed.</p> <p>All python files are formatted by black. It is run in the pre-commit hooks and can be run after each file save. The line length is extended to 120 but keep lines as small and readable as possible.</p> <p>The semantic and stylistic verifications are performed by pylint and flake8 in the CI workflow.</p> <p>The doc-strings are written using the markdown because the Python API is generated using mkdocs.</p> <p>The following file header should be added at the start of each python file.</p> <pre><code># Copyright 2024 CS Group\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</code></pre>"},{"location":"rs-server/docs/doc/dev/code-style/#unit-test-style","title":"Unit test style","text":"<p>The unittests are written with pytest.</p> <p>We use marks to categorize tests. The following marks are defined currently.</p> <pre><code>import pytest\n\n@pytest.mark.integration\ndef a_fixture_for_integration_only():\n    return None\n\n@pytest.mark.unit\ndef test_a_unit_test():\n    assert False\n\n@pytest.mark.integration\ndef test_an_integration_test(a_fixture_for_integration_only):\n    assert False\n</code></pre>"},{"location":"rs-server/docs/doc/dev/code-style/#commit-style","title":"Commit style","text":"<p>The commit messages are written using the conventional commit.</p> <p>The messages try to follow the following best practices.</p>"},{"location":"rs-server/docs/doc/dev/code-style/#changelog","title":"Changelog","text":"<p>The project changelog is produced following keepchangelog practices. Please keep the changelog up-to-date after each modification.</p>"},{"location":"rs-server/docs/doc/dev/code-style/#documentation","title":"Documentation","text":"<p>The documentation is written in mkdocs following the documentation system.</p>"},{"location":"rs-server/docs/doc/dev/background/ci/","title":"CI","text":"<p>The CI/CD (Continuous Integration and Delivery) is implemented using GitHub Actions yaml scripts.</p> <p>Implemented workflows are:</p> <ul> <li> <p>Check code quality</p> </li> <li> <p>Publish wheels and Docker images</p> </li> <li> <p>Generate documentation</p> </li> </ul>"},{"location":"rs-server/docs/doc/dev/background/ci/#check-code-quality-workflow","title":"Check code quality workflow","text":"<p>This workflow is run either automatically after each <code>git push</code> command on each branch and pull request, or manually.</p> <p>It runs the following jobs:</p> <ul> <li> <p>Check format (pre-commit, black, isort)</p> <ul> <li>This job checks that you have run <code>pre-commit run --all-files</code>     in your local git repository before committing. The <code>pre-commit</code>     runs <code>black</code> and <code>isort</code> to auto-format your source code and     facilitate reviewing differences and merging between git     branches.</li> </ul> </li> <li> <p>Check linting (pylint, flake8)</p> <ul> <li>These linters analyse your code without actually running it.     They check for errors, enforce a coding standard, look for code     smells, and can make suggestions about how the code could be     refactored.</li> </ul> </li> <li> <p>Check typing (mypy)</p> <ul> <li>Add type annotations to your Python programs, and use mypy to     type check them.</li> </ul> </li> <li> <p>Check security (bandit, safety, trivy)</p> <ul> <li> <p>Bandit is a tool designed to find common security issues in     Python code.</p> </li> <li> <p>Safety checks Python dependencies for known security     vulnerabilities and suggests the proper remediations for     vulnerabilities detected.</p> </li> <li> <p>Trivy is a comprehensive and versatile security scanner that     looks for security issues, and targets where it can find those     issues. The scan results are updated into the GitHub Security     tab:     https://github.com/RS-PYTHON/rs-server/security/code-scanning</p> </li> </ul> </li> <li> <p>Run unit tests (pytest)</p> </li> <li> <p>Run integration tests (pytest)</p> <ul> <li> <p>Test source code and determine if it is fit to use.</p> </li> <li> <p>Unit tests are faster than integration tests and use mockup     servers. Integration tests are run on the production     infrastructure (to be confirmed).</p> </li> <li> <p>They also calculate code coverage i.e. percentage measure of the     degree to which the source code is executed when the tests are     run. Only the unit tests coverage is shown in sonarqube.</p> </li> </ul> </li> <li> <p>Quality report (sonarqube)</p> <ul> <li> <p>SonarQube is a platform for continuous inspection of code     quality to perform automatic reviews with static analysis of     code to detect bugs and code smells. It offers reports on     duplicated code, coding standards, unit tests, code coverage,     code complexity, comments, bugs, and security recommendations.</p> </li> <li> <p>It renders the reports from pylint, flake8, bandit and code     coverage in a more graphical way.</p> </li> <li> <p>Reports are displayed at:     https://sonarqube.ops-csc.com/dashboard?branch=develop&amp;id=RS-PYTHON_rs-server_AYw0m7ixvQv-JMsowILQ</p> </li> </ul> </li> </ul>"},{"location":"rs-server/docs/doc/dev/background/ci/#publish-wheels-and-docker-images-workflow","title":"Publish wheels and Docker images workflow","text":"<p>This workflow is run either automatically after adding a git tag to a commit, or manually. It:</p> <ul> <li> <p>Builds source code into Python wheel packages.</p> <ul> <li> <p>Upload them into the Python registry.</p> </li> <li> <p>Make them available for manual download as GitHub Actions     artifacts.</p> </li> </ul> </li> <li> <p>Builds the project Docker images (that use the Python wheel     packages).</p> <ul> <li> <p>Scan them with Trivy for security issues and update results into     the GitHub Security tab:     https://github.com/RS-PYTHON/rs-server/security/code-scanning</p> </li> <li> <p>Upload them into the ghcr.io GitHub package registry:     https://github.com/orgs/RS-PYTHON/packages</p> </li> </ul> </li> </ul> <p>The git tag name must conform to the following syntax examples:</p> <ul> <li> <p><code>v0.2</code> is a version for a release with at least major updates.</p> </li> <li> <p><code>v0.2.1</code> is a version for a release without any major updates.</p> </li> <li> <p><code>v0.1rc1</code> is the version for the first 0.1 release-candidate.</p> </li> <li> <p><code>v0.1a3</code> is the version for sprint 3 features integration.</p> </li> <li> <p><code>v0.1a3+dev9afcfc1</code> is dev commit version during the sprint 3.</p> </li> </ul> <p>The <code>Poetry dynamic versioning</code> python plugin is then used to determine automatically the wheels and Docker images version name. Note that it is slighty different from the git tag name:</p> <ul> <li> <p><code>v</code> is removed.</p> </li> <li> <p>For Docker images, <code>+</code> is replaced by <code>.</code></p> </li> </ul> <p>So we have e.g. the wheel filenames:</p> <ul> <li> <p><code>rs_server-0.2-py3-none-any.whl</code></p> </li> <li> <p><code>rs_server-0.2.1-py3-none-any.whl</code></p> </li> <li> <p><code>rs_server-0.2.1rc1-py3-none-any.whl</code></p> </li> <li> <p><code>rs_server-0.1a3-py3-none-any.whl</code></p> </li> <li> <p><code>rs_server-0.1a3+dev9afcfc1-py3-none-any.whl</code></p> </li> </ul> <p>And the Docker images:</p> <ul> <li> <p><code>ghcr.io/rs-python/rs-server:0.2</code></p> </li> <li> <p><code>ghcr.io/rs-python/rs-server:0.2.1</code></p> </li> <li> <p><code>ghcr.io/rs-python/rs-server:0.2.1rc1</code></p> </li> <li> <p><code>ghcr.io/rs-python/rs-server:0.1a3</code></p> </li> <li> <p><code>ghcr.io/rs-python/rs-server:0.1a3.dev9afcfc1</code></p> </li> </ul> <p>When running manually the workflow, the version name is determined automatically as:</p> <ul> <li> <p><code>0.0.0.post1.dev0+&lt;short-hash-commit&gt;</code> for the wheels.</p> </li> <li> <p><code>0.0.0.post1.dev0.&lt;short-hash-commit&gt;</code> for the Docker images.</p> </li> </ul> <p>e.g. <code>0.0.0.post1.dev0+216f7fa</code> and <code>0.0.0.post1.dev0.216f7fa</code>.</p> <p>But running manually the workflow is discouraged, it is preferred to generate the wheels and Docker images by creating a new git tag (to be discussed).</p>"},{"location":"rs-server/docs/doc/dev/background/ci/#generate-documentation-workflow","title":"Generate documentation workflow","text":"<p>This workflow is run either automatically after adding a git tag to a commit, or manually.</p> <p>It builds the documentation following the how-to procedure to generate the documentation.</p> <p>Then, the result is published as the GitHub Pages of the rs-server.</p>"},{"location":"rs-server/docs/doc/dev/background/tree-structure/","title":"Tree structure","text":"<p>The rs-server repository tree structure is given in the following graph</p> <pre><code>skinparam Legend {\n        BorderColor transparent\n}\n\nlegend\nrs-server\n|_ docs/\n|_ services/\n|_ tests/\n|_ .gitignore\n|_ .pre-commit-config.yaml\n|_ CHANGELOG.md\n|_ LICENSE\n|_ poetry.lock\n|_ pyproject.toml\n|_ README.md\n\nend legend\n</code></pre> <ul> <li> <p>services folder contains all the production code</p> </li> <li> <p>tests folder contains all the unit and integration tests</p> </li> <li> <p>docs folder contains all the technical documentation</p> </li> </ul>"},{"location":"rs-server/docs/doc/dev/background/workflow/","title":"Git","text":"<p>The project is using the gitflow.</p> <p>The feature branches are named following the pattern \"feat-&lt;jira-id&gt;/&lt;short-description&gt;\" For example : \"feat-rspy31/init-tech-doc\"</p> <p>Sometimes, a branch can implement multiple stories. For example : \"feat-rspy36-37/read-write-storage\"</p>"},{"location":"rs-server/docs/doc/dev/background/workflow/#jira-tickets","title":"JIRA tickets","text":"<p>The backlog is handled on a private JIRA instance.</p> <p>The ticket is initially in the TODO state. When the implementation starts, the state becomes \"IN PROGRESS\" and the ticket is assigned to the responsible developer. When the implementation is completed, the state becomes \"IMPLEMENTED\".</p>"},{"location":"rs-server/docs/doc/dev/background/workflow/#development-dor-definition-of-ready","title":"Development DoR (Definition of Ready)","text":"<ul> <li> <p>Development team understands what is expected</p> </li> <li> <p>Test cases are writen and clear</p> </li> <li> <p>Identify the specific integration tests if needed</p> </li> <li> <p>Technical documentation to write is identified</p> </li> <li> <p>User documentation to write is identified</p> </li> </ul>"},{"location":"rs-server/docs/doc/dev/background/workflow/#development-dod-definition-of-done","title":"Development DoD (Definition of Done)","text":"<ul> <li> <p>Code written covers the functionality</p> </li> <li> <p>New code is covered by unit tests</p> </li> <li> <p>New code is covered by integration tests if any</p> </li> <li> <p>New test cases have been automated</p> </li> <li> <p>Documentation has been updated</p> </li> <li> <p>Changelog has been updated</p> </li> <li> <p>All unit tests are green</p> </li> <li> <p>All integration tests are green</p> </li> <li> <p>All acceptance tests are green</p> </li> <li> <p>The best practices are followed</p> <ul> <li> <p>The design is followed</p> </li> <li> <p>The CI checks have been run and are green</p> </li> <li> <p>The sonarqube errors have been fixed</p> </li> </ul> </li> <li> <p>A code review with a team member has been made</p> </li> </ul>"},{"location":"rs-server/docs/doc/dev/background/workflow/#code-review","title":"Code review","text":"<p>The objectives of the code reviews are :</p> <ul> <li> <p>Double-check the DoD completion</p> </li> <li> <p>Share knowledge accros development team</p> </li> <li> <p>Human feedback on written tests, code and documentation</p> </li> </ul>"},{"location":"rs-server/docs/doc/dev/design/design/","title":"Design","text":"<p>The rs-server is divided into two main elements :</p> <ul> <li> <p>the rs-server frontend</p> </li> <li> <p>the rs-server backend</p> </li> </ul> <p>The rs-server frontend is a facade for users that verify the user privileges before accessing to the services provided by the rs-server backend.</p>"},{"location":"rs-server/docs/doc/dev/design/design/#rs-server-frontend","title":"rs-server frontend","text":"<p>TODO To Be Defined</p>"},{"location":"rs-server/docs/doc/dev/design/design/#rs-server-backend","title":"rs-server backend","text":"<p>The rs-server backend is divided in several services.</p> <p>TODO insert a table of the service with short description and the list of functionalities</p> <p>Each service provides several functionalities as https endpoints. All services are independent of others. Each service is provided as an independent Docker image. Each service can be started in laptop or cluster mode (see later for more details). Each service can handle multiple requests in parallel in cluster mode. Services are stateless, so they can easily be scaled and the activity be divided into the multiple instances.</p> <p>All services share some common structures and mechanisms. These common elements are provided by the rs-server-common package.</p>"},{"location":"rs-server/docs/doc/dev/design/design/#sources-organisation","title":"Sources organisation","text":"<p>TODO insert the service tree structure here</p> <p>Each service has its own folder named with the service name. This folder is a python project. It contains the sources, tests, configuration,\u2026 for this service. It also contains a Dockerfile describing the service output image. The sources of each layer are separated in a specific package.</p> <p>The shared elements are provided by the rs-server-common package. It is handled as a python project as if it is a rs-server service.</p>"},{"location":"rs-server/docs/doc/dev/design/design/#services","title":"Services","text":""},{"location":"rs-server/docs/doc/dev/design/design/#cadip-service","title":"Cadip service","text":"<p>The cadip service enables users to retrieve data from cadip stations. Each instance of this service is started for a given station.</p> <p>It provides 2 endpoints :</p> <ul> <li> <p>a search endpoint that enables to search files for a time period</p> </li> <li> <p>a download endpoint that enables to download a file from its id and     name</p> </li> </ul> <p>The processes are realized by a DataRetriever configured to use the EODAG provider \"CADIP\".</p>"},{"location":"rs-server/docs/doc/dev/design/design/#configuration","title":"Configuration","text":"<p>This service uses several configurations elements :</p> <ul> <li> <p>the station to url mapping that defines the url of the cadip server     for each station</p> </li> <li> <p>the station handled by the instance</p> </li> <li> <p>the eodag configuration used to interact with the cadip station =     Frontend service</p> </li> </ul> <p>The frontend is a rs-service service. It only provides REST API documentation. It contains : * a /doc endpoint displaying the aggregated openapi specification of all the rs-server services using SwaggerUI * a /openapi.json displaying the same openapi specification on a json format</p> <p>The openapi specification is provided by a json file. The location of this file is configurable. It is given by an environment variable read at the start of the service.</p> <p>It contains no functional implementation of the rs-server services endpoints. Nevertheless, the \"Try it out\" functionality from Swagger is working since the rs-server is deployed with an ingres controller that handles the redirection of the requests to the dedicated rs-server service.</p> <p>The content of the documentation is static. It is computed during the frontend build procedure and then integrated statically in the frontend service. Each time a service endpoint is updated, the frontend has to be rebuilt also to provide an up-to-date documentation.</p> <p>The openapi specification is built on the CI workflow that build the frontend service executable. The procedure starts all the rs-server services and retrieve the openapi specification of each service. Once done, it creates an aggregated specification and store it in a file. This file is then integrated in the docker image of the service.</p> <p>The procedure uses a configuration file. this file describes all the rs-server services that should be integrated in the aggregated openapi specification. Each time a new service is added, its configuration has to be added in this configuration file.</p>"},{"location":"rs-server/docs/doc/dev/design/services/cadip/","title":"Cadip","text":"<p>The cadip service enables users to retrieve data from cadip stations. Each instance of this service is started for a given station.</p> <p>It provides 2 endpoints :</p> <ul> <li> <p>a search endpoint that enables to search files for a time period</p> </li> <li> <p>a download endpoint that enables to download a file from its id and     name</p> </li> </ul> <p>The processes are realized by a DataRetriever configured to use the EODAG provider \"CADIP\".</p>"},{"location":"rs-server/docs/doc/dev/design/services/cadip/#configuration","title":"Configuration","text":"<p>This service uses several configurations elements :</p> <ul> <li> <p>the station to url mapping that defines the url of the cadip server     for each station</p> </li> <li> <p>the station handled by the instance</p> </li> <li> <p>the eodag configuration used to interact with the cadip station</p> </li> </ul>"},{"location":"rs-server/docs/doc/dev/design/services/design/","title":"Design","text":"<p>The rs-server backend is divided in several services.</p> <p>TODO insert a table of the service with short description and the list of functionalities</p> <p>Each service provides several functionalities as https endpoints. All services are independent of others. Each service is provided as an independent Docker image. Each service can be started in laptop or cluster mode (see later for more details). Each service can handle multiple requests in parallel in cluster mode. Services are stateless, so they can easily be scaled and the activity be divided into the multiple instances.</p> <p>All services share some common structures and mechanisms. These common elements are provided by the rs-server-common package.</p>"},{"location":"rs-server/docs/doc/dev/design/services/design/#sources-organisation","title":"Sources organisation","text":"<p>TODO insert the service tree structure here</p> <p>Each service has its own folder named with the service name. This folder is a python project. It contains the sources, tests, configuration,\u2026 for this service. It also contains a Dockerfile describing the service output image. The sources of each layer are separated in a specific package.</p> <p>The shared elements are provided by the rs-server-common package. It is handled as a python project as if it is a rs-server service.</p>"},{"location":"rs-server/docs/doc/dev/design/services/design/#services","title":"Services","text":""},{"location":"rs-server/docs/doc/dev/design/services/design/#cadip-service","title":"Cadip service","text":"<p>The cadip service enables users to retrieve data from cadip stations. Each instance of this service is started for a given station.</p> <p>It provides 2 endpoints :</p> <ul> <li> <p>a search endpoint that enables to search files for a time period</p> </li> <li> <p>a download endpoint that enables to download a file from its id and     name</p> </li> </ul> <p>The processes are realized by a DataRetriever configured to use the EODAG provider \"CADIP\".</p>"},{"location":"rs-server/docs/doc/dev/design/services/design/#configuration","title":"Configuration","text":"<p>This service uses several configurations elements :</p> <ul> <li> <p>the station to url mapping that defines the url of the cadip server     for each station</p> </li> <li> <p>the station handled by the instance</p> </li> <li> <p>the eodag configuration used to interact with the cadip station =     Frontend service</p> </li> </ul> <p>The frontend is a rs-service service. It only provides REST API documentation. It contains : * a /doc endpoint displaying the aggregated openapi specification of all the rs-server services using SwaggerUI * a /openapi.json displaying the same openapi specification on a json format</p> <p>The openapi specification is provided by a json file. The location of this file is configurable. It is given by an environment variable read at the start of the service.</p> <p>It contains no functional implementation of the rs-server services endpoints. Nevertheless, the \"Try it out\" functionality from Swagger is working since the rs-server is deployed with an ingres controller that handles the redirection of the requests to the dedicated rs-server service.</p> <p>The content of the documentation is static. It is computed during the frontend build procedure and then integrated statically in the frontend service. Each time a service endpoint is updated, the frontend has to be rebuilt also to provide an up-to-date documentation.</p> <p>The openapi specification is built on the CI workflow that build the frontend service executable. The procedure starts all the rs-server services and retrieve the openapi specification of each service. Once done, it creates an aggregated specification and store it in a file. This file is then integrated in the docker image of the service.</p> <p>The procedure uses a configuration file. this file describes all the rs-server services that should be integrated in the aggregated openapi specification. Each time a new service is added, its configuration has to be added in this configuration file.</p>"},{"location":"rs-server/docs/doc/dev/design/services/frontend/","title":"Frontend","text":"<p>The frontend is a rs-service service. It only provides REST API documentation. It contains : * a /doc endpoint displaying the aggregated openapi specification of all the rs-server services using SwaggerUI * a /openapi.json displaying the same openapi specification on a json format</p> <p>The openapi specification is provided by a json file. The location of this file is configurable. It is given by an environment variable read at the start of the service.</p> <p>It contains no functional implementation of the rs-server services endpoints. Nevertheless, the \"Try it out\" functionality from Swagger is working since the rs-server is deployed with an ingres controller that handles the redirection of the requests to the dedicated rs-server service.</p> <p>The content of the documentation is static. It is computed during the frontend build procedure and then integrated statically in the frontend service. Each time a service endpoint is updated, the frontend has to be rebuilt also to provide an up-to-date documentation.</p> <p>The openapi specification is built on the CI workflow that build the frontend service executable. The procedure starts all the rs-server services and retrieve the openapi specification of each service. Once done, it creates an aggregated specification and store it in a file. This file is then integrated in the docker image of the service.</p> <p>The procedure uses a configuration file. this file describes all the rs-server services that should be integrated in the aggregated openapi specification. Each time a new service is added, its configuration has to be added in this configuration file.</p>"},{"location":"rs-server/docs/doc/dev/design/uac/design/","title":"Design","text":"<p>TODO To Be Defined</p>"},{"location":"rs-server/docs/doc/dev/environment/description/","title":"Description","text":"<p>The following table describes the development technical stack and explains briefly the choices that have been made.</p> Need Chosen techno Rational elements <p>language</p> <p>python</p> <p>the language commonly used by the final users</p> <p>language version</p> <p>python 3.11</p> <p>python 3.12 is too recent to be chosen</p> <p>dependency management</p> <p>poetry</p> <p>easy to use, good dependency management</p> <p>code formatting</p> <p>black</p> <p>the current standard</p> <p>unittests</p> <p>pytest</p> <p>standard</p> <p>lint</p> <p>pylint, flake8</p> <p>standard</p> <p>type check</p> <p>mypy</p> <p>commonly used by the team</p> <p>quality check</p> <p>sonarqube</p> <p>commonly used by the team</p> <p>commit check</p> <p>pre-commit</p> <p>commonly used by the team</p> <p>security check</p> <p>trivy</p> <p>used in the previous phase</p> <p>technical documentation</p> <p>asciidoctor</p> <p>good standard, simple syntax, good feedback from a team member</p>"},{"location":"rs-server/docs/doc/dev/environment/installation/","title":"Prerequisites","text":"<p>This tutorial assumes the developer has already basic knowledge and is using a Ubuntu/macOS based computer. No specific IDE is recommended for the development process, user can extend their environment with specific IDE integration.</p>"},{"location":"rs-server/docs/doc/dev/environment/installation/#checkout-the-project","title":"Checkout the project","text":"<p>The source code is hosted on GitHub. Personal Access Token (PAT) with specific rights on RS-Server repository should be configured before this process.</p> <ul> <li>Create a project folder that will contain all useful files</li> </ul> <pre><code>RSPY_ROOT=~/projects/rspy\nmkidr -P $RSPY_ROOT\nmkdir $RSPY_ROOT/src/\ncd $RSPY_ROOT\n</code></pre> <ul> <li>Checkout the project</li> </ul> <pre><code>cd $RSPY_ROOT/src\ngit clone --branch develop git@github.com:RS-PYTHON/rs-server.git\n</code></pre>"},{"location":"rs-server/docs/doc/dev/environment/installation/#install-python","title":"Install Python","text":"<p>RS-Server is using Python 3.11.</p> <p>Using pyenv is recommended for managing Python versions. Pyenv offers a straightforward method to switch between various Python versions.</p> <pre><code>cd $RSPY_ROOT/src\npyenv install 3.11.3\npyenv local 3.11.3\n</code></pre> <p>The Python version is an example, more recent version can be used.</p> <p>The following commands install Python 3.11 on the workstation and ensure that this version is used exclusively for RS-Server.</p>"},{"location":"rs-server/docs/doc/dev/environment/installation/#install-poetry","title":"Install Poetry","text":"<p>The dependency management on RS-Server is done with Poetry.</p> <p>Is it recommended to use pipx to install Python tools. Pipx provides an isolated environment for any Python tool.</p> <pre><code>cd $RSPY_ROOT/src\npipx install poetry\n</code></pre>"},{"location":"rs-server/docs/doc/dev/environment/installation/#setup-the-project","title":"Setup the project","text":"<p>The project is managed by poetry, and should be initialized locally first.</p> <pre><code>cd $RSPY_ROOT/src/rs-server\npoetry install --with dev\npoetry run opentelemetry-bootstrap -a install\n</code></pre> <p>This command initializes and activates a virtual environment for project. It installs in this environment the project dependencies and the develop dependencies (--with dev).</p> <p>An IDE can provide poetry integration.</p>"},{"location":"rs-server/docs/doc/dev/environment/installation/#verify-your-local-environment","title":"Verify your local environment","text":"<p>To verify if the local environment is correctly installed, all unittests should run normally using this command:</p> <pre><code>cd $RSPY_ROOT/src/rs-server\npytest tests -m \"unit\"\n</code></pre> <p>An IDE can provide integration to run unittests.</p>"},{"location":"rs-server/docs/doc/dev/environment/installation/#install-trivy","title":"Install trivy","text":"<p>trivy is used to perform security and license checks on the repository, the generated wheel packages and the docker images.</p> <p>trivy is run on the pre-commit and the CI to verify the repository compliance with common vulnerabilities, secrets and licenses.</p> <p>To install trivy on the local environment, follow the official procedure.</p>"},{"location":"rs-server/docs/doc/dev/environment/installation/#install-pre-commit-hooks","title":"Install pre-commit hooks","text":"<p>pre-commit is used to check basic rules before each commit. Can be activated using:</p> <pre><code>cd $RSPY_ROOT/src/rs-server\npoetry run pre-commit install\n</code></pre> <p>Once installed, checks will be performed before each commit to ensure the code is compliant with project best practices :</p> <ul> <li> <p>assert no unresolved merge conflicts</p> </li> <li> <p>fix end of file</p> </li> <li> <p>fix trailing whitespace</p> </li> <li> <p>check toml</p> </li> <li> <p>run black formatter</p> </li> <li> <p>run linters</p> </li> <li> <p>run trivy repo checks</p> </li> </ul> <p>It is useful to configure an IDE to run black at each file saving so that you keep your code compliant with the project coding style.</p>"},{"location":"rs-server/docs/doc/dev/environment/installation/#additional-ide-configuration","title":"Additional IDE configuration","text":"<p>We will not give specific IDE configurations. Nevertheless, it is relevant to configure your IDE to run format and lint after each save to keep your code compliant with the coding style at any moment.</p>"},{"location":"rs-server/docs/doc/dev/environment/installation/#the-next-steps","title":"The next steps","text":"<p>The document titled Development Technical Stack provides additional details about the environment. Additionally, the Development Workflow document describes the workflow followed by developers to implement stories. The Code Style document may also be useful for review.</p>"},{"location":"rs-server/docs/doc/users/architecture/","title":"RS-Server inside RS Python","text":"<p>The RS-Server is the major component of Reference-System Python. It controls user access to all sensitive interfaces:</p> <ul> <li>Catalog (fine access control per Collection)</li> <li>LTA</li> <li>ADGS</li> <li>PRIP</li> <li>CADIP</li> </ul> <p>The RS-Server components are the following ones:</p> <ul> <li>RS-Server Frontend</li> <li>RS-Server Backend / Catalog</li> <li>RS-Server Backend / PRIP</li> <li>RS-Server Backend / CADIP</li> <li>RS-Server Backend / LTA</li> <li>RS-Server Backend / AUXIP</li> </ul>"},{"location":"rs-server/docs/doc/users/architecture/#dynamic-view","title":"Dynamic View","text":"<p>On the following schema, we see that RS-Server is in the middle of the system with access control to Copernicus sensitive interfaces.</p> <p></p>"},{"location":"rs-server/docs/doc/users/architecture/#stac-item-lifecycle","title":"STAC item lifecycle","text":"<p>STAC standard is everywhere on RS. The following schema highligts where a STAC item is created, stored and updated.</p> <p></p>"},{"location":"rs-server/docs/doc/users/catalog/","title":"Catalog","text":""},{"location":"rs-server/docs/doc/users/catalog/#catalog","title":"Catalog","text":"<p>The following section groups all the endpoints used to interact with a STAC-compatible catalog of Sentinel products, auxiliary files and CADU chunks.</p>"},{"location":"rs-server/docs/doc/users/catalog/#stac-item","title":"STAC Item","text":"<p>A STAC Item represents a single geospatial asset or dataset. Items are built upon community extensions including the eo, eopf, sar, sat, processing, proj and timestamps extensions. It encapsulates metadata describing the asset, including its spatial and temporal extent, properties, and links to associated data files. STAC Items provide a standardized way to describe individual geospatial datasets, making it easier to discover, access, and use such data across different platforms and tools.</p>"},{"location":"rs-server/docs/doc/users/catalog/#stac-collection","title":"STAC Collection","text":"<p>A STAC Collection is a logical grouping of related STAC Features. It serves as a container for organizing and categorizing similar datasets based on common characteristics, themes, or purposes. Collections can represent various geospatial data themes such as satellite imagery, aerial photography, or land cover classifications. They provide a structured framework for managing and querying multiple related datasets collectively, simplifying data organization and access workflows.</p> <p>Using the endpoints described below, a user shall be able to:</p> <ul> <li> <p>Create / Read / Update / Delete a STAC item.</p> </li> <li> <p>Create / Read / Update / Delete a collection of STAC items.</p> </li> <li> <p>Search details of existing items and collections.</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/catalog/#create-a-collection","title":"Create a collection","text":"<p>This endpoint converts a request with a correct JSON body collection descriptor to a database entry.</p> <pre><code>POST /catalog/collections\n\n{\n    \"id\": \"test_collection\",\n    \"type\": \"Collection\",\n    \"description\": \"Collection description\",\n    \"stac_version\": \"1.0.0\",\n    \"owner\": \"test_owner\"\n}\n</code></pre>"},{"location":"rs-server/docs/doc/users/catalog/#get-a-collection","title":"Get a collection","text":"<p>This endpoint returns a collection details based on parameters given in request. The <code>ownerId</code> parameter is optional. If this is missing from the endpoint, a default user is used with the following priority:</p> <ul> <li>the user found in the <code>apikey security</code> in the case when the process is running on <code>cluster</code></li> <li> <p>the current user in the case when the process is running in <code>local mode</code></p> <p>GET /catalog/collections/{[ownerId:]collectionId}</p> <p>{   \"collections\": [     {       \"id\": \"test_collection\",       \"type\": \"Collection\",       \"owner\": \"test_owner\",       \"description\": \"Collection description\",       \"stac_version\": \"1.0.0\",       \"links\": [         {           \"rel\": \"items\",           \"type\": \"application/geo+json\",           \"href\": \"http://testserver/catalog/test_owner/collections/test_collection/items\"         },         {           \"rel\": \"parent\",           \"type\": \"application/json\",           \"href\": \"http://testserver/catalog/test_owner\"         },         {           \"rel\": \"root\",           \"type\": \"application/json\",           \"href\": \"http://testserver/catalog/test_owner\"         },         {           \"rel\": \"self\",           \"type\": \"application/json\",           \"href\": \"http://testserver/catalog/test_owner/collections/test_collection\"         }       ]     }   ],   \"links\": [     {       \"rel\": \"root\",       \"type\": \"application/json\",       \"href\": \"http://testserver/catalog/test_owner\"     },     {       \"rel\": \"parent\",       \"type\": \"application/json\",       \"href\": \"http://testserver/catalog/test_owner\"     },     {       \"rel\": \"self\",       \"type\": \"application/json\",       \"href\": \"http://testserver/catalog/test_owner/collections\"     }   ] }</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/catalog/#update-a-collection","title":"Update a collection","text":"<p>This endpoint updates a collection from STAC if it exists and request body json data is STAC compatible. The <code>ownerId</code> parameter is optional. If this is missing from the endpoint, a default user is used with the following priority:</p> <ul> <li>the user found in the <code>apikey security</code> in the case when the process is running on <code>cluster</code></li> <li> <p>the current user in the case when the process is running in <code>local mode</code></p> <p>PUT /catalog/collections/{[ownerId:]collectionId}</p> <p>{     \"id\": \"test_collection\",     \"type\": \"Collection\",     \"description\": \"Updated collection description\",     \"stac_version\": \"1.0.0\",     \"owner\": \"test_owner\" }</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/catalog/#delete-a-collection","title":"Delete a collection","text":"<p>This endpoint deletes a collection from STAC if it exists and owner has right to perform this action. The <code>ownerId</code> parameter is optional. If this is missing from the endpoint, a default user is used with the following priority:</p> <ul> <li>the user found in the <code>apikey security</code> in the case when the process is running on <code>cluster</code></li> <li> <p>the current user in the case when the process is running in <code>local mode</code></p> <p>DELETE /catalog/collections/{[ownerId:]collectionId}</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/catalog/#add-an-item","title":"Add an Item","text":"<p>This endpoint converts a request with a correct JSON body feature descriptor to a database entry. RS-Server Backend also move assets between s3 storages and updates hypertext reference of each STAC Feature with s3 locations. The <code>ownerId</code> parameter is optional. If this is missing from the endpoint, a default user is used with the following priority:</p> <ul> <li>the user found in the <code>apikey security</code> in the case when the process is running on <code>cluster</code></li> <li> <p>the current user in the case when the process is running in <code>local mode</code></p> <p>POST /catalog/collections/{[ownerId:]collectionId}/items</p> <p>{   \"collection\": \"S1_L2\",   \"assets\": {     \"zarr\": {       \"href\": \"s3://temp-bucket/S1SIWOCN_20220412T054447_0024_S139_T717.zarr.zip\",       \"roles\": [         \"data\"       ]     },     \"cog\": {       \"href\": \"s3://temp-bucket/S1SIWOCN_20220412T054447_0024_S139_T420.cog.zip\",       \"roles\": [         \"data\"       ]     },     \"ncdf\": {       \"href\": \"s3://temp-bucket/S1SIWOCN_20220412T054447_0024_S139_T902.nc\",       \"roles\": [         \"data\"       ]     }   },   \"bbox\": [0],   \"geometry\": {     \"type\": \"Polygon\",     \"coordinates\": [       [[-94.6334839, 37.0595608],         [-94.6334839, 37.0332547],         [-94.6005249, 37.0332547],         [-94.6005249, 37.0595608],         [-94.6334839, 37.0595608]]     ]   },   \"id\": \"S1SIWOCN_20220412T054447_0024_S139\",   \"links\": [     {       \"href\": \"./.zattrs.json\",       \"rel\": \"self\",       \"type\": \"application/json\"     }   ],   \"other_metadata\": {},   \"properties\": {     \"gsd\": 0.5971642834779395,     \"width\": 2500,     \"height\": 2500,     \"datetime\": \"2000-02-02T00:00:00Z\",     \"proj:epsg\": 3857,     \"orientation\": \"nadir\"   },   \"stac_extensions\": [     \"https://stac-extensions.github.io/eopf/v1.0.0/schema.json\"   ],   \"stac_version\": \"1.0.0\",   \"type\": \"Feature\" }</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/catalog/#get-an-item","title":"Get an Item","text":"<p>This endpoint returns a feature details based on parameters given in request. The <code>ownerId</code> parameter is optional. If this is missing from the endpoint, a default user is used with the following priority:</p> <ul> <li>the user found in the <code>apikey security</code> in the case when the process is running on <code>cluster</code></li> <li> <p>the current user in the case when the process is running in <code>local mode</code></p> <p>GET /catalog/collections/{[ownerId:]collectionId}/items/{featureID}</p> <p>{   \"id\": \"S1SIWOCN_20220412T054447_0024_S139\",   \"bbox\": [0],   \"type\": \"Feature\",   \"links\": [     {       \"rel\": \"collection\",       \"type\": \"application/json\",       \"href\": \"http://testserver/catalog/fixture_owner/collections/fixture_collection\"     },     {       \"rel\": \"parent\",       \"type\": \"application/json\",       \"href\": \"http://testserver/catalog/fixture_owner/collections/fixture_collection\"     },     {       \"rel\": \"root\",       \"type\": \"application/json\",       \"href\": \"http://testserver/catalog/fixture_owner\"     },     {       \"rel\": \"self\",       \"type\": \"application/geo+json\",       \"href\": \"http://testserver/catalog/fixture_owner/collections/fixture_collection/items/new_feature_id\"     }   ],   \"assets\": {     \"cog\": {       \"href\": \"https://rs-server/catalog/fixture_owner/collections/fixture_collection/items/some_file.cog.zip/download/cog\",       \"roles\": [         \"data\"       ],       \"alternate\": {         \"s3\": {           \"href\": \"s3://catalog-bucket/correct_location/some_file.cog.zip\"         }       }     },     \"ncdf\": {       \"href\": \"https://rs-server/catalog/fixture_owner/collections/fixture_collection/items/some_file.ncdf.zip/download/ncdf\",       \"roles\": [         \"data\"       ],       \"alternate\": {         \"s3\": {           \"href\": \"s3://catalog-bucket/correct_location/some_file.ncdf.zip\"         }       }     },     \"zarr\": {       \"href\": \"https://rs-server/catalog/fixture_owner/collections/fixture_collection/items/some_file.zarr.zip/download/zarr\",       \"roles\": [         \"data\"       ],       \"alternate\": {         \"s3\": {           \"href\": \"s3://catalog-bucket/correct_location/some_file.zarr.zip\"         }       }     }   },   \"geometry\": {     \"type\": \"Polygon\",     \"coordinates\": [       [[-94.6334839, 37.0595608],         [-94.6334839, 37.0332547],         [-94.6005249, 37.0332547],         [-94.6005249, 37.0595608],         [-94.6334839, 37.0595608]]     ]   },   \"collection\": \"fixture_collection\",   \"properties\": {     \"gsd\": 0.5971642834779395,     \"owner\": \"fixture_owner\",     \"width\": 2500,     \"height\": 2500,     \"datetime\": \"2000-02-02T00:00:00Z\",     \"proj:epsg\": 3857,     \"orientation\": \"nadir\"   },   \"stac_version\": \"1.0.0\",   \"stac_extensions\": [     \"https://stac-extensions.github.io/eopf/v1.0.0/schema.json\"   ] }</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/catalog/#update-an-item","title":"Update an Item","text":"<p>This endpoint updates content of a feature is request JSON data is completely STAC-compatible. The <code>ownerId</code> parameter is optional. If this is missing from the endpoint, a default user is used with the following priority:</p> <ul> <li>the user found in the <code>apikey security</code> in the case when the process is running on <code>cluster</code></li> <li> <p>the current user in the case when the process is running in <code>local mode</code></p> <p>PUT /catalog/collections/{[ownerId:]collectionId}/items/{featureID}</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/catalog/#download-an-item","title":"Download an Item","text":"<p>This endpoint returns a S3 presigned url that can directly download the file when accessed. The <code>ownerId</code> parameter is optional. If this is missing from the endpoint, a default user is used with the following priority:</p> <ul> <li>the user found in the <code>apikey security</code> in the case when the process is running on <code>cluster</code></li> <li> <p>the current user in the case when the process is running in <code>local mode</code></p> <p>GET /catalog/collections/{[ownerId:]collectionId}/items/{featureID}/download/{assetId}</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/configuration/","title":"Configuration","text":"<p>The rs-server configuration will be described here</p>"},{"location":"rs-server/docs/doc/users/functionalities/","title":"Main Functionalities","text":""},{"location":"rs-server/docs/doc/users/functionalities/#cadip","title":"CADIP","text":"<p>The CADU Interface delivery Point (CADIP) is a pick-up point for Sentinel CADU data. The CADIP allows clients to straightforwardly discover and retrieve available data files through a standard OData RESTful API. The following endpoints have been implemented in RS-Server to interact with CADIP RESTful API</p>"},{"location":"rs-server/docs/doc/users/functionalities/#search-endpoint","title":"Search Endpoint","text":"<p>This endpoint retrieves a list of CADUs from a specified station within a given time range and returns a response compatible with the SpatioTemporal Asset Catalog (STAC) format.</p> <p>The response from the data pickup-point is in OData format, which is then transformed into the STAC format within RS-Server using a configurable mapping between OData and STAC. The mapping file can be viewed to understand how the conversion is performed and to customize it if needed. The mapping file can be viewed here.</p>"},{"location":"rs-server/docs/doc/users/functionalities/#api-reference","title":"API Reference","text":"<p><code>/cadip/{station}/cadu/search</code></p>"},{"location":"rs-server/docs/doc/users/functionalities/#parameters","title":"Parameters","text":"<ul> <li> <p><code>station</code> (str): Identifier for the CADIP station (e.g., MTI, SGS,     MPU, INU, etc).</p> </li> <li> <p><code>datetime</code> (str): Specifies a date interval for time series filtering, with the start and end dates separated by a slash ('/'). The format follows ISO 8601 standards. (format: \"YYYY-MM-DDThh:mm:sssZ/YYYY-MM-DDThh:mm:sssZ\").</p> </li> <li> <p><code>session_id</code> (str): Session from which file belong. Can be a single value or a comma-separated list. (?session_id=S1A_20231120061537234567) or (?session_id=S1A_20231120061537234567, S2B_20231117033237234567) to search files comming from multiple acquisition sessions.</p> </li> <li> <p><code>limit</code> (int, optional): Maximum number of products to return,    default set to 1000.</p> </li> <li> <p><code>sortby</code> (str, optional): Sorting criteria. +/-fieldName indicates     ascending/descending order and field name (e.g. sortby=+created)     Default no sorting is applied.</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/functionalities/#request-example","title":"Request example","text":"<pre><code>GET /cadip/station123/cadu/search?datetime=2023-01-01T00:00:00Z/2023-01-02T23:59:59Z&amp;limit=50&amp;sortby=-created\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#return","title":"Return","text":"<pre><code>{\n    \"stac_version\": \"1.0.0\",\n    \"stac_extensions\": [\"https://stac-extensions.github.io/file/v2.1.0/schema.json\"],\n    \"type\": \"Feature\",\n    \"id\": \"DCS_01_S1A_20170501121534062343_ch1_DSDB_00001.raw\",\n    \"geometry\": null,\n    \"properties\": {\n        \"datetime\": \"2019-02-16T12:00:00.000Z\",\n        \"eviction_datetime\": \"2019-12-16T12:00:00.000Z\",\n        \"cadip:id\": \"2b17b57d-fff4-4645-b539-91f305c27c69\",\n        \"cadip:retransfer\": false,\n        \"cadip:final_block\": true,\n        \"cadip:block_number\": 1,\n        \"cadip:channel\": 2,\n        \"cadip:session_id\": \"S1A_20170501121534062343\",\n    },\n    \"links\": [],\n    \"assets\": {\n        \"file\": {\n            \"file:size\": 32553\n        }\n    },\n}\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#session-search-endpoint","title":"Session search Endpoint","text":"<p>This endpoint retrieves a list of sessions from the CADU system for a specified station within a given parameter and/or time range and return a STAC compatible FeatureCollection response.</p>"},{"location":"rs-server/docs/doc/users/functionalities/#api-reference_1","title":"API Reference","text":"<p><code>/cadip/{station}/session</code></p>"},{"location":"rs-server/docs/doc/users/functionalities/#parameters_1","title":"Parameters","text":"<ul> <li> <p><code>station</code> (str): Identifier for the CADIP station (e.g., MTI, SGS,     MPU, INU, etc).</p> </li> <li> <p><code>id</code> (str, list[str], optional): DSIB SessionId value. Can be used     with coma-separated values (e.g., id=S1A_20170501121534062343 or id=S1A_20170501121534062343,S1A_20241212111534094212).</p> </li> <li> <p><code>platform</code> (str, list[str], optional): Platform / Satellite     identifier. Can be used with coma-separated values (e.g: platform =     S1A or platform=S1A, S2B).</p> </li> <li> <p><code>start_date</code> (str, optional): Start time of session     (PublicationTime). (format: YYYY-MM-DDThh:mm:sssZ).</p> </li> <li> <p><code>stop_date</code> (str, optional): Stop time of session (PublicationTime).     (format: YYYY-MM-DDThh:mm:sssZ).</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/functionalities/#note","title":"Note","text":"<p>A valid session search request must contain at least a value for either id or platform or time interval (start_date and stop_date correctly defined).</p>"},{"location":"rs-server/docs/doc/users/functionalities/#request-example_1","title":"Request example","text":"<pre><code>GET /cadip/station123/session?id=S1A_20170501121534062343,S1A_20240328185208053186\n\nGET /cadip/station123/session?start_date=2020-02-16T12:00:00Z&amp;stop_date=2023-02-16T12:00:00Z&amp;platform=S1A\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#return_1","title":"Return","text":"<pre><code>{\n    \"type\": \"FeatureCollection\",\n    \"numberMatched\": 3,\n    \"numberReturned\": 3,\n    \"features\": [\n        {\n            \"stac_version\": \"1.0.0\",\n            \"stac_extensions\": [\n                \"https://stac-extensions.github.io/timestamps/v1.1.0/schema.json\"\n            ],\n            \"type\": \"Feature\",\n            \"id\": \"S1A_20240328185208053186\",\n            \"geometry\": null,\n            \"properties\": {\n                \"start_datetime\": \"2024-03-28T18:52:08.000Z\",\n                \"datetime\": \"2024-03-28T18:52:08.000Z\",\n                \"end_datetime\": \"2024-03-28T19:00:52.000Z\",\n                \"published\": \"2024-03-28T18:52:26Z\",\n                \"platform\": \"S1A\",\n                \"cadip:id\": \"726f387b-ad2d-3538-8834-95e3cf8894c6\",\n                \"cadip:num_channels\": 2,\n                \"cadip:station_unit_id\": \"01\",\n                \"cadip:downlink_orbit\": 53186,\n                \"cadip:acquisition_id\": 531861,\n                \"cadip:antenna_id\": \"MSP21\",\n                \"cadip:front_end_id\": \"01\",\n                \"cadip:retransfer\": false,\n                \"cadip:antenna_status_ok\": true,\n                \"cadip:front_end_status_ok\": true,\n                \"cadip:planned_data_start\": \"2024-03-28T18:52:08.336Z\",\n                \"cadip:planned_data_stop\": \"2024-03-28T19:00:51.075Z\",\n                \"cadip:downlink_status_ok\": true,\n                \"cadip:delivery_push_ok\": true\n            },\n            \"links\": [],\n            \"assets\": {}\n        },\n        {\n            \"stac_version\": \"1.0.0\",\n            \"stac_extensions\": [\n                \"https://stac-extensions.github.io/timestamps/v1.1.0/schema.json\"\n            ],\n            \"type\": \"Feature\",\n            \"id\": \"S1A_20240328185208053186\",\n            \"geometry\": null,\n            \"properties\": {\n                \"start_datetime\": \"2024-03-28T18:52:08.000Z\",\n                \"datetime\": \"2024-03-28T18:52:08.000Z\",\n                \"end_datetime\": \"2024-03-28T19:00:52.000Z\",\n                \"published\": \"2024-03-28T18:52:26Z\",\n                \"platform\": \"S1A\",\n                \"cadip:id\": \"726f387b-ad2d-3538-8834-95e3cf8894c6\",\n                \"cadip:num_channels\": 2,\n                \"cadip:station_unit_id\": \"01\",\n                \"cadip:downlink_orbit\": 53186,\n                \"cadip:acquisition_id\": 531861,\n                \"cadip:antenna_id\": \"MSP21\",\n                \"cadip:front_end_id\": \"01\",\n                \"cadip:retransfer\": false,\n                \"cadip:antenna_status_ok\": true,\n                \"cadip:front_end_status_ok\": true,\n                \"cadip:planned_data_start\": \"2024-03-28T18:52:08.336Z\",\n                \"cadip:planned_data_stop\": \"2024-03-28T19:00:51.075Z\",\n                \"cadip:downlink_status_ok\": true,\n                \"cadip:delivery_push_ok\": true\n            },\n            \"links\": [],\n            \"assets\": {}\n        },\n        {\n            \"stac_version\": \"1.0.0\",\n            \"stac_extensions\": [\n                \"https://stac-extensions.github.io/timestamps/v1.1.0/schema.json\"\n            ],\n            \"type\": \"Feature\",\n            \"id\": \"S1A_20240329083700053194\",\n            \"geometry\": null,\n            \"properties\": {\n                \"start_datetime\": \"2024-03-28T18:52:08.000Z\",\n                \"datetime\": \"2024-03-28T18:52:08.000Z\",\n                \"end_datetime\": \"2024-03-28T19:00:52.000Z\",\n                \"published\": \"2024-03-29T08:37:22Z\",\n                \"platform\": \"S2B\",\n                \"cadip:id\": \"726f387b-ad2d-3538-8834-95e3cf8894c6\",\n                \"cadip:num_channels\": 2,\n                \"cadip:station_unit_id\": \"01\",\n                \"cadip:downlink_orbit\": 53186,\n                \"cadip:acquisition_id\": 531861,\n                \"cadip:antenna_id\": \"MSP21\",\n                \"cadip:front_end_id\": \"01\",\n                \"cadip:retransfer\": false,\n                \"cadip:antenna_status_ok\": true,\n                \"cadip:front_end_status_ok\": true,\n                \"cadip:planned_data_start\": \"2024-03-28T18:52:08.336Z\",\n                \"cadip:planned_data_stop\": \"2024-03-28T19:00:51.075Z\",\n                \"cadip:downlink_status_ok\": true,\n                \"cadip:delivery_push_ok\": true\n            },\n            \"links\": [],\n            \"assets\": {}\n        }\n    ]\n}\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#download-endpoint","title":"Download Endpoint","text":"<p>This endpoint initiates an asynchronous download process for a CADU file using EODAG. If specific parameters are provided, endpoint also upload the file to an S3 bucket.</p>"},{"location":"rs-server/docs/doc/users/functionalities/#api-reference_2","title":"API Reference","text":"<p><code>/cadip/{station}/cadu</code></p>"},{"location":"rs-server/docs/doc/users/functionalities/#parameters_2","title":"Parameters","text":"<ul> <li> <p><code>station</code> (str): The EODAG station identifier (e.g., MTI, SGS, MPU,     INU, etc).</p> </li> <li> <p><code>name</code> (str): The name of the CADU file to be downloaded.</p> </li> <li> <p><code>local</code> (str, optional): The local path where the CADU file will     be downloaded.</p> </li> <li> <p><code>obs</code> (str, optional): S3 storage path where the CADU file will be     uploaded. (e.g. s3://bucket/path/to/file.raw). Connection to S3     bucket is required, and should be written in the environmental     variables, S3_ACCESSKEY, S3_SECRETKEY, S3_ENDPOINT     and S3_REGION.</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/functionalities/#returns","title":"Returns","text":"<ul> <li><code>dict</code>: A dictionary indicating whether the download process has     started.</li> </ul>"},{"location":"rs-server/docs/doc/users/functionalities/#request-example_2","title":"Request example","text":"<pre><code>GET /cadip/station123/cadu?name=DCS_04_S1A_20231121072204051312_ch1_DSDB_00001.raw\n\nGET /cadip/station123/cadu?name=DCS_04_S1A_20231121072204051312_ch1_DSDB_00001.raw&amp;local=/tmp/file.raw\n\nGET /cadip/station123/cadu?name=DCS_04_S1A_20231121072204051312_ch1_DSDB_00001.raw&amp;local=/tmp/file.raw&amp;obs=s3://bucket/path/to/file.raw\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#response","title":"Response","text":"<pre><code>{\n  \"started\": \"true\"\n}\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#status-endpoint","title":"Status Endpoint","text":"<p>This endpoint is used to query the download status of an CADU file.</p>"},{"location":"rs-server/docs/doc/users/functionalities/#api-reference_3","title":"API Reference","text":"<p><code>/cadip/{station}/cadu/status</code></p>"},{"location":"rs-server/docs/doc/users/functionalities/#parameters_3","title":"Parameters","text":"<ul> <li> <p><code>station</code> (str): The EODAG station identifier (e.g., MTI, SGS, MPU,     INU, etc).</p> </li> <li> <p><code>name</code> (str): The name of the CADU file to be queried from database.</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/functionalities/#request","title":"Request","text":"<pre><code>GET /cadip/{station}/cadu/status?name=DCS_04_S1A_20231121072204051312_ch1_DSDB_00001.raw\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#response_1","title":"Response","text":"<pre><code>{\n  \"product_id\": \"2b17b57d-fff4-4645-b539-91f305c27c69\",\n  \"name\": \"DCS_04_S1A_20231121072204051312_ch1_DSDB_00001.raw\",\n  \"available_at_station\": \"2019-02-16T12:00:00.000Z\",\n  \"db_id\": 1,\n  \"download_start\": \"2023-02-16T12:00:00.000Z\",\n  \"download_stop\": null,\n  \"status\": \"IN_PROGRESS\",\n  \"status_fail_message\": null\n}\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#adgs","title":"ADGS","text":"<p>The Auxiliary Data Gathering Service (ADGS) is a pick-up point for Sentinel auxiliary files. This service allows clients to discover and retrieve available auxiliary data files through a standard OData RESTful API. The following endpoints have been implemented in RS-Server to interact with ADGS RESTful API.</p> <p>The data pickup-point response is a OData formatted content which is then converted to STAC format inside rs-server using a configurable mapping between OData and STAC. The mapping file can be viewed here.</p>"},{"location":"rs-server/docs/doc/users/functionalities/#search-endpoint_1","title":"Search Endpoint","text":"<p>This endpoint handles the search for products in the AUX station within a specified time interval and return a STAC compatible FeatureCollection response.</p>"},{"location":"rs-server/docs/doc/users/functionalities/#api-reference_4","title":"API Reference","text":"<p><code>/adgs/aux/search</code></p>"},{"location":"rs-server/docs/doc/users/functionalities/#parameters_4","title":"Parameters","text":"<ul> <li> <p><code>datetime</code> (str): Specifies a date interval for time series filtering, with the start and end dates separated by a slash ('/'). The format follows ISO 8601 standards. (format: \"YYYY-MM-DDThh:mm:sssZ/YYYY-MM-DDThh:mm:sssZ\").</p> </li> <li> <p><code>limit</code> (int, optional): Maximum number of products to return,     default set to 1000.</p> </li> <li> <p><code>sortby</code> (str, optional): Sorting criteria. +/-fieldName indicates     ascending/descending order and field name (e.g. sortby=+created)     Default no sorting is applied.</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/functionalities/#request-example_3","title":"Request Example","text":"<pre><code>GET /adgs/aux/search?datetime=2018-01-01T00:00:00Z/2023-01-02T23:59:59Z&amp;limit=10&amp;sortby=+properties.adgs:id\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#response_2","title":"Response","text":"<pre><code>{\n    \"stac_version\": \"1.0.0\",\n    \"stac_extensions\": [\"https://stac-extensions.github.io/file/v2.1.0/schema.json\"],\n    \"type\": \"Feature\",\n    \"id\": \"DCS_01_S1A_20170501121534062343_ch1_DSDB_00001.raw\",\n    \"geometry\": null,\n    \"properties\": {\n        \"adgs:id\": \"2b17b57d-fff4-4645-b539-91f305c27c69\",\n        \"datetime\": \"2019-02-16T12:00:00.000Z\",\n        \"start_datetime\": \"2019-02-16T11:59:58.000Z\",\n        \"end_datetime\": \"2019-02-16T12:00:00.000Z\",\n    },\n    \"links\": [],\n    \"assets\": {\n        \"file\": {\n            \"file:size\": 29301\n        }\n    }\n}\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#download-endpoint_1","title":"Download Endpoint","text":"<p>This endpoint initiates an asynchronous download process for an AUX product using EODAG. If specific parameters are provided, endpoint also upload the file to an S3 bucket.</p>"},{"location":"rs-server/docs/doc/users/functionalities/#api-reference_5","title":"API Reference","text":"<p><code>/adgs/aux</code></p>"},{"location":"rs-server/docs/doc/users/functionalities/#parameters_5","title":"Parameters","text":"<ul> <li> <p><code>name</code> (str): The name of the AUX product to be downloaded</p> </li> <li> <p><code>local</code> (str, optional): The local path where the AUX product will     be downloaded.</p> </li> <li> <p><code>obs</code> (str, optional): S3 storage path where the AUX file will be     uploaded. (e.g. s3://bucket/path/to/file.tgz). Connection to S3     bucket is required, and should be written in the environmental     variables, S3_ACCESSKEY, S3_SECRETKEY, S3_ENDPOINT     and S3_REGION.</p> </li> </ul>"},{"location":"rs-server/docs/doc/users/functionalities/#returns_1","title":"Returns","text":"<ul> <li><code>dict</code>: A dictionary indicating whether the download process has     started.</li> </ul>"},{"location":"rs-server/docs/doc/users/functionalities/#request-example_4","title":"Request Example","text":"<pre><code>GET /adgs/aux?name=S2__OPER_AUX_ECMWFD_PDMC_20190216T120000_V20190217T090000_20190217T210000.TGZ\n\nGET /adgs/aux?name=S2__OPER_AUX_ECMWFD_PDMC_20190216T120000_V20190217T090000_20190217T210000.TGZ&amp;local=/tmp/aux.tar.gz\n\nGET /adgs/aux?name=S2__OPER_AUX_ECMWFD_PDMC_20190216T120000_V20190217T090000_20190217T210000.TGZ&amp;local=/tmp/aux.tar.gz&amp;obs=s3://bucket/path/to/aux.tar.gz\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#response_3","title":"Response","text":"<pre><code>{\n  \"started\": \"true\"\n}\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#status-endpoint_1","title":"Status Endpoint","text":"<p>This endpoint is used to query the download status of an AUX file.</p>"},{"location":"rs-server/docs/doc/users/functionalities/#endpoint","title":"Endpoint","text":"<p><code>/adgs/aux/status</code></p>"},{"location":"rs-server/docs/doc/users/functionalities/#parameters_6","title":"Parameters","text":"<ul> <li><code>name</code> (str): The name of the AUX file to be queried from database.</li> </ul>"},{"location":"rs-server/docs/doc/users/functionalities/#request-example_5","title":"Request Example","text":"<pre><code>GET /adgs/aux/status?name=S2__OPER_AUX_ECMWFD_PDMC_20200216T120000_V20190217T090000_20190217T210000.TGZ\n</code></pre>"},{"location":"rs-server/docs/doc/users/functionalities/#response_4","title":"Response","text":"<pre><code>{\n  \"product_id\": \"id2\",\n  \"name\": \"S2__OPER_AUX_ECMWFD_PDMC_20200216T120000_V20190217T090000_20190217T210000.TGZ\",\n  \"available_at_station\": \"2020-02-16T12:00:00\",\n  \"db_id\": 2,\n  \"download_start\": \"2023-02-16T12:00:00\",\n  \"download_stop\": \"2023-02-16T12:01:00\",\n  \"status\": \"DONE\",\n  \"status_fail_message\": null\n}\n</code></pre>"},{"location":"rs-server/docs/doc/users/installation/","title":"Installation","text":"<p>The rs-server installation will be described here</p>"},{"location":"rs-server/docs/doc/users/oauth2_apikey_manager/","title":"OAuth2 and API Key Manager","text":"<p>All the RS-Server HTTP services are protected: to use them, you must authenticate using your RS-Server KeyCloak account.</p> <p>There are two different authentication methods:</p> <ul> <li>OAuth2 authentication</li> <li>Using an API Key</li> </ul> <p>When you authenticate with OAuth2, it automatically opens a new KeyCloak logging window:</p> <p></p> <p>The downside of this method is that, for security reasons, it has a short lifespan and you have to manually authenticate again every few minutes or hours. So this method cannot be used to run long scripts for example.</p> <p>This is why we use an API key: after being authenticated with OAuth2, you can create yourself a personal API key, with a longer lifespan, that you will use to authenticate to RS-Server.</p> <p>To do this:</p> <ol> <li>Go to the API Key Manager section of the RS-Server frontend:</li> </ol> <p></p> <ul> <li>Notes:<ul> <li>You will need to login to your KeyCloak account if requested (i.e. if your last session has expired).</li> <li>To login using a different KeyCloak account, either remove your cookies from your browser settings, or use an incognito/private browser session.</li> </ul> </li> </ul> <ol> <li> <p>Expand the <code>Create Api Key</code> endpoint, click the <code>Try it out</code> button on the right, fill your API key parameters and click the <code>Execute</code> button on the bottom. The parameters are:</p> <ul> <li>name: free text.</li> <li>never_expires: if false, this API key will expire in 15 days.</li> <li>config: not used for now.</li> <li>allowed_referers: not used for now.</li> </ul> </li> </ol> <p></p> <ol> <li>Note your created API key value and save it for later. You will find it in the <code>Response body</code> frame. It must be saved without the quotes, like in the example below:</li> </ol> <p></p> <ol> <li> <p>Try the other API Key Manager endpoints:</p> <ul> <li>Show My Information: show my KeyCloak account information</li> <li>List My Api Keys: list all API keys and usage information associated with my account.</li> <li>Revoke Api Key: Revoke an API key associated with my account.</li> <li>Renew Api Key: Renew an API key associated with my account, reactivate it if it was revoked.</li> </ul> </li> </ol> <p>Now you can use your API key to authenticate to RS-Server and call the Auxip, Cadip and STAC catalog HTTP endpoints. If you want to call them from the website, click on the lock icons on the right and enter your API key value:</p> <p></p>"}]}